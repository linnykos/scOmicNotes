[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "scOmicNotes",
    "section": "",
    "text": "This is a Quarto book made from the content of BIOST 545 (Biostatistical Methods for Big Omics Data), taught by Dr. Kevin Lin at the University of Washington, Winter 2025. The lecture notes in BIOST 545 are meant to ease you into reading these notes, and the notes are meant to be a gateway to many references where the course’s goal is for you 1) to determine for yourself what areas you find interesting and are relevant to your research goals, and 2) use the references as a portal to find many ideas to bolster your own research. This Quarto book is assembled with the help of many student volunteers at the University of Washington: Turbo Du [TBD].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why study cell biology in public health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ahlmann-Eltze, Constantin, and Wolfgang Huber. 2023. “Comparison\nof Transformations for Single-Cell RNA-Seq Data.” Nature\nMethods, 1–8.\n\n\nArora, Sanjeev, Wei Hu, and Pravesh K Kothari. 2018. “An Analysis\nof the t-Sne Algorithm for Data Visualization.” In Conference\non Learning Theory, 1455–62. PMLR.\n\n\nCai, T Tony, and Rong Ma. 2022. “Theoretical Foundations of t-Sne\nfor Visualizing High-Dimensional Clustered Data.” Journal of\nMachine Learning Research 23 (301): 1–54.\n\n\nCarbonetto, Peter, Abhishek Sarkar, Zihao Wang, and Matthew Stephens.\n2021. “Non-Negative Matrix Factorization Algorithms Greatly\nImprove Topic Model Fits.” arXiv Preprint\narXiv:2105.13440.\n\n\nChari, Tara, and Lior Pachter. 2023. “The Specious Art of\nSingle-Cell Genomics.” PLOS Computational Biology 19\n(8): e1011288.\n\n\nChen, Wenan, Yan Li, John Easton, David Finkelstein, Gang Wu, and Xiang\nChen. 2018. “UMI-Count Modeling and Differential\nExpression Analysis for Single-Cell RNA Sequencing.”\nGenome Biology 19 (1): 70.\n\n\nClaringbould, Annique, and Judith B Zaugg. 2021. “Enhancers in\nDisease: Molecular Basis and Emerging Treatment Strategies.”\nTrends in Molecular Medicine 27 (11): 1060–73.\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes\nMethods and False Discovery Rates for Microarrays.” Genetic\nEpidemiology 23 (1): 70–86.\n\n\nGhojogh, Benyamin, Ali Ghodsi, Fakhri Karray, and Mark Crowley. 2021.\n“Factor Analysis, Probabilistic Principal Component Analysis,\nVariational Inference, and Variational Autoencoder: Tutorial and\nSurvey.” arXiv Preprint arXiv:2101.00734.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and\nKenneth J Livak. 2021. “Applying High-Dimensional Single-Cell\nTechnologies to the Analysis of Cancer Immunotherapy.” Nature\nReviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle\nGaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular\nClassification of Cancer: Class Discovery and Class Prediction by Gene\nExpression Monitoring.” Science 286 (5439): 531–37.\n\n\nHafemeister, Christoph, and Rahul Satija. 2019. “Normalization and\nVariance Stabilization of Single-Cell RNA-seq Data Using Regularized Negative Binomial\nRegression.” Genome Biology 20 (1): 1–15.\n\n\nHaghverdi, Laleh, Florian Buettner, and Fabian J Theis. 2015.\n“Diffusion Maps for High-Dimensional Single-Cell Analysis of\nDifferentiation Data.” Bioinformatics 31 (18): 2989–98.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023.\n“Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and\nTrials.” Nature Aging 3 (5): 506–19.\n\n\nHeumos, Lukas, Anna C Schaar, Christopher Lance, Anastasia Litinetskaya,\nFelix Drost, Luke Zappia, Malte D Lücken, et al. 2023. “Best\nPractices for Single-Cell Analysis Across Modalities.” Nature\nReviews Genetics 24 (8): 550–72.\n\n\nHou, Wenpin, Zhicheng Ji, Hongkai Ji, and Stephanie C Hicks. 2020.\n“A Systematic Evaluation of Single-Cell\nRNA-Sequencing Imputation Methods.” Genome\nBiology 21 (1): 1–30.\n\n\nHuang, Haiyang, Yingfan Wang, Cynthia Rudin, and Edward P Browne. 2022.\n“Towards a Comprehensive Evaluation of Dimension Reduction Methods\nfor Transcriptomic Data Visualization.” Communications\nBiology 5 (1): 719.\n\n\nIto, Shinsuke, Nando Dulal Das, Takashi Umehara, and Haruhiko Koseki.\n2022. “Factors and Mechanisms That Influence Chromatin-Mediated\nEnhancer–Promoter Interactions and Transcriptional Regulation.”\nCancers 14 (21): 5404.\n\n\nJohnson, Eric M, William Kath, and Madhav Mani. 2022.\n“EMBEDR: Distinguishing Signal from Noise in\nSingle-Cell Omics Data.” Patterns 3 (3).\n\n\nKent, JT, John Bibby, and KV Mardia. 1979. Multivariate\nAnalysis. Academic Press Amsterdam.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N\nShokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse\nProteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease\nBrain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nKharchenko, Peter V, Lev Silberstein, and David T Scadden. 2014.\n“Bayesian Approach to Single-Cell Differential Expression\nAnalysis.” Nature Methods 11 (7): 740.\n\n\nKorsunsky, Ilya, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang,\nKevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya\nRaychaudhuri. 2019. “Fast, Sensitive and Accurate Integration of\nSingle-Cell Data with Harmony.” Nature Methods, 1–8.\n\n\nLause, Jan, Philipp Berens, and Dmitry Kobak. 2021. “Analytic\nPearson Residuals for Normalization of Single-Cell RNA-Seq UMI\nData.” Genome Biology 22: 1–20.\n\n\n———. 2024. “The Art of Seeing the Elephant in the Room: 2D\nEmbeddings of Single-Cell Data Do Make Sense.” bioRxiv.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun\nYu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in\nCancer Research: Progress and Perspectives.” Journal of\nHematology & Oncology 14 (1): 91.\n\n\nLi, Lei, Yumei Li, Xudong Zou, Fuduan Peng, Ya Cui, Eric J Wagner, and\nWei Li. 2022. “Population-Scale Genetic Control of Alternative\nPolyadenylation and Its Association with Human Diseases.”\nQuantitative Biology 10 (1): 44–54.\n\n\nLin, Kevin Z, Yixuan Qiu, and Kathryn Roeder. 2022. “eSVD: Cohort-Level Differential Expression in\nMulti-Individual Single-Cell RNA-Seq Data Using\nExponential-Family Embeddings.” (In Preparation).\n\n\n———. 2024. “eSVD-DE: Cohort-Wide\nDifferential Expression in Single-Cell RNA-Seq Data Using\nExponential-Family Embeddings.” BMC Bioinformatics 25\n(1): 113.\n\n\nLopez, Romain, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir\nYosef. 2018. “Deep Generative Modeling for Single-Cell\nTranscriptomics.” Nature Methods 15 (12): 1053.\n\n\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014.\n“Moderated Estimation of Fold Change and Dispersion for\nRNA-Seq Data with DESeq2.” Genome\nBiology 15 (12): 550.\n\n\nLuecken, Malte D, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese,\nMarta Interlandi, Michaela F Müller, Daniel C Strobl, et al. 2022.\n“Benchmarking Atlas-Level Data Integration in Single-Cell\nGenomics.” Nature Methods 19 (1): 41–50.\n\n\nLuecken, Malte D, and Fabian J Theis. 2019. “Current Best\nPractices in Single-Cell RNA-Seq Analysis: A\nTutorial.” Molecular Systems Biology 15 (6): e8746.\n\n\nMa, Rong, Eric D Sun, David Donoho, and James Zou. 2024.\n“Principled and Interpretable Alignability Testing and Integration\nof Single-Cell Data.” Proceedings of the National Academy of\nSciences 121 (10): e2313719121.\n\n\nMaden, Sean K, Sang Ho Kwon, Louise A Huuki-Myers, Leonardo\nCollado-Torres, Stephanie C Hicks, and Kristen R Maynard. 2023.\n“Challenges and Opportunities to Computationally Deconvolve\nHeterogeneous Tissue with Varying Cell Sizes Using Single-Cell\nRNA-Sequencing Datasets.” Genome Biology 24 (1): 288.\n\n\nMcGinnis, Christopher S, Lyndsay M Murrow, and Zev J Gartner. 2019.\n“DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing\nData Using Artificial Nearest Neighbors.” Cell Systems 8\n(4): 329–37.\n\n\nPrater, Katherine E, and Kevin Z Lin. 2024. “All the Single Cells:\nSingle-Cell Transcriptomics/Epigenomics Experimental Design and Analysis\nConsiderations for Glial Biologists.” Glia.\n\n\nQian, Kun, Shiwei Fu, Hongwei Li, and Wei Vivian Li. 2022.\n“scINSIGHT for Interpreting Single-Cell Gene Expression from\nBiologically Heterogeneous Data.” Genome Biology 23 (1):\n1–23.\n\n\nRisso, Davide, John Ngai, Terence P Speed, and Sandrine Dudoit. 2014.\n“Normalization of RNA-Seq Data Using Factor Analysis\nof Control Genes or Samples.” Nature Biotechnology 32\n(9): 896–902.\n\n\nSalzberg, Steven L. 2018. “Open Questions: How Many Genes Do We\nHave?” BMC Biology 16 (1): 94.\n\n\nSarkar, Abhishek, and Matthew Stephens. 2021. “Separating\nMeasurement and Expression Models Clarifies Confusion in Single Cell\nRNA-Seq Analysis.” Nature Genetics 53 (6):\n770–77.\n\n\nSaunders, Lauren M, Sanjay R Srivatsan, Madeleine Duran, Michael W\nDorrity, Brent Ewing, Tor H Linbo, Jay Shendure, et al. 2023.\n“Embryo-Scale Reverse Genetics at Single-Cell Resolution.”\nNature 623 (7988): 782–91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde,\nand Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable\nResponses to Sequential and Prolonged Treatment.” Cell\nSystems 15 (3): 213–26.\n\n\nSetty, Manu, Vaidotas Kiseliovas, Jacob Levine, Adam Gayoso, Linas\nMazutis, and Dana Pe’er. 2019. “Characterization of Cell Fate\nProbabilities in Single-Cell Data with Palantir.”\nNature Biotechnology 37 (4): 451–60.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to\nTailor Treatments.” Science Translational Medicine 9\n(408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the\nMolecular and Cellular Mechanisms of Aging.” Journal of\nInvestigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year\n2019: Single-Cell Multimodal Omics.” Nature Methods 17\n(1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith\nKnight. 2005. “Sparsity and Smoothness via the Fused\nLasso.” Journal of the Royal Statistical Society Series B:\nStatistical Methodology 67 (1): 91–108.\n\n\nTipping, Michael E, and Christopher M Bishop. 1999. “Probabilistic\nPrincipal Component Analysis.” Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) 61 (3):\n611–22.\n\n\nTownes, F William, Stephanie C Hicks, Martin J Aryee, and Rafael A\nIrizarry. 2019. “Feature Selection and Dimension Reduction for\nSingle-Cell RNA-seq Based on a Multinomial\nModel.” Genome Biology 20 (1): 1–16.\n\n\nTran, Hoa Thi Nhu, Kok Siong Ang, Marion Chevrier, Xiaomeng Zhang,\nNicole Yee Shin Lee, Michelle Goh, and Jinmiao Chen. 2020. “A\nBenchmark of Batch-Effect Correction Methods for Single-Cell\nRNA Sequencing Data.” Genome Biology 21\n(1): 1–32.\n\n\nVan Dijk, David, Roshan Sharma, Juozas Nainys, Kristina Yim, Pooja\nKathail, Ambrose J Carr, Cassandra Burdziak, et al. 2018.\n“Recovering Gene Interactions from Single-Cell Data Using Data\nDiffusion.” Cell 174 (3): 716–29.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME\nBowdish. 2015. “An Introduction to Automated Flow Cytometry Gating\nTools and Their Implementation.” Frontiers in Immunology\n6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n.\n2015. “Microglial Cell Dysregulation in Brain Aging and\nNeurodegeneration.” Frontiers in Aging Neuroscience 7:\n124.\n\n\nWei, Lai, Derek Lee, Cheuk-Ting Law, Misty Shuo Zhang, Jialing Shen, Don\nWai-Ching Chin, Allen Zhang, et al. 2019. “Genome-Wide CRISPR/Cas9\nLibrary Screening Identified PHGDH as a Critical Driver for Sorafenib\nResistance in HCC.” Nature Communications 10 (1): 4681.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao\nZhu, et al. 2022. “Single-Cell Technologies: From Research to\nApplication.” The Innovation 3 (6).\n\n\nXi, Nan Miles, and Jingyi Jessica Li. 2021. “Benchmarking\nComputational Doublet-Detection Methods for Single-Cell RNA Sequencing\nData.” Cell Systems 12 (2): 176–94.\n\n\nXia, Lucy, Christy Lee, and Jingyi Jessica Li. 2024. “Statistical\nMethod scDEED for Detecting Dubious 2D Single-Cell Embeddings and\nOptimizing t-SNE and UMAP Hyperparameters.” Nature\nCommunications 15 (1): 1753.\n\n\nYazar, Seyhan, Jose Alquicira-Hernandez, Kristof Wing, Anne Senabouth, M\nGrace Gordon, Stacey Andersen, Qinyi Lu, et al. 2022. “Single-Cell\neQTL Mapping Identifies Cell Type-Specific\nGenetic Control of Autoimmune Disease.” Science 376\n(6589): eabf3041.\n\n\nYoung, Matthew D, and Sam Behjati. 2020. “SoupX Removes Ambient\nRNA Contamination from Droplet-Based Single-Cell RNA Sequencing\nData.” Gigascience 9 (12): giaa151.\n\n\nZhang, Yuqing, Giovanni Parmigiani, and W Evan Johnson. 2020.\n“ComBat-Seq: Batch Effect Adjustment for RNA-Seq Count\nData.” NAR Genomics and Bioinformatics 2 (3): lqaa078.\n\n\nZhao, Ruzhang, Jiuyao Lu, Weiqiang Zhou, Ni Zhao, and Hongkai Ji. 2024.\n“A Systematic Evaluation of Highly Variable Gene Selection Methods\nfor Single-Cell RNA-Sequencing.” bioRxiv, 2024–08.\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human\nBrain Development.” Nature Reviews Genetics 25 (1):\n26–45.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "intro.html#what-are-omics",
    "href": "intro.html#what-are-omics",
    "title": "2  Introduction",
    "section": "3.1 What are “omics”?",
    "text": "3.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n3.1.1 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 3.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 3.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 3.3.\n\n\n\n\n\n\n\n\nFigure 3.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#how-omics-comes-into-the-picture",
    "href": "intro.html#how-omics-comes-into-the-picture",
    "title": "2  Introduction",
    "section": "3.2 How “omics” comes into the picture",
    "text": "3.2 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 3.4 and Figure 3.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 3.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 3.7.\n\n\n\n\n\n\n\nFigure 3.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised—mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an ``average/typical’’ scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.\n\n\n\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "href": "intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "title": "2  Introduction",
    "section": "2.2 What do we hope to learn from single-cell data?",
    "text": "2.2 What do we hope to learn from single-cell data?\n\n2.2.1 Basic biology\nSingle-cell data offer a transformative lens to study the fundamental processes of life at unparalleled resolution. Unlike bulk data, which averages signals across populations of cells, single-cell technologies allow researchers to examine the diversity and complexity of individual cells within a tissue or organism. This level of detail provides insights into key biological phenomena, such as cellular differentiation during development, the plasticity of cell states in response to environmental cues, and the organization of complex tissues. For instance, single-cell RNA-sequencing (scRNA-seq) has uncovered new cell types in the brain and immune system, challenging traditional classifications and offering a more nuanced understanding of cellular identities and functions. These insights are essential for constructing more accurate models of how life operates at a cellular level.\nMany of the examples shown in Figs. Figure 2.1–Figure 2.2 are basic biology questions. Two additional examples are shown in Figure 2.8 and Figure 2.9.\n\n\n\n\n\n\n\nFigure 2.8: How do cancer cells survive successive therapies? (Schaff et al. 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: What deteriorates in a cell as it ages (Silva and Schumacher 2021)\n\n\n\n\n\n\n2.2.2 Benchside-to-bedside applications\nSingle-cell data have enormous potential to revolutionize clinical practice by bridging molecular biology and medicine. After learning basic biology, the next stage is to use our newfound understanding to advance treatments. (We typically call this translation research, to denote translating our basic biology knowledge to therapeutic improvements.) This is also called benchside (for the wet-bench, i.e. laboratory setting) to bedside (for the hospital setting).\nBy mapping cellular heterogeneity in diseased and healthy tissues, researchers can identify specific cell populations driving disease progression and therapeutic resistance. For example, in cancer, single-cell analyses have uncovered rare tumor subclones that evade treatment, providing critical targets for drug development. Similarly, in autoimmune diseases like rheumatoid arthritis, single-cell profiling of synovial tissues has identified inflammatory cell states that correlate with disease severity and treatment response. Beyond diagnostics, this technology enables precision medicine by tailoring treatments to the molecular profiles of individual patients. As single-cell approaches continue to evolve, they are poised to refine drug-discovery pipelines, improve vaccine design, and ultimately transform how diseases are diagnosed and treated.\nThere are a few ways this typically happens. Figure 2.10 shows one example, where single-cell research helps identify the specific cell types and specific edits needed to improve cellular function. Figure 2.11 shows another example, where understanding the cellular functions, we can improve how conventional methods can be used to measure more accurate biomarkers. Cancer research has been (by far) the biggest beneficiary of single-cell research, and Figure 2.12 illustrates how single-cell improves cancer therapies.\n\n\n\n\n\n\n\nFigure 2.10: There are current single-cell therapies that involve extracting blood from a donor, altering the cells outside the body, and then infusing the altered blood back into the donor. (Source: https://www.cancer.gov/news-events/cancer-currents-blog/2020/crispr-cancer-research-treatment)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: After learning the causal mechanisms for specific cell types for Alzheimer’s disease that can be detected from blood draws, we can refine existing biomarkers to monitor Alzheimer’s. (Hansson et al. 2023)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Example of how insights from single-cell studies transform cancer therapies. (Gohil et al. 2021)\n\n\n\n\n\n\n\n2.2.3 What existed prior to single-cell data?\nWestern blots and flow cytometry.\nBefore the advent of single-cell technologies, biological research relied heavily on methods like western blots and flow cytometry to study cells and molecules. Western blotting (shown in Figure 2.13 and Figure 2.14), developed in the late 1970 s, enabled researchers to detect and quantify specific proteins in a sample, providing insights into cellular pathways and protein-expression levels. However, this technique required lysing entire tissues or cell populations, averaging the signals from thousands or millions of cells. Flow cytometry (shown in Figure 2.15 and Figure 2.16), emerging in the 1980s, represented a major step forward by allowing researchers to analyse individual cells’ physical and chemical characteristics in suspension. While flow cytometry offered single-cell resolution, it was limited to analysing predefined markers and could not capture the full complexity of cellular states or gene expression.\n\n\n\n\n\n\n\nFigure 2.13: Illustration of the Western-blot technique. (Source: https://www.bio-rad.com/en-us/applications-technologies/western-blotting-electrophoresis-techniques)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Example Western-blot data. (Kepchia et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Illustration of flow-cytometry technique. (Source: https://www.streck.com/blog/principles-of-flow-cytometry/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: Example flow-cytometry data. (Verschoor et al. 2015)\n\n\n\n\nMicroarray technology.\nThe 1990s marked the rise of microarray technology (shown in Figure 2.17 and Figure 2.18), which allowed scientists to measure the expression levels of thousands of genes simultaneously. Microarrays revolutionised transcriptomics by enabling high-throughput studies of gene activity in various conditions, tissues, and diseases. Despite its transformative impact, microarray analysis was fundamentally a bulk method, averaging signals across all cells in a sample. It is also based on light intensity, which can be tricky to extract consistently. These limitations obscured cellular heterogeneity, especially in complex tissues where distinct cell types or states contribute uniquely to biological processes or disease mechanisms.\n\n\n\n\n\n\n\nFigure 2.17: Illustration of microarray technique. (Source: https://www.onlinebiologynotes.com/dna-microarray-principle-types-and-steps-involved-in-cdna-microarrays/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.18: Example microarray image prior to quantification. (Source: https://online.stat.psu.edu/stat555/node/28/)\n\n\n\n\n\nRemark (Personal opinion: the close history between microarrays and high-dimensional statistics).\nHistorically, the rise of microarray data spurred advances in high-dimensional statistics — e.g. the Lasso (Tibshirani et al. 2005), gene-expression classification for leukaemia (Golub et al. 1999), and empirical-Bayes multiple testing (Efron and Tibshirani 2002).\n\nBulk sequencing technologies.\nIn the 2000s, bulk sequencing technologies for DNA and RNA emerged, further advancing the study of genomes and transcriptomes. RNA-sequencing (RNA-seq) became a powerful tool for capturing the entire transcriptome with greater accuracy and dynamic range than microarrays. Similarly, DNA sequencing enabled comprehensive studies of genetic variation, from point mutations to structural alterations. However, like microarrays, bulk sequencing aggregated signals across many cells, masking rare cell populations and the heterogeneity critical to understanding dynamic processes such as tumour evolution or immune responses. These bulk techniques laid the groundwork for single-cell methods by driving innovations in high-throughput sequencing and data analysis, which would later be adapted for single-cell resolution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "href": "intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "title": "2  Introduction",
    "section": "2.3 When did single-cell data become popular, and how has the technology advanced?",
    "text": "2.3 When did single-cell data become popular, and how has the technology advanced?\n\n2.3.1 The rise of single-cell data\nSingle-cell data began gaining prominence in the early 2010s, fuelled by advances in microfluidics and next-generation sequencing. Single-cell RNA-sequencing (scRNA-seq), pioneered around 2009 – 2011, was among the first methods to achieve widespread adoption. It enabled measurement of gene expression in individual cells, uncovering heterogeneity that bulk analyses masked. Early applications revealed new cell types and states, reshaped our understanding of development, and identified rare populations in cancers and neurodegenerative disorders. Popularity grew as throughput increased, costs fell, and workflows became standardised.\nFigure 2.19 and Figure 2.20 contrasts bulk and single-cell sequencing. Figure 2.21 shows how single-cell data tease apart different sources of heterogeneity.\n\n\n\n\n\n\n\nFigure 2.19: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.21: Single-cell insights disentangle underlying biological mechanisms. (Lei et al. 2021)\n\n\n\n\n\n\n2.3.2 Expansion into other omics and spatial technologies\nBuilding on single-cell transcriptomics, the field rapidly expanded into other omics. Single-cell proteomics allows detailed analysis of protein expression and signalling pathways. Single-cell ATAC-seq profiles chromatin accessibility; Hi-C and related methods reveal 3D genome architecture. Spatial transcriptomics connects gene expression with tissue context. CRISPR-based single-cell screens enable high-throughput perturbations, and lineage-tracing barcodes add a temporal dimension, charting cell ancestry in development and disease. Together, these advances transformed single-cell biology into a multi-dimensional, integrative discipline.\nMany of these technologies appear in Figure 2.22.\n\n\n\n\n\n\n\nFigure 2.22: Illustration (circa 2020) of technologies that pair various omics at single-cell resolution. (Teichmann and Efremova 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "href": "intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "title": "2  Introduction",
    "section": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?",
    "text": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?\nCell biology is vast – especially for students trained primarily in statistics or biostatistics. Many disciplines intersect with cell biology. For example:\n\nStatistics / Biostatistics – statistical models for complex biological processes; translation between maths and biology\nComputational biology – scalable computation, leveraging public data\nBioinformatics / Genetics – tools that draw on large‐consortium resources\nEpidemiology – population-level data and policy recommendations\nBioengineering – new laboratory technologies for cheaper/faster measurement or imaging\nBiology – mechanistic studies in model organisms\nBiochemistry / Molecular biology – structure, function, interactions of specific molecules\nWet-bench medicine – disease mechanisms via tissues, models, cell lines\nClinical-facing medicine – patient treatment and real-world sample collection\nPharmacology – integrating evidence to design new drugs and therapies\n\nGiven so many players, what does a biostatistician contribute?\n\n2.4.1 How a biostatistician perceives the world\nGive me a concrete (ideally cleaned) dataset: the larger the better—and I will analyse it from many angles.\nCausality: A mathematically stricter notion than correlation, usually via (1) counterfactual reasoning, or (2) a directed-acyclic-graph picture of how variables relate.\n\nRemark (Personal opinion: statistical causality for cell biology is extremely difficult). Obstacles: (i) tracking the same cell over time is impossible because sequencing lyses it; (ii) longitudinal human tissue samples are rare. Strong modelling assumptions can help but must withstand biological scrutiny. The bottleneck is often data, not maths—ambitious statisticians who learn enough biology still have a fighting chance.\n\n\nRemark (Single-cell methods are largely an “associative” world): Most single-cell analyses discover mechanisms that are statistically correlational; the causal proof comes from experiments and biology.\n\nThe research inquiry starts and ends with a method: (How to integrate modalities? learn a gene-regulatory network? perform valid post-clustering tests?) Start with a statistical model and a parameter of interest \\(\\theta^*\\), then typically:\n\nDevelop a novel estimator of \\(\\theta^*\\), explaining why current methods fail (e.g. lack robustness, accuracy, power, or are heuristic). Focus on statistical logic: A clear, simple mathematical intuition should show the gap and how the new method fills it.\nProve theorems showing the estimate \\(\\hat\\theta\\) converges to \\(\\theta^*\\) under stated assumptions. Focus on consistency & convergence: More data should provably yield more accurate results (often the highlight of a statistics paper).\nSimulations demonstrating that when the true \\(\\theta^*\\) is known, \\(\\hat\\theta\\) beats competing estimators across many settings. Illustration via benchmarking: Empirically recover the correct answer more often than existing methods.\nReal-data demonstration showing results align with known biology or provide biologically sensible new insights. Focus on practicality: The method must work in real scenarios mirroring its target audience.\n\nMindset: deliver a reliable tool that others can trust as-is. Human validations are often impractical; guard-rails and diagnostics are vital.\nWhy biostatisticians need wet-lab biologists / clinicians: We rarely generate data ourselves, so collaborators supply (i) exciting data with novel questions, (ii) biological context for sensible assumptions, and (iii) experimental validation of statistical findings.\n\n\n2.4.2 How a wet-lab biologist / clinician perceives the world\nExperiments, experiments, experiments: Carefully controlled—even if small—to make downstream analysis straightforward.\nCausality comes from a chain of experiments. Suppose we study a gene’s role in disease:\n\nTemporal evidence, such as change in gene expression preceding a change in cell phenotype. A causal mechanism should occur before the phenotype.\nBiological logic providing explanation of the underlying mechanism (binding factors, protein function, evolutionary rationale, etc.). For example, there must be a coherent pathway from gene → protein → phenotype.\nUniversality of how the described association persists across cell lines or organisms. A causal mechanism should be discoverable in other systems (extent depends on how general the logic is). This is offten the highlight of a biology paper.\nValidation, such as knocking out the gene alters the outcome, whereas similar genes do not. For example, perturbing the specific gene (not its close counterparts) changes the outcome.\n\nInquiry starts and ends with a biological hypothesis: Large intellectual effort goes into proposing explanations and designing experiments to rule them in or out.\nMindset: Assemble overwhelming evidence for a mechanism, combining careful experiments and biological logic.\nWhy wet-lab scientists need biostatisticians: Data are now complex and plentiful; exhaustive experiments for every hypothesis are infeasible. Statistical methods can (i) account for data & biological complexity and (ii) prioritise hypotheses worth experimental investment.\n\nSo how does a biostatistician develop computational / statistical methods for cell biology?\n\n\nBiological context – What is the biological system and the “north-star” question? Which premises are accepted, which ones are to be tested, and why is it important to understand this mechanism better?\nTechnology, experiment, data – How are data collected and why did you choose this particular {technology, experiment, data} trio? What technical artefacts arise?\nBoundaries of current tools – Simple analyses first: what “breaks” in existing workflows? Is there preliminary evidence a new computational method would do better?\nStatistical model – What is the insight that a different computational method could interrogate the biology better? (Here the statistics training begins.)\nDevelop a method & show robustness – Robustness can be defined numerically (i.e., noise tolerance) or biologically (i.e., applicable across contexts/environments). Often, the biological question you study lacks a ground truth, so validity arguments lean on biological logic.\nUncover new / refined biology – Does the method advance our biological understanding? How confident are we that findings generalise beyond the original {technology, experiment, data} trio? (This is usually the crown-jewel of your computational biology paper – it’s not necessary your specific analyses, but the potential that your tool can be used on other biological studies beyond what you’ve demonstrated your method on.)\n\n\n\n\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes Methods and False Discovery Rates for Microarrays.” Genetic Epidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and Kenneth J Livak. 2021. “Applying High-Dimensional Single-Cell Technologies to the Analysis of Cancer Immunotherapy.” Nature Reviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023. “Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and Trials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N Shokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse Proteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease Brain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun Yu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in Cancer Research: Progress and Perspectives.” Journal of Hematology & Oncology 14 (1): 91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde, and Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable Responses to Sequential and Prolonged Treatment.” Cell Systems 15 (3): 213–26.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to Tailor Treatments.” Science Translational Medicine 9 (408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the Molecular and Cellular Mechanisms of Aging.” Journal of Investigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year 2019: Single-Cell Multimodal Omics.” Nature Methods 17 (1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. “Sparsity and Smoothness via the Fused Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME Bowdish. 2015. “An Introduction to Automated Flow Cytometry Gating Tools and Their Implementation.” Frontiers in Immunology 6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-study-cell-biology-in-public-health",
    "href": "intro.html#why-study-cell-biology-in-public-health",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n\n2.1.2 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 2.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 2.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)\n\n\n\n\n\n\n2.1.3 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 2.4 and Figure 2.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 2.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised – its instead mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an “average/typical” scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html",
    "href": "chapter1_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why study cell biology in public health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#why-study-cell-biology-in-public-health",
    "href": "chapter1_intro.html#why-study-cell-biology-in-public-health",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n\n2.1.2 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 2.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 2.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)\n\n\n\n\n\n\n2.1.3 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 2.4 and Figure 2.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 2.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised – its instead mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an “average/typical” scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "href": "chapter1_intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "title": "2  Introduction",
    "section": "2.2 What do we hope to learn from single-cell data?",
    "text": "2.2 What do we hope to learn from single-cell data?\n\n2.2.1 Basic biology\nSingle-cell data offer a transformative lens to study the fundamental processes of life at unparalleled resolution. Unlike bulk data, which averages signals across populations of cells, single-cell technologies allow researchers to examine the diversity and complexity of individual cells within a tissue or organism. This level of detail provides insights into key biological phenomena, such as cellular differentiation during development, the plasticity of cell states in response to environmental cues, and the organization of complex tissues. For instance, single-cell RNA-sequencing (scRNA-seq) has uncovered new cell types in the brain and immune system, challenging traditional classifications and offering a more nuanced understanding of cellular identities and functions. These insights are essential for constructing more accurate models of how life operates at a cellular level.\nMany of the examples shown in Figs. Figure 2.1–Figure 2.2 are basic biology questions. Two additional examples are shown in Figure 2.8 and Figure 2.9.\n\n\n\n\n\n\n\nFigure 2.8: How do cancer cells survive successive therapies? (Schaff et al. 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: What deteriorates in a cell as it ages (Silva and Schumacher 2021)\n\n\n\n\n\n\n2.2.2 Benchside-to-bedside applications\nSingle-cell data have enormous potential to revolutionize clinical practice by bridging molecular biology and medicine. After learning basic biology, the next stage is to use our newfound understanding to advance treatments. (We typically call this translation research, to denote translating our basic biology knowledge to therapeutic improvements.) This is also called benchside (for the wet-bench, i.e. laboratory setting) to bedside (for the hospital setting).\nBy mapping cellular heterogeneity in diseased and healthy tissues, researchers can identify specific cell populations driving disease progression and therapeutic resistance. For example, in cancer, single-cell analyses have uncovered rare tumor subclones that evade treatment, providing critical targets for drug development. Similarly, in autoimmune diseases like rheumatoid arthritis, single-cell profiling of synovial tissues has identified inflammatory cell states that correlate with disease severity and treatment response. Beyond diagnostics, this technology enables precision medicine by tailoring treatments to the molecular profiles of individual patients. As single-cell approaches continue to evolve, they are poised to refine drug-discovery pipelines, improve vaccine design, and ultimately transform how diseases are diagnosed and treated.\nThere are a few ways this typically happens. Figure 2.10 shows one example, where single-cell research helps identify the specific cell types and specific edits needed to improve cellular function. Figure 2.11 shows another example, where understanding the cellular functions, we can improve how conventional methods can be used to measure more accurate biomarkers. Cancer research has been (by far) the biggest beneficiary of single-cell research, and Figure 2.12 illustrates how single-cell improves cancer therapies.\n\n\n\n\n\n\n\nFigure 2.10: There are current single-cell therapies that involve extracting blood from a donor, altering the cells outside the body, and then infusing the altered blood back into the donor. (Source: https://www.cancer.gov/news-events/cancer-currents-blog/2020/crispr-cancer-research-treatment)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: After learning the causal mechanisms for specific cell types for Alzheimer’s disease that can be detected from blood draws, we can refine existing biomarkers to monitor Alzheimer’s. (Hansson et al. 2023)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Example of how insights from single-cell studies transform cancer therapies. (Gohil et al. 2021)\n\n\n\n\n\n\n\n2.2.3 What existed prior to single-cell data?\nWestern blots and flow cytometry.\nBefore the advent of single-cell technologies, biological research relied heavily on methods like western blots and flow cytometry to study cells and molecules. Western blotting (shown in Figure 2.13 and Figure 2.14), developed in the late 1970 s, enabled researchers to detect and quantify specific proteins in a sample, providing insights into cellular pathways and protein-expression levels. However, this technique required lysing entire tissues or cell populations, averaging the signals from thousands or millions of cells. Flow cytometry (shown in Figure 2.15 and Figure 2.16), emerging in the 1980s, represented a major step forward by allowing researchers to analyse individual cells’ physical and chemical characteristics in suspension. While flow cytometry offered single-cell resolution, it was limited to analysing predefined markers and could not capture the full complexity of cellular states or gene expression.\n\n\n\n\n\n\n\nFigure 2.13: Illustration of the Western-blot technique. (Source: https://www.bio-rad.com/en-us/applications-technologies/western-blotting-electrophoresis-techniques)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Example Western-blot data. (Kepchia et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Illustration of flow-cytometry technique. (Source: https://www.streck.com/blog/principles-of-flow-cytometry/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: Example flow-cytometry data. (Verschoor et al. 2015)\n\n\n\n\nMicroarray technology.\nThe 1990s marked the rise of microarray technology (shown in Figure 2.17 and Figure 2.18), which allowed scientists to measure the expression levels of thousands of genes simultaneously. Microarrays revolutionised transcriptomics by enabling high-throughput studies of gene activity in various conditions, tissues, and diseases. Despite its transformative impact, microarray analysis was fundamentally a bulk method, averaging signals across all cells in a sample. It is also based on light intensity, which can be tricky to extract consistently. These limitations obscured cellular heterogeneity, especially in complex tissues where distinct cell types or states contribute uniquely to biological processes or disease mechanisms.\n\n\n\n\n\n\n\nFigure 2.17: Illustration of microarray technique. (Source: https://www.onlinebiologynotes.com/dna-microarray-principle-types-and-steps-involved-in-cdna-microarrays/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.18: Example microarray image prior to quantification. (Source: https://online.stat.psu.edu/stat555/node/28/)\n\n\n\n\n\nRemark (Personal opinion: the close history between microarrays and high-dimensional statistics).\nHistorically, the rise of microarray data spurred advances in high-dimensional statistics — e.g. the Lasso (Tibshirani et al. 2005), gene-expression classification for leukaemia (Golub et al. 1999), and empirical-Bayes multiple testing (Efron and Tibshirani 2002).\n\nBulk sequencing technologies.\nIn the 2000s, bulk sequencing technologies for DNA and RNA emerged, further advancing the study of genomes and transcriptomes. RNA-sequencing (RNA-seq) became a powerful tool for capturing the entire transcriptome with greater accuracy and dynamic range than microarrays. Similarly, DNA sequencing enabled comprehensive studies of genetic variation, from point mutations to structural alterations. However, like microarrays, bulk sequencing aggregated signals across many cells, masking rare cell populations and the heterogeneity critical to understanding dynamic processes such as tumour evolution or immune responses. These bulk techniques laid the groundwork for single-cell methods by driving innovations in high-throughput sequencing and data analysis, which would later be adapted for single-cell resolution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "href": "chapter1_intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "title": "2  Introduction",
    "section": "2.3 When did single-cell data become popular, and how has the technology advanced?",
    "text": "2.3 When did single-cell data become popular, and how has the technology advanced?\n\n2.3.1 The rise of single-cell data\nSingle-cell data began gaining prominence in the early 2010s, fuelled by advances in microfluidics and next-generation sequencing. Single-cell RNA-sequencing (scRNA-seq), pioneered around 2009 – 2011, was among the first methods to achieve widespread adoption. It enabled measurement of gene expression in individual cells, uncovering heterogeneity that bulk analyses masked. Early applications revealed new cell types and states, reshaped our understanding of development, and identified rare populations in cancers and neurodegenerative disorders. Popularity grew as throughput increased, costs fell, and workflows became standardised.\nFigure 2.19 and Figure 2.20 contrasts bulk and single-cell sequencing. Figure 2.21 shows how single-cell data tease apart different sources of heterogeneity.\n\n\n\n\n\n\n\nFigure 2.19: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.21: Single-cell insights disentangle underlying biological mechanisms. (Lei et al. 2021)\n\n\n\n\n\n\n2.3.2 Expansion into other omics and spatial technologies\nBuilding on single-cell transcriptomics, the field rapidly expanded into other omics. Single-cell proteomics allows detailed analysis of protein expression and signalling pathways. Single-cell ATAC-seq profiles chromatin accessibility; Hi-C and related methods reveal 3D genome architecture. Spatial transcriptomics connects gene expression with tissue context. CRISPR-based single-cell screens enable high-throughput perturbations, and lineage-tracing barcodes add a temporal dimension, charting cell ancestry in development and disease. Together, these advances transformed single-cell biology into a multi-dimensional, integrative discipline.\nMany of these technologies appear in Figure 2.22.\n\n\n\n\n\n\n\nFigure 2.22: Illustration (circa 2020) of technologies that pair various omics at single-cell resolution. (Teichmann and Efremova 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "href": "chapter1_intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "title": "2  Introduction",
    "section": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?",
    "text": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?\nCell biology is vast – especially for students trained primarily in statistics or biostatistics. Many disciplines intersect with cell biology. For example:\n\nStatistics / Biostatistics – statistical models for complex biological processes; translation between maths and biology\nComputational biology – scalable computation, leveraging public data\nBioinformatics / Genetics – tools that draw on large‐consortium resources\nEpidemiology – population-level data and policy recommendations\nBioengineering – new laboratory technologies for cheaper/faster measurement or imaging\nBiology – mechanistic studies in model organisms\nBiochemistry / Molecular biology – structure, function, interactions of specific molecules\nWet-bench medicine – disease mechanisms via tissues, models, cell lines\nClinical-facing medicine – patient treatment and real-world sample collection\nPharmacology – integrating evidence to design new drugs and therapies\n\nGiven so many players, what does a biostatistician contribute?\n\n2.4.1 How a biostatistician perceives the world\nGive me a concrete (ideally cleaned) dataset: the larger the better—and I will analyse it from many angles.\nCausality: A mathematically stricter notion than correlation, usually via (1) counterfactual reasoning, or (2) a directed-acyclic-graph picture of how variables relate.\n\nRemark (Personal opinion: statistical causality for cell biology is extremely difficult). Obstacles: (i) tracking the same cell over time is impossible because sequencing lyses it; (ii) longitudinal human tissue samples are rare. Strong modelling assumptions can help but must withstand biological scrutiny. The bottleneck is often data, not maths—ambitious statisticians who learn enough biology still have a fighting chance.\n\n\nRemark (Single-cell methods are largely an “associative” world): Most single-cell analyses discover mechanisms that are statistically correlational; the causal proof comes from experiments and biology.\n\nThe research inquiry starts and ends with a method: (How to integrate modalities? learn a gene-regulatory network? perform valid post-clustering tests?) Start with a statistical model and a parameter of interest \\(\\theta^*\\), then typically:\n\nDevelop a novel estimator of \\(\\theta^*\\), explaining why current methods fail (e.g. lack robustness, accuracy, power, or are heuristic). Focus on statistical logic: A clear, simple mathematical intuition should show the gap and how the new method fills it.\nProve theorems showing the estimate \\(\\hat\\theta\\) converges to \\(\\theta^*\\) under stated assumptions. Focus on consistency & convergence: More data should provably yield more accurate results (often the highlight of a statistics paper).\nSimulations demonstrating that when the true \\(\\theta^*\\) is known, \\(\\hat\\theta\\) beats competing estimators across many settings. Illustration via benchmarking: Empirically recover the correct answer more often than existing methods.\nReal-data demonstration showing results align with known biology or provide biologically sensible new insights. Focus on practicality: The method must work in real scenarios mirroring its target audience.\n\nMindset: deliver a reliable tool that others can trust as-is. Human validations are often impractical; guard-rails and diagnostics are vital.\nWhy biostatisticians need wet-lab biologists / clinicians: We rarely generate data ourselves, so collaborators supply (i) exciting data with novel questions, (ii) biological context for sensible assumptions, and (iii) experimental validation of statistical findings.\n\n\n2.4.2 How a wet-lab biologist / clinician perceives the world\nExperiments, experiments, experiments: Carefully controlled—even if small—to make downstream analysis straightforward.\nCausality comes from a chain of experiments. Suppose we study a gene’s role in disease:\n\nTemporal evidence, such as change in gene expression preceding a change in cell phenotype. A causal mechanism should occur before the phenotype.\nBiological logic providing explanation of the underlying mechanism (binding factors, protein function, evolutionary rationale, etc.). For example, there must be a coherent pathway from gene → protein → phenotype.\nUniversality of how the described association persists across cell lines or organisms. A causal mechanism should be discoverable in other systems (extent depends on how general the logic is). This is offten the highlight of a biology paper.\nValidation, such as knocking out the gene alters the outcome, whereas similar genes do not. For example, perturbing the specific gene (not its close counterparts) changes the outcome.\n\nInquiry starts and ends with a biological hypothesis: Large intellectual effort goes into proposing explanations and designing experiments to rule them in or out.\nMindset: Assemble overwhelming evidence for a mechanism, combining careful experiments and biological logic.\nWhy wet-lab scientists need biostatisticians: Data are now complex and plentiful; exhaustive experiments for every hypothesis are infeasible. Statistical methods can (i) account for data & biological complexity and (ii) prioritise hypotheses worth experimental investment.\n\nSo how does a biostatistician develop computational / statistical methods for cell biology?\n\n\nBiological context – What is the biological system and the “north-star” question? Which premises are accepted, which ones are to be tested, and why is it important to understand this mechanism better?\nTechnology, experiment, data – How are data collected and why did you choose this particular {technology, experiment, data} trio? What technical artefacts arise?\nBoundaries of current tools – Simple analyses first: what “breaks” in existing workflows? Is there preliminary evidence a new computational method would do better?\nStatistical model – What is the insight that a different computational method could interrogate the biology better? (Here the statistics training begins.)\nDevelop a method & show robustness – Robustness can be defined numerically (i.e., noise tolerance) or biologically (i.e., applicable across contexts/environments). Often, the biological question you study lacks a ground truth, so validity arguments lean on biological logic.\nUncover new / refined biology – Does the method advance our biological understanding? How confident are we that findings generalise beyond the original {technology, experiment, data} trio? (This is usually the crown-jewel of your computational biology paper – it’s not necessary your specific analyses, but the potential that your tool can be used on other biological studies beyond what you’ve demonstrated your method on.)\n\n\n\n\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes Methods and False Discovery Rates for Microarrays.” Genetic Epidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and Kenneth J Livak. 2021. “Applying High-Dimensional Single-Cell Technologies to the Analysis of Cancer Immunotherapy.” Nature Reviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023. “Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and Trials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N Shokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse Proteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease Brain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun Yu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in Cancer Research: Progress and Perspectives.” Journal of Hematology & Oncology 14 (1): 91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde, and Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable Responses to Sequential and Prolonged Treatment.” Cell Systems 15 (3): 213–26.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to Tailor Treatments.” Science Translational Medicine 9 (408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the Molecular and Cellular Mechanisms of Aging.” Journal of Investigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year 2019: Single-Cell Multimodal Omics.” Nature Methods 17 (1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. “Sparsity and Smoothness via the Fused Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME Bowdish. 2015. “An Introduction to Automated Flow Cytometry Gating Tools and Their Implementation.” Frontiers in Immunology 6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html",
    "href": "chapter2_sequencing.html",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "3.1 The sparse count matrix and the negative binomial\nAt the heart of single-cell sequencing data lies the sparse count matrix, where rows represent genes, columns represent cells, and entries capture the number of sequencing reads (or molecules) detected for each gene in each cell. This matrix is typically sparse because most genes are not expressed in any given cell, leading to a prevalence of zeros. The data’s sparsity reflects both the biological reality of selective gene expression and technical limitations such as sequencing depth. To model the variability in these counts, the negative binomial distribution is often employed. This distribution accommodates overdispersion, where the observed variability in gene expression counts exceeds what would be expected under simpler models like the Poisson distribution. By accounting for both biological heterogeneity and technical noise, the negative binomial provides a robust framework for analyzing sparse single-cell data.\nLet \\(X \\in \\mathbb{R}^{n\\times p}\\) denote the single-cell data matrix with \\(n\\) cells (rows) and \\(p\\) features (columns). In a single-cell RNA-seq data matrix, the \\(p\\) features represent the \\(p\\) genes.\nHere are some basic statistics about these matrices (based mainly from my experience):\nBased on these observations, the negative binomial distribution is very commonly used.\nIllustration of data from https://satijalab.org/seurat/articles/pbmc3k_tutorial.html showing a snippet of the count matrix (left) or percentage of cells with non-zero counts for each gene (right). (satijalab_tutorial?) ::::\nThe negative binomial (NB) distribution is a widely used model in single-cell RNA-seq analysis due to its ability to handle overdispersion, which is common in gene expression data. Overdispersion occurs when the variance of the data exceeds the mean, a phenomenon that cannot be captured by the Poisson distribution. The NB distribution introduces an additional parameter to model this extra variability, making it more flexible for single-cell data.\nLet us focus on the count for cell \\(i\\) and gene \\(j\\), i.e., the value \\(X_{ij}\\). The probability mass function (pmf) of the NB distribution for a random variable ( X_{ij} ) is given by:\n\\[\nP(X_{ij} = k) = \\binom{k + r_j - 1}{k} p_{ij}^r (1 - p_{ij})^k, \\quad k = 0, 1, 2, \\ldots\n\\tag{3.1}\\]\nwhere \\(r_j \\&gt; 0\\) is the dispersion (or “overdispersion”) parameter and \\(p\\_{ij} \\in (0, 1)\\) is the probability of success. This is the “standard” parameterization, mentioned in https://en.wikipedia.org/wiki/Negative_binomial_distribution. This specific parameterization is actually not very commonly used.\nAlternatively, the NB distribution is most commonly parameterized in terms of the mean \\(\\mu_{ij}\\) and the dispersion parameter \\(r_j\\), which is often preferred in single-cell analysis:\n\\[\n\\text{Mean: } \\mu_{ij}, \\quad \\text{Variance: } \\mu_{ij} + \\frac{\\mu_{ij}^2}{r_j} = \\mu_{ij}\\left(1 + \\frac{\\mu_{ij}}{r_j}\\right).\n\\tag{3.2}\\]\n(Compare these relations to the Poisson distribution, where both the mean and variance would be \\(\\mu_{ij}\\).)\nTo relate equation Equation 3.1 to Equation 3.2, observe that we can derive,\n\\[\n\\mu_{ij} = \\frac{r_j (1 - p_{ij})}{p_{ij}}.\n\\]\nWe typically use a different overdispersion parameter \\(r_j\\) for each gene \\(j \\in \\{1,\\ldots,p\\}\\). Here, \\(r_j\\) controls the degree of overdispersion. When \\(r_j \\to \\infty\\), the variance approaches the mean, and the NB distribution converges to the Poisson distribution. For single-cell RNA-seq data, the introduction of \\(r_j\\) allows the model to capture both the biological variability between cells and technical noise, making it robust to the sparse and overdispersed nature of gene expression data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-sparse-count-matrix-and-the-negative-binomial",
    "href": "chapter2_sequencing.html#the-sparse-count-matrix-and-the-negative-binomial",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "For \\(n\\), we usually have 10,000 to 500,000 cells. These cells originate from a specific set of samples/donors/organisms1. See Figure 3.4 and Figure 3.5 for larger examples of single-cell datasets.\n\nFor \\(p\\), we usually have about 30,000 genes (but as you’ll see, we usually limit the analysis to 2000 to 5000 genes chosen from this full set).2\n\nIn a typical scRNA-seq matrix, more than 70% of the elements are exactly 0. (And among the remaining 30%, typically half are exactly 1. The maximum count can be in the hundreds, i.e., the distribution is extremely right-skewed.) This is illustrated in Figure 3.1 and Figure 3.2. We’ll see later in ?sec-scrnaseq_tech where these “counts” come from.\n\n\n\n\n\n\n\n\n\nFigure 3.1: Count matrix\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Histogram of count distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy would genes even have different overdispersions? We’re assuming in most statistical models that: 1) genes are modeled as a negative binomial random variable, and 2) genes have different overdispersion parameters \\(r_j\\). Why is this reasonable? Is there a biomolecular reason for this?\nWe draw upon Sarkar and Stephens (2021), explaining how the NB distribution is justified (both theoretically and empirically) for scRNA-seq data. After reading this paper, you’ll see that the overdispersion originates from the “biological noise” (i.e., the Gamma distribution). This differs from gene to gene because gene expression is subject to many aspects: transcriptomic bursting, the proximity between the gene and the promoter, the mRNA stability and degradation, copy number variation, the cell cycle stage, etc.\n\n\n3.1.0.1 How do you estimate the overdispersion parameter in practice?\nThe easiest way is to use MASS::glm.nb in R. However, this is very noisy for single-cell data (see Remark 3.1). Since we are estimate one dispersion parameter per gene, many methods use an Empirical Bayes framework to “smooth” the overdispersion parameters (Love, Huber, and Anders 2014). More sophisticated methods (such as SCTransform (Hafemeister and Satija 2019)) use the Bioconductor R package called glmGamPoi. Deep-learning methods simply incorporate estimating the dispersion parameter into the architecture and objective function, see scVI (Lopez et al. 2018).\nAlso, note that which we are assuming here that the overdispersion parameter \\(r_j\\) is shared across all the cells for gene \\(j\\), there are methods that also use different overdispersion parameters for different cell populations. See Chen et al. (2018) or the dispersion parameter in scVI (see https://docs.scvi-tools.org/en/stable/api/reference/scvi.model.SCVI.html#scvi.model.SCVI). In general, using different overdispersion parameters for the same gene across different cell populations is not common, so my suggestion is to: 1) have a diagnostic in mind on how would you know if need to use different overdispersion parameters for the same gene, and then 2) try analyzing the genes where each gene only has one overdispersion parameter and see if you have enough concrete evidence that such a model was too simplistic.\n\nPersonal opinion: My preferred parameterization The formulation Equation 3.2 by far is the most common way people write the NB distribution. We typically call \\(r_j\\) the “overdispersion” parameter, since the inclusion of \\(r_j\\) in our modeling is typically to denote that there is more variance than a Poisson. I personally don’t like it because I find it confusing that a larger \\(r_j\\) denotes a distribution with smaller variance. Hence, I usually write the variance as \\(\\mu_{ij}(1+\\alpha_j \\mu_{ij})\\) (where \\(\\alpha_j = 1/r_j\\)). This way, \\(\\alpha_j = 0\\) is a Poisson distribution. However, even though this makes more sense to me, it’s not commonly used. (It’s because this parameterization makes it confusing to work with the exponential-family distributions mathematically.)\nThe main takeaway is to always pay close attention to each paper’s NB parameterization. You’re looking for the mean-variance relation written somewhere in the paper to ensure you understand the author’s notation.\n\n\nRemark 3.1. What makes the negative binomial tricky Let me use an analogy that is based on the more familiar coin toss. Suppose 30 people each are given a weighted coin where the probability of heads is \\(10^{-9}\\) (basically, it’s impossible to get a heads). Everyone coerce and agree to a secret number \\(N\\), and everyone independently flips their own coin \\(N\\) times and records the number of heads. I (the moderator) do not know \\(N\\), but I get to see how many heads each person got during this experiment. My goal is to estimate \\(N\\). What makes this problem very difficult?\nIf \\(N\\) were 10, or 100, or maybe even 10,000, almost everyone would get 0 heads. That is, we cannot reliably estimate \\(N\\) since the log-likelihood function is very flat. The issue is because count data has a finite resolution — 0 is always the smallest possible value, and 1 is always the second smallest value. Hence, unlike continuous values (which have “infinite resolution” in some sense), non-negative integers “lose” information the smaller the range is.\n\nReturning the negative binomials, consider a simple coding example:\n\nset.seed(10)\nmu &lt;- 1\ntrue_overdispersion &lt;- 1000\ntrue_size &lt;- mu / (true_overdispersion - 1)\ndata &lt;- stats::rnbinom(1e5, size = true_size, mu = mu)\noverdispersion_vec &lt;- exp(seq(log(1), log(1e7), length.out = 100))\nlog_likelihood &lt;- sapply(overdispersion_vec, function(overdispersion) {\n  size &lt;- mu / (overdispersion - 1)\n  sum(dnbinom(data, size = size, mu = mu, log = T))\n})\nhist(data,\n     main = paste(\"% 0:\", round(length(which(data == 0)) / length(data) * 100, 2),\n                  \"\\nEmpirical mean:\", round(mean(data), 2),\n                  \"\\nEmpirical variance:\", round(var(data), 2))\n)\nhist(data[data != 0])\nplot(log10(overdispersion_vec), log_likelihood)\nplot(log10(overdispersion_vec[-c(1:5)]), log_likelihood[-c(1:5)])\n\n\n\n\n\n\n\n\nFigure 3.3: (Left) Histogram of a dataset where the true mean is 1 and the overdispersion (i.e., \\(\\alpha = 1/r\\)) is 1000, where almost all the values are 0. Almost 100% of the values are 0 among all 10,000 samples. (Right) The log-likelihood of the overdispersion parameter on the \\(\\log_{10}\\) scale.\n\n\n\n\n\n\n3.1.1 Remembering that these cells come from donors/tissues!\nThe \\(n\\) cells in our single-cell dataset originate from \\(m\\) tissues/samples/donors/etc. That is, the cells are stratified (or hierarchically organized). Based on your scientific question of interest, this is a consideration you might want your analysis to take into account. In general, each of the \\(n\\) cells in your single-cell dataset will have “metadata” (such as which sample the cell came from). See Figure 3.4 and Figure 3.5 as examples. We will see in Section 4.4.12 on how some methods explicitly take into account this “hierarchical” structure of the data. However, most methods that are intended for cell lines or genetically-controlled scenarios do not focus on this hierarchical structure (since all the cell-lines or organisms are nearly identical).\n\n\n\n\n\n\n\nFigure 3.4: 982 individuals, from which roughly 1200 cells were sequenced from each (from the blood) to give rise to roughly \\(n=1.27\\) million cells. (Yazar et al. 2022)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: 1,223 zebrafish embryos, from which roughly 500 cells were sequenced from each to give rise to roughly \\(n=1.25\\) million cells. (Saunders et al. 2023)\n\n\n\n\nTypically, you can expect that the cells in a single-cell analysis come from one of three categories:\n\nCell-lines (In vitro): Cells grown in controlled laboratory conditions, such as HeLa or HEK293, typically on a petri dish, are commonly used for experiments requiring consistency and ease of manipulation. These cells are ideal for studying specific pathways or drug responses in a simplified system.\nModel organisms (In vivo): Cells derived from model organisms (such as zebrafish, genetically engineered mice, yeast, specific plants, etc.) allow researchers to study conserved biological processes in a controlled, organismal context. These systems often provide genetic and developmental insights that are directly relevant to human biology.\nHumans (In vivo): Cells from human tissues or blood samples provide direct insights into human biology, disease mechanisms, and patient-specific variability. They are often used in translational research to bridge findings from cell lines and model organisms to clinical applications.\n\nOther categories, such as xenografts, organoids, and co-culture systems, also play critical roles in capturing specific aspects of cellular behavior and interactions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#footnotes",
    "href": "chapter2_sequencing.html#footnotes",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "You can have a simple hierarchical model in your head – each human donor contributes one (or more) tissue sample. Each tissue sample contains many cells.↩︎\nHow many genes are there in the human body? This is not a well-defined questions. If you want to only ask about protein-coding genes, there’s probably about 24,000 of them (Salzberg 2018). However, most gene callers (such as CellRanger https://www.10xgenomics.com/support/software/cell-ranger/latest, which is what we usually use for 10x single-cell data) also label genes that don’t translate. We’ll talk about this in Section 4.8.↩︎\nTypically, the feature selection step doesn’t use the normalized matrix even though normalization happens before feature selection. See https://github.com/satijalab/seurat/blob/ece572a/R/preprocessing.R#L3995. This is because it’s preferable to normalize using all the reads, even from genes you don’t actually include in your analysis.↩︎\nSee https://en.wikipedia.org/wiki/Low-rank_approximation.↩︎\nTypically, the feature selection step doesn’t use the normalized matrix even though normalization happens before feature selection. See https://github.com/satijalab/seurat/blob/ece572a/R/preprocessing.R#L3995. This is because it’s preferable to normalize using all the reads, even from genes you don’t actually include in your analysis.↩︎\nSee https://en.wikipedia.org/wiki/Low-rank_approximation.↩︎\nSometimes, people use the word “overcrowding” to refer to the fact that embedding methods might collapse nearby points too aggressively.↩︎\nSee Seurat’s recommendation in https://satijalab.org/seurat/articles/seurat5_integration.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html",
    "href": "chapter3_rna.html",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "4.1 Some important nouns and verbs",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#footnotes",
    "href": "chapter3_rna.html#footnotes",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "Technically, genomics (as a field) is a broader categorization and encapsulates genetics. But usually when people say they work on “genomics,” they are colloquially implying they work on biology that is not genetics (otherwise, they usually say they’re a geneticist).↩︎\nThis term is not very often used anymore. It was an umbrella label popularized in the mid-2000s. We instead typically refer to a more specific category of technology. For example, most of the data in this scRNA-seq chapter are referred to as “short-read 3′ single-cell RNA-sequencing.” (You could throw in “droplet-based” and “UMI” to be even more precise, but that’s usually not needed in most casual contexts.)↩︎\nSee https://cole-trapnell-lab.github.io/monocle3/docs/introduction/.↩︎\nSee https://www.qiagen.com/us/resources/faq?id=06a192c2-e72d-42e8-9b40-3171e1eb4cb8.↩︎\nFor more context about PCR, see: https://www.youtube.com/watch?v=zaXKQ70q4KQ.↩︎\nThis can be a bit confusing. When we refer to the “10x machine” (e.g., the 10x Chromium platform, as described at https://www.10xgenomics.com/products/universal-three-prime-gene-expression), we are typically talking about the steps involved in droplet formation, cDNA synthesis. PCR and library preparation are usually not done in the 10x machine itself. Also, the resulting libraries are not sequenced on the 10x machine itself. Instead, sequencing is performed on a separate platform, such as an Illumina machine (e.g., see https://www.illumina.com/systems/sequencing-platforms.html).↩︎\nSee https://kb.10xgenomics.com/hc/en-us/articles/360000939852-What-is-the-difference-between-Single-Cell-3-and-5-Gene-Expression-libraries and https://kb.10xgenomics.com/hc/en-us/articles/218137123-What-is-the-expected-size-range-for-amplified-cDNA for typical length of the cDNA fragment.↩︎\nJust to give you context, a “typical” gene is thousands of base pairs (it depends on if you’re talking about the transcript itself, with or without introns, etc.) See (Lopes et al. 2021; Grishkevich and Yanai 2014; Leung et al. 2021). So what “short-read 3’ RNA-sequencing” is referring to is that we only measure (roughly) 400 base pairs on the 3’ side of the gene.↩︎\nSee https://satijalab.org/costpercell (but most institutions publish their own rates. For example, see https://www.bumc.bu.edu/microarray/pricing/ (for BU) and https://cat.ucsf.edu/sequencing (UCSF) for typical rates. For students who are not familiar with wet-lab experiments, a rough ballpark sense of cost is that a “reasonable” setting (i.e., sequencing depth of 20,000 reads per cell across 10,000 cells) costs about $2000. This is usually for just one sample (i.e., one tissue for one donor), so as your experiment involves more tissues or more donors, you multiply $2000 by the total number of samples you’re thinking of. Remember, this is gets you a random set of 10,000 cells in the sample – your biological question might only care about a small cell population among these 10,000 cells.↩︎\nSee https://www.10xgenomics.com/support/software/cell-ranger/latest/algorithms-overview/cr-gex-algorithm on how CellRanger works. See https://www.10xgenomics.com/analysis-guides/quality-assessment-using-the-cell-ranger-web-summary for the typical output of CellRanger. Importantly, usually only 60%-70% of the reads get mapped to the transcriptome (i.e., “genes”). So remember: 1) not all the 20,000 expected reads get mapped to the transcriptome, and 2) these are reads before you account for UMI duplicates. A reasonable ballpark is that 20,000 reads yields 4,000 UMIs, so this means in your scRNA-seq matrix, the total reads per cell is roughly 4,000 (across roughly 20,000 genes). See https://cf.10xgenomics.com/samples/cell-exp/2.1.0/pbmc8k/pbmc8k_web_summary.html for an example, or https://www.10xgenomics.com/datasets for other examples.↩︎\nSee https://cole-trapnell-lab.github.io/monocle3/docs/introduction/.↩︎\nSee https://cole-trapnell-lab.github.io/monocle3/docs/introduction/.↩︎\nSee https://cole-trapnell-lab.github.io/monocle3/docs/introduction/.↩︎\nSee https://www.qiagen.com/us/resources/faq?id=06a192c2-e72d-42e8-9b40-3171e1eb4cb8.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#some-important-nouns-and-verbs",
    "href": "chapter3_rna.html#some-important-nouns-and-verbs",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "Sequencing (verb): The process of determining the order of nucleotides (A, T, C, G) in a DNA or RNA molecule, providing the primary structure of these biomolecules. Sequencing technologies have evolved to be high-throughput, enabling the analysis of entire genomes or transcriptomes.\nSequencing depth, read depth, library size (noun; all synonyms): These terms refer to the number of times a specific nucleotide or region of the genome is sequenced in an experiment. Greater depth provides more accurate detection of rare variants or lowly expressed genes but requires increased computational and financial resources.\nChromosome, DNA, RNA, protein (noun):\n\nChromosome: A large, organized structure of DNA and associated proteins that contains many genes and regulatory elements.\n\nDNA: The molecule that encodes genetic information in a double-helical structure.\n\nRNA: A single-stranded molecule transcribed from DNA that can act as a messenger (mRNA), a structural component (rRNA), or a regulator (e.g., miRNA). When we talk about scRNA-seq, we are usually referring to exclusively measuring mRNA.\n\nProtein: The functional biomolecule synthesized from RNA via translation, performing structural, enzymatic, and regulatory roles in cells.\n\nGenome vs. gene vs. intergenic region (noun):\n\nGenome: The complete set of DNA in an organism, encompassing all of its genetic material, including coding genes, non-coding regions, and regulatory elements. The genome is the blueprint that defines the biological potential of the organism.\n\nGene: A specific sequence within the genome that encodes a functional product, typically a protein or functional RNA. Genes include regions such as exons (coding sequences), introns (non-coding regions within a gene), and regulatory sequences (e.g., promoters and enhancers) that control gene expression. We will see more about the architecture of a gene in ?sec-genetics_basics.\n\nIntergenic region: The stretches of DNA between genes that do not directly code for proteins or RNA. Intergenic regions were once considered “junk DNA,” but they often contain regulatory elements, such as enhancers and silencers, that influence the expression of nearby or distant genes. These regions also play roles in chromatin organization and genome stability.\n\nGenetics vs. genomics (noun): Genetics typically focuses on the role of the DNA among large populations (of people, of species, etc.), while genomics can encapsulate any omic, and does not necessarily imply studies across a large population1.\n“Next generation sequencing” (noun)2: A collection of high-throughput technologies that allow for the parallel sequencing of millions of DNA or RNA molecules. It has revolutionized biology by enabling large-scale studies of genomes, transcriptomes, and epigenomes.\nRead fragment (noun): A short sequence of DNA or RNA produced as an output from high-throughput sequencing. Fragments are typically between 50 bp and 300 bp long, depending on the sequencing technology, and they represent segments of the original molecule being sequenced.\nReference genome (noun): A curated, complete assembly of the genomic sequence for a species, used as a template to align and interpret sequencing reads. It serves as a baseline for identifying genetic variations, such as mutations or structural changes, and for annotating functional elements.\nCoding genes vs. non-coding genes (noun):\n\nCoding genes: Genes that contain instructions for producing proteins. They are transcribed into mRNA, which is then translated into functional proteins that perform structural, enzymatic, or regulatory roles in cells.\n\nNon-coding genes: Genes that do not produce proteins but instead generate functional RNA molecules, such as rRNA, tRNA, miRNA, or lncRNA, which regulate gene expression, maintain genomic stability, or perform other cellular functions. Non-coding genes highlight the complexity of gene regulation and cellular processes beyond protein synthesis.\n\nEpigenetics vs. epigenomics (noun): Epigenetics studies modifications to DNA and histones (e.g., methylation, acetylation) that regulate gene expression without altering the DNA sequence. Epigenomics examines these modifications across the entire genome.\nTranscriptome (noun): The complete set of RNA transcripts expressed in a cell or tissue at a given time, reflecting dynamic gene activity.\nProteome (noun): The full complement of proteins expressed in a cell, tissue, or organism, representing functional output.\nSingle-cell sequencing (noun): Sequencing technologies applied at the resolution of individual cells, allowing for the study of heterogeneity in gene expression, epigenetics, or genetic variation across cell populations.\nBulk sequencing (noun): Sequencing technologies that aggregate material (e.g., RNA, DNA) from many cells, providing an average profile of the population but masking individual cell variability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html",
    "href": "chapter4_protein.html",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "5.1 Review of the central dogma\nThe central dogma of molecular biology outlines the flow of genetic information within a cell: DNA is transcribed into RNA, and RNA is then translated into protein (see Figure 5.1). Specifically, coding genes in DNA are transcribed into messenger RNA (mRNA), which serves as a template for protein synthesis. During translation, mRNA sequences are read in sets of three nucleotides, known as codons, each corresponding to a specific amino acid. These amino acids are then linked together to form proteins, which carry out a vast array of functions within the cell. While this process provides a foundational framework, it is a dramatic over-simplification. As we’ll explore later in the course, the correlation between a gene and its corresponding protein levels is often surprisingly low, highlighting the complexity of gene expression regulation.\nUnderstanding proteins is crucial because they are the primary effectors of cellular function. Most cellular activities — whether structural, enzymatic, or signaling—are mediated by proteins. While RNA intermediates, such as mRNA, play important roles in carrying genetic information, the majority of RNA fragments never leave the cell, with a few exceptions like extracellular RNA in communication. Proteins, however, directly influence both intracellular processes and extracellular interactions. Given the weak correlation between genes and proteins and the central role proteins play in biological function, studying proteins arguably provides a more direct and meaningful insight into cellular and organismal behavior. This dual perspective on the central dogma will frame much of our exploration in this course.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#review-of-the-central-dogma",
    "href": "chapter4_protein.html#review-of-the-central-dogma",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "Figure 5.1: What I call the ``AP Biology textbook’’ figure. Screenshot taken from https://www.slideshare.net/slideshow/lecture-on-dna-to-proteins-the-central-dogma-of-molecular-biology/38811421.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: From (buccitelli2020mrnas?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: From (buccitelli2020mrnas?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#other-ways-to-study-proteins-that-were-not-going-to-discuss-here",
    "href": "chapter4_protein.html#other-ways-to-study-proteins-that-were-not-going-to-discuss-here",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.2 Other ways to study proteins that we’re not going to discuss here",
    "text": "5.2 Other ways to study proteins that we’re not going to discuss here\n\n5.2.1 So You Heard About AlphaFold…\nAlphaFold (see Figure 5.4) represents a revolutionary advancement in computational biology, designed to predict the three-dimensional structure of proteins from their amino acid sequences2. Historically, determining protein shapes required experimental techniques like X-ray crystallography, cryo-electron microscopy (EM) (see Figure 5.5), or nuclear magnetic resonance (NMR), which are resource-intensive and time-consuming. AlphaFold uses deep learning and structural biology insights to achieve high accuracy.\nHowever, significant challenges remain. There are still many open questions on how specific genetic modifications impact protein folding, how proteins dynamically change their conformation, or how they interact with other molecules such as DNA or other proteins. Additionally, ongoing developments in using large language models are showing promise in predicting not only shape but also potential functions directly from amino acid sequences.\n\n\n\n\n\n\n\nFigure 5.4: From (jumper2021highly?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: From https://myscope.training/CRYO_Introducing_Single_Particle_Analysis, as an example of what cryo-EM data “looks like,” just to give you a brief glimpse on how people study protein structure.\n\n\n\n\n\n\n5.2.2 Other Methods: Flow Cytometry, Spatial Proteomics, and FISH\nWhile AlphaFold focuses on protein structure, methods like flow cytometry and spatial proteomics explore proteins in their functional and cellular contexts. Flow cytometry, sometimes considered the “original” single-cell data method, measures the expression of surface and intracellular proteins across thousands of cells, providing rich insights into cellular heterogeneity. Spatial proteomics and techniques like fluorescence in situ hybridization (FISH) take this further by localizing proteins and RNA within tissue contexts, enabling researchers to map molecular interactions in their native environments. These approaches highlight the versatility of protein studies, from understanding their structure to dissecting their function and distribution in complex systems. While not the focus of this course, these methods are invaluable in expanding our understanding of proteins and their roles in biology.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#footnotes",
    "href": "chapter4_protein.html#footnotes",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "Proteins typically degrade much slower than mRNA fragments. See https://book.bionumbers.org/how-fast-do-rnas-and-proteins-degrade. For this reason, you might hypothesize that “cellular memory” is stored via proteins, not mRNA.↩︎\nSee https://www.youtube.com/watch?v=P_fHJIYENdI for a fun YouTube video for more about this.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html",
    "href": "chapter5_epigenetics.html",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "6.1 Primer on the genome, epigenetics, and enhancers\nThe genome is the complete set of DNA within an organism, encoding the instructions for life. While every cell in an organism typically contains the same genome, different cells exhibit distinct phenotypes and functions. This diversity arises not from changes in the underlying DNA sequence but from epigenetic regulation—heritable modifications that influence gene expression without altering the DNA itself. Epigenetics includes processes like DNA methylation, histone modification, and chromatin accessibility, all of which contribute to the dynamic regulation of gene activity in response to developmental cues and environmental signals.\nEnhancers play a critical role in this regulatory landscape. These are DNA sequences that, while not coding for proteins themselves, can dramatically increase the transcription of target genes. Enhancers act by binding specific transcription factors, proteins that recognize and attach to DNA sequences to regulate gene expression. Some transcription factors require assistance from chaperone proteins, which ensure their proper folding and functionality, or pioneer proteins, which can access and open tightly packed chromatin to allow other factors to bind. This interplay highlights the complexity of the regulatory machinery that governs cellular function. See Figure 6.1 and Figure 6.2 to appreciate how complex this machinery is.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#primer-on-the-genome-epigenetics-and-enhancers",
    "href": "chapter5_epigenetics.html#primer-on-the-genome-epigenetics-and-enhancers",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "Figure 6.1: From Ito et al. (2022).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: From Claringbould and Zaugg (2021).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#the-zoo-of-epigenetic-modalities",
    "href": "chapter5_epigenetics.html#the-zoo-of-epigenetic-modalities",
    "title": "6  Single-cell epigenetics",
    "section": "6.2 The zoo of epigenetic modalities",
    "text": "6.2 The zoo of epigenetic modalities\nEpigenetics encompasses a vast array of molecular mechanisms that regulate gene expression without altering the underlying DNA sequence. These mechanisms include modifications to DNA, RNA, chromatin, and the spatial organization of the genome, collectively forming a complex regulatory landscape. Below are some of the key modalities studied in epigenetics:\n\nDNA Accessibility: Techniques like ATAC-seq and DNase-seq measure how accessible DNA is to transcription factors and other regulatory proteins. Accessible regions often overlap with promoters and enhancers, providing critical insights into gene regulation. (This is what we’ll focus on this chapter.)\nDNA Methylation1: This modification, typically at cytosines in CpG dinucleotides, is a key epigenetic mark associated with gene silencing. Tools like bisulfite sequencing are used to map methylation patterns across the genome, revealing their roles in development and disease. See Figure 6.3.\n\n\n\n\n\n\n\n\nFigure 6.3: From (gauba2021immunomodulation?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: From (bruggeman2008using?).\n\n\n\n\n\nHi-C and Genome Organization2: Hi-C measures chromatin interactions to reveal the three-dimensional structure of the genome. It uncovers features like topologically associating domains (TADs) and enhancer-promoter loops, which are crucial for understanding how spatial organization influences gene regulation. See Figure 6.5.\n\n\n\n\n\n\n\n\nFigure 6.5: From (ea2015contribution?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: From (liu2021seeing?).\n\n\n\n\n\nHistone Modifications3: Post-translational modifications, such as acetylation, methylation, and phosphorylation, occur on histone proteins and regulate chromatin structure. Techniques like ChIP-seq and the newer Cut & Tag method are used to map these modifications and their role in gene expression at the bulk level. See Figure 6.7.\n\n\n\n\n\n\n\n\nFigure 6.7: From https://opened.cuny.edu/courseware/lesson/684/student/?section=2.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.8: From (steinbach2017pten?). (Note: This is an example of the “canonical” functions of common histone modifications. This is by no means exhaustive and guaranteed to hold true for all biological systems.)\n\n\n\n\nSome of you might be interested: Personally, I think one of the fascinating concepts based on histone modifications is bivalent chromatin (essentially, chromatin that is wrapped in such a way that is simultaneously activated and silenced), see Figure 6.9. It’s a particularly curious phenomenon that entire labs dedicate themselves to studying. See (blanco2020bivalent?) for an overview why this mechanism might be “beneficial.”\n\n\n\n\n\n\n\nFigure 6.9: From (macrae2023regulation?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.10: From (glancy2024bivalent?).\n\n\n\n\n\nRNA Modifications (m6A and Pseudouridine): Modifications like N6-methyladenosine (m6A) and pseudouridine occur on RNA molecules and are involved in processes like splicing, translation, and mRNA decay. See Figure 6.11 for what pseudouridine is, i.e., “a rotation of the uridine molecule.” These modifications add an epitranscriptomic layer to gene regulation.\n\n\n\n\n\n\n\n\nFigure 6.11: From (hamma2006pseudouridine?) about pseudouridine.\n\n\n\n\n\nUntranslated Regions (UTRs): Before talking about UTRs, it’s probably good to review what an mRNA fragment “looks like” at the different stages of transcription and translation, see Figure 6.12. The 5′ and 3′ UTRs contain regulatory elements that influence mRNA stability, localization, and translation efficiency. The 3′ UTR, in particular, serves as a binding platform for RNA-binding proteins and microRNAs, providing an additional layer of post-transcriptional gene regulation.\n\n\n\n\n\n\n\n\nFigure 6.12: From (niazi2023rna?). Notice the splicing (i.e., “removal” of the introns – see Alternative Splicing below) and that the protein-coding region is not the only part of the mRNA transcript – there are also the UTRs and poly-A tail. NOTE: The transcription start site (TSS) is not the same place as the promoter. The promoter is usually an un-transcribed region upstream of the TSS, while the TSS is where transcription actually starts.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.13: From (mignone2002untranslated?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.14: From https://www.cambio.co.uk/9/257/applications/methods/in-vitro-transcription/mrna-therapeutics/.\n\n\n\n\n\nAlternative Splicing: This post-transcriptional process generates multiple mRNA isoforms from the same gene, expanding the proteomic diversity, see Figure 6.15. This is a combinatorial explosion of isoforms. While not traditionally classified as epigenetic, splicing often intersects with chromatin modifications and RNA-binding proteins, blurring the lines between transcriptional and post-transcriptional regulation.\n\n\n\n\n\n\n\n\nFigure 6.15: From (chen2015alternative?).\n\n\n\n\n\nAlternative Polyadenylation (APA): APA generates transcript isoforms with different 3′ ends, affecting mRNA stability, localization, and translation, see Figure 6.16. By selecting distinct cleavage sites, APA alters the 3′ UTR length without affecting the poly-A tail itself, thereby modulating interactions with regulatory factors such as microRNAs and RNA-binding proteins.\n\n\n\n\n\n\n\n\nFigure 6.16: From Li et al. (2022).\n\n\n\n\nEfforts like the ENCODE project (https://pmc.ncbi.nlm.nih.gov/articles/PMC7061942/) have systematically mapped these modalities across cell types and tissues, creating a comprehensive resource for understanding genome function. ENCODE has provided invaluable datasets on DNA accessibility, histone modifications, and RNA-binding proteins, enabling researchers to uncover how epigenetic and transcriptomic layers work together to drive cellular processes.\nFinally, tools like MPRA (Massively Parallel Reporter Assays) are revolutionizing how we study enhancers and regulatory sequences. MPRA allows researchers to test thousands of DNA fragments for their regulatory activity, providing functional validation for epigenetic marks. This growing zoo of modalities continues to expand our understanding of how the genome is dynamically regulated in health and disease.\nSee Figure 6.17 for some courageous figures that try to display multiple types of epigenetic modifications all at once. See (lim2024advances?) for a broad overview about the different types of technologies to sequence different omics and layers of epigenetics, and how they are getting computationally put together.\n\n\n\n\n\n\n\nFigure 6.17: From https://www.sc-best-practices.org/chromatin_accessibility/introduction.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.18: From https://www.whatisepigenetics.com/type-2-diabetes-mellitus-and-epigenetics.\n\n\n\n\n\n\n\n\nClaringbould, Annique, and Judith B Zaugg. 2021. “Enhancers in Disease: Molecular Basis and Emerging Treatment Strategies.” Trends in Molecular Medicine 27 (11): 1060–73.\n\n\nIto, Shinsuke, Nando Dulal Das, Takashi Umehara, and Haruhiko Koseki. 2022. “Factors and Mechanisms That Influence Chromatin-Mediated Enhancer–Promoter Interactions and Transcriptional Regulation.” Cancers 14 (21): 5404.\n\n\nLi, Lei, Yumei Li, Xudong Zou, Fuduan Peng, Ya Cui, Eric J Wagner, and Wei Li. 2022. “Population-Scale Genetic Control of Alternative Polyadenylation and Its Association with Human Diseases.” Quantitative Biology 10 (1): 44–54.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#footnotes",
    "href": "chapter5_epigenetics.html#footnotes",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "The technology to measure this at single-cell resolution is still being developed (see snmC-seq2 (liu2021dna?)), and is particularly interesting due to methylation’s relation to cellular memory (kim2017dna?) and molecular clocks (hernando2019ageing?); (trapp2021profiling?); (gabbutt2022fluctuating?).↩︎\nSee Droplet-HiC (chang2024droplet?) for an example of single-cell Hi-C.↩︎\nSee Paired-tag (zhu2021joint?) for an example of single-cell histone modification.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter6_dna.html",
    "href": "chapter6_dna.html",
    "title": "7  ‘Single-cell’ DNA",
    "section": "",
    "text": "7.1 Genetics 101\nUnderstanding the fundamental concepts of genetics is essential for studying genomic variation, including copy-number variations (CNVs) and single nucleotide polymorphisms (SNPs). This section provides an overview of genetic architecture, SNPs and their detection, commonly sequenced tissues, and genome annotation resources such as the UCSC Genome Browser.\nSingle Nucleotide Polymorphisms (SNPs) and Their Detection.\nA single nucleotide polymorphism (SNP) is a variation at a single base pair position in the genome that is present in a significant fraction of the population. SNPs are the most common type of genetic variation and can have functional consequences depending on their location. When an SNP occurs within a coding region, it may alter the resulting protein sequence if it leads to an amino acid substitution (nonsynonymous SNP) or have no effect if the change is synonymous. SNPs in noncoding regions can impact gene regulation by affecting transcription factor binding sites, splicing efficiency, or untranslated regions (UTRs). Since you have “two copies” of each of your 23 chromosomes, this means SNP data is a data matrix of \\(n\\) people by \\(p\\) SNP regions (think of a couple million – more on this technicality later), where each value is \\(\\{0,1,2\\}\\), see Figure 7.1. Typically, the major allele is defined as “0”, and a “1” or “2” means if how many copies of the minor allele do you have.\nSNPs are detected using high-throughput sequencing technologies, primarily whole-genome sequencing (WGS) and whole-exome sequencing (WES). In these approaches, DNA is extracted from a biological sample, fragmented, and sequenced to generate short or long reads. The raw sequencing reads are then aligned to a reference genome, and variant calling algorithms such as those implemented in GATK (mckenna2010genome?), bcftools, and FreeBayes (garrison2012haplotype?) identify SNPs by comparing observed nucleotide differences to the reference sequence. (See (zverinova2022variant?) for a overview). The sequencing depth, or coverage, at a given genomic position determines the confidence in an SNP call, with higher coverage reducing the likelihood of sequencing errors.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>'Single-cell' DNA</span>"
    ]
  },
  {
    "objectID": "chapter6_dna.html#sec-genetics_basics",
    "href": "chapter6_dna.html#sec-genetics_basics",
    "title": "7  ‘Single-cell’ DNA",
    "section": "",
    "text": "Figure 7.1: From https://www.genome.gov/about-genomics/educational-resources/fact-sheets/human-genomic-variation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>'Single-cell' DNA</span>"
    ]
  },
  {
    "objectID": "chapter7_crispr.html",
    "href": "chapter7_crispr.html",
    "title": "8  Single-cell CRISPR editting",
    "section": "",
    "text": "8.1 Basics of how CRISPR works\nThe CRISPR-Cas system1 is a powerful genome-editing technology that allows for precise modifications of DNA in a wide range of biological systems. Originally derived from the bacterial adaptive immune system, CRISPR-Cas9 has been repurposed for genetic engineering by using a guide RNA (gRNA) to direct the Cas9 nuclease to a specific genomic locus for targeted DNA cleavage. This section discusses how CRISPR is performed in the wet lab, different functional applications of CRISPR, and how CRISPR-based perturbations are analyzed in single-cell gene expression studies.\nIn a typical CRISPR screen experiment (mainly a CRISPR knockout), a library of lentivirus-packaged guide RNAs is introduced into cells under conditions designed to infect each cell with only one or a few sgRNAs (single guide RNA), see Figure 8.1 and Figure 8.2. After selection to ensure stable integration, the cells are subjected to a particular stimulus such as drug treatment or other environmental challenge. Researchers then track the abundance of each sgRNA at the start and after the stimulus (for example, at day 0 and day 28) through next-generation sequencing. By comparing which sgRNAs become enriched or depleted, it is possible to discover genes essential for viability, pathways governing drug resistance, or other critical biological functions relevant to the phenotype under study, see Figure 8.3.\nTypical components of a CRISPR mechanism.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single-cell CRISPR editting</span>"
    ]
  },
  {
    "objectID": "chapter7_crispr.html#sec-chapter_7_basics",
    "href": "chapter7_crispr.html#sec-chapter_7_basics",
    "title": "8  Single-cell CRISPR editting",
    "section": "",
    "text": "Figure 8.1: Original from Wei et al. (2019), but this is directly from https://www.youtube.com/watch?v=JdCCl1uxCME.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: From https://www.idtdna.com/pages/education/decoded/article/overview-what-is-crispr-screening.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: From (esposito2019hacking?).\n\n\n\n\n\n\nCas9 protein: A DNA endonuclease that recognizes and cuts target DNA specified by the guide RNA; variations include nuclease-deactivated (dCas9) or nickase Cas9 for alternative applications. Other CRISPR-associated proteins like Cas12a (Cpf1), Cas13, or dCas9 (dead Cas9) can expand the range of target sequences, have different PAM requirements, or allow for gene regulation without cutting DNA.\nCas9 may bind sites with partial sequence complementarity, resulting in unintended cuts — this is called “off-target effects.” Various strategies (e.g., high-fidelity Cas9 variants, improved gRNA design) help minimize these effects.\nGuide RNA (gRNA): A customizable RNA sequence that directs Cas9 to the desired genomic locus; variations include single-guide RNA (sgRNA) and dual-RNA formats depending on the experimental design.\nTo streamline CRISPR-Cas9 applications in genetic engineering, scientists have designed a synthetic fusion of crRNA and tracrRNA into a single-guide RNA (sgRNA). This chimeric RNA retains both the target-specific recognition (crRNA component) and Cas9-binding function (tracrRNA component) but simplifies the system by reducing the number of necessary molecules. sgRNAs are commonly used in research and therapeutic applications due to their ease of design and efficiency in genome editing.\n\ncrRNA (CRISPR RNA): A short RNA sequence that is complementary to the target DNA and provides sequence specificity for Cas9 binding. This is typically 20 nucleotides long.\n\ntracrRNA (Trans-activating CRISPR RNA): A structural RNA that base-pairs with the crRNA and interacts with Cas9 to activate its nuclease function.\nIn this system, the crRNA and tracrRNA must form a duplex to guide Cas9 to the target sequence, which then leads to DNA cleavage.\nMultiple guide RNAs can be delivered simultaneously to target different loci at once, enabling complex genome-scale screens or combinatorial gene perturbations. Computational tools help optimize sgRNA sequences to maximize on-target efficiency while minimizing off-target effects. Chemically modified guide RNAs can improve stability and efficiency in vivo.\n\nPAM (Protospacer Adjacent Motif): A short DNA sequence adjacent to the target region that is essential for Cas9 to recognize and bind to DNA. The PAM sequence is typically “NGG” (where “N” represents any nucleotide, and “GG” is required). Without the correct PAM sequence, Cas9 cannot efficiently bind or cut the DNA, ensuring some level of specificity in genome editing. Different Cas9 variants have evolved or been engineered to recognize alternative PAM sequences, which offer broader targeting possibilities with reduced off-target effects. The cut typically happens 3 base pairs upstream of the PAM on the target strand (i.e., in the 5′ direction).\nLentivirus: Lentiviruses are widely used as viral vectors for delivering CRISPR components into cells (variations include different promoters, packaging systems, and envelope proteins to optimize transduction efficiency), particularly for experiments requiring stable and long-term expression of Cas9 and guide RNAs. Since many cell types are difficult to transfect using conventional methods, lentiviral transduction provides an efficient way to introduce CRISPR machinery into a broad range of cell types, including non-dividing and primary cells.\nWhat is delivered to cells? (See Figure 8.4–Figure 8.6 for the construction and architecture of the lentiviral vector.)\n\nCas9 expression construct: A lentiviral vector encoding the Cas9 nuclease under a suitable promoter. In some systems, inducible promoters (e.g., doxycycline-inducible) are used to control Cas9 activity.\n\nGuide RNA (gRNA) expression construct: A separate lentiviral vector encoding the guide RNA sequence under a promoter to ensure efficient transcription.\n\nSelection markers: Often, antibiotic resistance genes or fluorescent markers are included to facilitate selection of successfully transduced cells.\n\nLentiviral delivery is essential for CRISPR applications due to its ability to stably integrate into the host genome, ensuring long-term expression of Cas9 and guide RNAs. This stability is particularly advantageous for genome-wide CRISPR knockout screens and lineage tracing experiments. Additionally, many cell types, such as primary cells, stem cells, and immune cells, are notoriously difficult to transfect2; lentiviral transduction provides a more efficient and reliable alternative. See Figure 8.7–Figure 8.8 for how the gRNA can be injected into the cell, or if it’s integrated into the DNA, how it “borrows” the cell’s transcriptional machinery.\n\n\n\n\n\n\n\n\nFigure 8.4: Top: From (khan2022crispr?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: Bottom left: https://bpsbioscience.com/pd-1-crispr-cas9-lentivirus-integrating-78052.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6: Bottom right: From (mao2017heritability?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.7: Left: https://www.benchling.com/blog/how-to-synthesize-your-grnas-for-crispr.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.8: Right: https://bpsbioscience.com/lentiviruses?product_type_filter=5567.\n\n\n\n\n\n\n\n\nWei, Lai, Derek Lee, Cheuk-Ting Law, Misty Shuo Zhang, Jialing Shen, Don Wai-Ching Chin, Allen Zhang, et al. 2019. “Genome-Wide CRISPR/Cas9 Library Screening Identified PHGDH as a Critical Driver for Sorafenib Resistance in HCC.” Nature Communications 10 (1): 4681.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single-cell CRISPR editting</span>"
    ]
  },
  {
    "objectID": "chapter7_crispr.html#footnotes",
    "href": "chapter7_crispr.html#footnotes",
    "title": "8  Single-cell CRISPR editting",
    "section": "",
    "text": "CRISPR stands for “clustered regularly interspaced short palindromic repeats,” but it’s not too important to know why exactly it’s called this for the purposes of this chapter.↩︎\nTransfection is the process of introducing foreign nucleic acids (such as DNA or RNA) into eukaryotic cells to manipulate gene expression or enable genetic modifications. It is commonly used in plasmid-based CRISPR applications. In opposition, transduction refers to the introduction of genetic material using viral vectors.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single-cell CRISPR editting</span>"
    ]
  },
  {
    "objectID": "chapter8_lineage.html",
    "href": "chapter8_lineage.html",
    "title": "9  Single-cell lineage tracing",
    "section": "",
    "text": "9.1 Why are we interested in learning temporal dynamics?\nStudying how cells change over time provides critical insight into the sequence of events driving biological processes. By observing changes in cell populations across multiple time points, researchers can pinpoint when specific transitions or bifurcations occur. Such temporal information reveals which factors influence a cell’s fate and how quickly new traits emerge.\nMoreover, understanding temporal dynamics can help us develop better interventions. If we can identify the earliest signs of disease or undesirable changes in cells, then targeted therapies can be designed to prevent or slow progression. Such strategies are especially powerful for complex, multi-stage diseases where later intervention might be less effective.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Single-cell lineage tracing</span>"
    ]
  },
  {
    "objectID": "chapter8_lineage.html#why-are-we-interested-in-learning-temporal-dynamics",
    "href": "chapter8_lineage.html#why-are-we-interested-in-learning-temporal-dynamics",
    "title": "9  Single-cell lineage tracing",
    "section": "",
    "text": "Cancer subclone evolution: Tumors consist of various subclones that compete and evolve. Observing which subclones become dominant over time helps illuminate how certain cells acquire and propagate new mutations, sometimes conferring resistance to treatments. See Figure 9.1 and Figure 9.2.\n\n\n\n\n\n\n\n\nFigure 9.1: (Top) From (ashouri2023decoding?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: (Bottom) From (marine2020non?).\n\n\n\n\n\nCancer metastasis: Cells that detach from the primary tumor site and successfully colonize new tissues undergo significant genetic and phenotypic changes. Understanding these changes in temporal sequence highlights the adaptations needed for invasion and survival in distant environments. See Figure 9.3.\n\n\n\n\n\n\n\n\nFigure 9.3: From (fu2023emerging?).\n\n\n\n\n\nDisease progression: Many diseases advance in stages, even beyond cancer, with cells accumulating subtle changes that eventually manifest as severe pathologies. Timing the acquisition of these changes reveals how early molecular events cascade into full-blown disease. See Figure 9.4 for an example of this investigated in COVID.\n\n\n\n\n\n\n\n\nFigure 9.4: From (stephenson2021single?). You can see that at different stages of COVID, there is a slightly different proportion of cell types that constitute the immune system.\n\n\n\n\n\nEmbryonic/organ development and cell fate: During embryonic development, cells undergo a series of tightly regulated fate decisions that determine their final identity. These decisions are influenced by both intrinsic genetic programs and extrinsic signaling cues from the surrounding environment. Understanding the temporal dynamics of these transitions allows researchers to uncover the molecular mechanisms guiding differentiation and tissue formation. By tracking how cells commit to specific lineages, we gain insight into how organs form or how “cells make decisions”. See Figure 9.5.\n\n\n\n\n\n\n\n\nFigure 9.5: From (weinreb2020lineage?). This dataset is recorded longitudinally (i.e., cells were sequenced at Day 2, 4, and 6.) What’s shown here are cells known to be in different lineages, but certain lineages don’t differentiate by Day 6, differentiate into multiple cell types, or differentiate only into one cell type.\n\n\n\n\n\nStem cell research: Stem cells differentiate into specialized cell types following a tightly regulated timeline. Tracking these progressions uncovers the signals that guide each step and may inform regenerative medicine strategies for repairing damaged tissues. See Figure 9.6.\n\n\n\n\n\n\n\n\nFigure 9.6: From https://stemcellthailand.org/induced-pluripotent-stem-cells-ips-ipscs-hipscs/.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Single-cell lineage tracing</span>"
    ]
  },
  {
    "objectID": "chapter9_spatial.html",
    "href": "chapter9_spatial.html",
    "title": "10  ‘Single-cell’ spatial transcriptomics",
    "section": "",
    "text": "Single-cell spatial transcriptomics is an emerging technology that enables the measurement of gene expression while preserving the spatial organization of cells within a tissue. See Figure 10.1 for a useful schematic I like. Unlike traditional single-cell RNA sequencing, which dissociates cells and loses spatial context, spatial transcriptomics allows researchers to analyze gene expression in relation to cellular neighborhoods, tissue architecture, and microenvironments, see Figure 10.2.\n\n\n\n\n\n\n\nFigure 10.1: From https://www.mscience.com.au/supplier/10x-genomics/.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: From (kim2023single?).\n\n\n\n\nThe spatial arrangement of cells is fundamental to understanding cell biology, as cellular functions are often influenced by their local microenvironment:\n\nCancer: The tumor microenvironment plays a crucial role in disease progression, immune evasion, and therapeutic response. Understanding where specific cell populations reside within a tumor and how they interact with stromal and immune cells can reveal mechanisms of resistance and potential therapeutic targets.\nDevelopmental biology: Spatial positioning dictates lineage specification and organ formation, where disruptions in cell placement can lead to congenital defects. We can also overlay the gene expression at each spatial location with other imaging technologies or our current understanding of the structure of an organ, see Figure 10.3. This can help us learn how the cells in different regions of an organ have different functions, even if they are all the “same cell type.”\n\n\n\n\n\n\n\n\nFigure 10.3: From .\n\n\n\n\n\nDisease: In neurodegenerative diseases such as Alzheimer’s, the spatial distribution of microglia and astrocytes based on their (spatial) proximity to pathological hallmarks like amyloid plaques and tau tangles provides critical insights into disease mechanisms.\n\nAs one concrete example of how spatial transcriptomics advances cell biology beyond what other sequencing technologies offers: spatial transcriptomics is particularly valuable for studying cell-cell communication, as it enables the identification of ligand-receptor interactions that mediate signaling between neighboring cells. By integrating spatial data with single-cell transcriptomics, researchers can infer functional relationships between different cell types, uncovering regulatory networks that drive biological processes.\nNote: Spatial information in cell biology has been studied for decades, long before single-cell sequencing became commercially feasible. Techniques like fluorescence in situ hybridization (FISH) have been used to visualize the spatial localization of specific RNA molecules within cells, providing crucial insights into gene expression patterns. Microscopy-based methods, including confocal and super-resolution imaging, have also been instrumental in understanding cellular structures and interactions. The key novelty of modern spatial transcriptomics is its ability to scale these analyses – rather than measuring just a few genes at a time, we can now capture the spatial expression patterns of hundreds to thousands of genes simultaneously, enabling a much more comprehensive view of cellular organization and function.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>'Single-cell' spatial transcriptomics</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-role-of-normalization-to-adjust-for-sequencing-depth",
    "href": "chapter2_sequencing.html#the-role-of-normalization-to-adjust-for-sequencing-depth",
    "title": "3  Single-cell sequencing",
    "section": "3.2 The role of normalization to adjust for sequencing depth",
    "text": "3.2 The role of normalization to adjust for sequencing depth\nNormalization is a critical step in the analysis of single-cell sequencing data, addressing the technical variability introduced by differences in sequencing depth across cells. Sequencing depth refers to the total number of reads obtained for each cell, which can vary due to technical factors like library preparation or instrument sensitivity. Without normalization, cells with higher sequencing depth might appear to express more genes simply because of greater read coverage, not biological differences. Normalization methods aim to make gene expression counts comparable across cells by adjusting for these differences, ensuring that downstream analyses reflect true biological variation rather than technical artifacts. This step is foundational for accurate clustering, differential expression analysis, and other interpretative tasks in single-cell studies.\nIt has been known for a long time that when dealing with count data, proper normalization is foundational to properly doing meaningful inference. See (Risso et al. 2014). As we will see in Chapter 3, sequencing technologies cannot control how many reads (i.e., “counts”) are sequenced from each cell – this depends on the biochemistry efficency. Also, larger cells typically have more reads (which is itself a confounder) (Maden et al. 2023). We will describe normalization in more detail in Section 3.3.6.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-typical-workflow-for-any-sc-seq-data",
    "href": "chapter2_sequencing.html#the-typical-workflow-for-any-sc-seq-data",
    "title": "3  Single-cell sequencing",
    "section": "3.3 The typical workflow for any sc-seq data",
    "text": "3.3 The typical workflow for any sc-seq data\nWe will now describe the common steps at the start of a single-cell sequencing computational workflow (i.e., the steps that happen after the data has been collected and been aggregated into a matrix). More details about the downstream steps will be discussed in detail in Chapter 3. See (Luecken and Theis 2019; Heumos et al. 2023) for fantastic tutorials on best practices (among many). I’ll like to plug my own paper about best practices (Prater and Lin 2024).\n\n\n\n\n\n\n\nFigure 3.6: Abridged and typical workflow of a sc-seq dataset. (Luecken and Theis 2019)\n\n\n\n\nFor additional reference: See https://www.sc-best-practices.org/ and https://www.bigbioinformatics.org/intro-to-scrnaseq.\n\n3.3.1 (Optional) Ambient reads\n(This step will make more sense once you know a bit about how scRNA-seq data is generated, see Chapter 3.)\nAmbient reads and doublets (see Figure 3.7) are common technical artifacts in single-cell sequencing data that can distort biological interpretations if left unaddressed. Ambient reads arise from background RNA that is captured during sequencing but does not originate from the cell being analyzed. These reads can falsely inflate gene expression levels, particularly for highly abundant transcripts. Detecting and mitigating these artifacts is an optional but valuable step in data preprocessing, as it improves the overall quality and biological relevance of downstream analyses.\n\n\n\n\n\n\n\nFigure 3.7: Cartoon illustrating either ambient RNA or doublets. From https://www.10xgenomics.com/analysis-guides/introduction-to-ambient-rna-correction.\n\n\n\n\n\nInput/Output. The input to ambient detection is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes), and the output is \\(X'\\in \\mathbb{Z}_+^{n'\\times p}\\) where \\(X'_{ij} \\leq X_{ij}\\) for all cell \\(i\\) and feature \\(j\\).\n\n\n\n3.3.2 How to do this in practice\nThis is not always done in a standard analysis of 10x scRNA-seq data because the CellRanger pipeline does a pretty good job for you already by default. However, for certain finicky biological systems that deviate from the norm, you might purposely not use CellRanger’s default ambient detection and doublet detection, and instead choose to manually perform your own so that you have more control. In that case, the main ambient RNA detection method I’ve seen used is SoupX (Young and Behjati 2020).\n\n\n3.3.3 Cell filtering (or doublet detection)\nCell filtering is a crucial preprocessing step in single-cell sequencing analysis, aimed at removing low-quality or irrelevant cells to ensure reliable downstream analyses. During the sequencing process, some cells may produce insufficient data due to poor capture efficiency, leading to low gene counts or incomplete profiles. Additionally, some “cells” may actually be empty droplets or doublets. Doublets occur when two cells are captured together in the same droplet or well, leading to mixed gene expression profiles that do not represent any single cell. Filtering typically involves setting thresholds on metrics like the total number of detected genes, the fraction of mitochondrial gene reads (a marker of stressed or dying cells), and overall sequencing depth. By carefully selecting cells that meet quality standards, researchers can reduce noise, improve the robustness of analyses, and focus on biologically meaningful signals. This step helps ensure that the dataset represents a true and interpretable snapshot of cellular diversity.\n(Typically, ambient RNA means you’re trying to subtract “background” counts from your scRNA-seq matrix. The number of cells remains the same. In contrast, cell filtering is removing cells, typically geared to target cells deemed to be empty droplets or doublets.)\n\nInput/Output. The input to cell filtering is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) genes, and the output is \\(X'\\in \\mathbb{Z}_+^{n'\\times p}\\) where \\(n' \\leq n\\), whose rows are a strict subset of those in \\(X\\).\n\n\n\n3.3.4 How to do this in practice\nThis is commonly using the subset function in R, where we commonly filter based on nFeature_RNA (i.e., the number of genes that are expressed in a cell) and percent.mt (i.e., the fraction of counts that are originating from mitochondrial genes). The latter is usually computed using Seurat::PercentageFeatureSet3. See Figure 3.8 for an illustration. There is no commonly used threshold for these filters, but it typically depends on removing the extreme quantiles in your dataset. If this simple filter isn’t sophisticated enough to detect doublets (and you have good biological/technical reasons to suspect doublets, typically due to either the cell isolation or droplet formulation step), the doublet detection method I’ve seen used is DoubletFinder (McGinnis, Murrow, and Gartner 2019).\nNote: In Seurat, the cells are denoted as columns and features (such as genes) as rows. This is the transpose of the typical statistical notation.\n\n\n\n\n\n\n\nFigure 3.8: The filtering in the Seurat tutorial dataset goes from 2700 cells to 2638 cells. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\nWe usually filter based on nFeature_RNA because cells with too few genes are potentially deemed as empty droplets, and cells with too many genes are potentially deemed as doublets. High percentage of gene expression from mitochondrial genes is typically an indicator of poor sample quality4.\n\n\n3.3.5 A brief note on other approaches\nSee (Xi and Li 2021) for a benchmarking of doublet detection methods.\n\n\n3.3.6 Normalization\nNormalization is a key step in single-cell sequencing analysis, designed to adjust raw data so that gene expression measurements are comparable across cells. Variability in sequencing depth and technical artifacts can cause discrepancies in the total number of reads captured per cell, making raw counts unsuitable for direct comparison. Normalization methods address this by scaling or transforming the data to account for these differences, ensuring that observed expression levels more accurately reflect true biological variation. Approaches can range from simple scaling based on total counts to more sophisticated strategies that model the underlying distribution of the data. Normalization not only improves the accuracy of downstream analyses, such as clustering and differential expression, but also enhances the interpretability of results by reducing technical noise.\nTypically, the normalization has the following form:\n\\[\nX_{ij} \\leftarrow \\log\\Big(\\frac{10,000 \\cdot X_{ij}}{\\sum_{j'=1}^{p}X_{ij'}}+1\n\\Big).\n\\tag{3.3}\\]\nThe fraction \\(X_{ij}/\\sum_{j'=1}^{p}X_{ij'}\\) allows us to model the relative proportions (instead of absolute counts) of gene expression in each cell. The \\(\\log(\\cdot)\\) is to handle the right-skewed nature of the data (since an entry of \\(X_{ij}\\) is still 0 even after computing the fraction, and taking a fraction doesn’t adjust for the skewed nature by itself). The \\(+1\\) is handle the fact that we cannot take \\(\\log(0)\\). The only arbitrary thing in the factor of 10,000 – it’s simply for convenience to scale all the values (remember, \\(\\log(AB) = \\log(A) + \\log(B)\\), so essentially, this normalization is shifting all the fractions up by \\(\\log(10,000)\\)). The number of 10,000 (or sometimes 1 million) became commonplace because we can now interpret the number \\(10,000 \\cdot (X_{ij}/\\sum_{j'=1}^{p}X_{ij'})\\) as the hypothetical number of counts we would’ve gotten in cell \\(i\\) for gene \\(j\\) if the sequencing depth of cell \\(i\\) were 10,000.\n\n\n3.3.7 How to do this in practice\nSee the Seurat::LogNormalize function, illustrated in Figure 3.9.\n\n\n\n\n\n\n\nFigure 3.9: The normalization step in the Seurat tutorial. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\nInput/Output. The input to normalization is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes), and the output is \\(X'\\in \\mathbb{R}^{n\\times p}\\).\n\n\n\n3.3.8 A brief note on other approaches\nThere’s actually a lot of normalization methods since normalizing count data has existed ever since bulk-sequencing. However, three notable mentions are: SCTransform (Hafemeister and Satija 2019) which uses a NB GLM model to normalize data and GLM-PCA (Townes et al. 2019) which uses a GLM matrix factorization to adjust for sequencing depth, where both methods use adjust using the observed sequencing depth for each cell. Deep-learning methods like scVI (Lopez et al. 2018) include the library size as a latent variable to be estimated itself. See (Lause, Berens, and Kobak 2021; Ahlmann-Eltze and Huber 2023) for benchmarkings.\n\nRemark (Personal opinion: The log-transformation and lack of the negative binomial). It is well documented that this log-normalization is not best – see (Townes et al. 2019) and (Hafemeister and Satija 2019) for in-depth discussions. The issues stem from “discrete-ness” of the log transformation. However, surprisingly, as shown in papers such as (Ahlmann-Eltze and Huber 2023), the log-transformation is actually quite robust.\n\n\n\n3.3.9 Feature selection\nFeature selection is a critical step in single-cell sequencing analysis that focuses on identifying the most informative genes for downstream analyses. Single-cell datasets often include thousands of genes, many of which may be uninformative due to low variability or consistent expression across all cells. By narrowing the focus to a subset of highly variable genes (HVGs), feature selection reduces noise, enhances computational efficiency, and highlights the genes most likely to drive biological differences. These selected features are used in clustering, dimensionality reduction, and other tasks where capturing meaningful variation is essential. Effective feature selection ensures that the resulting analyses are both interpretable and biologically relevant.\n\nInput/Output. The input to feature selection is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes)5, and the output is \\(X'\\in \\mathbb{R}^{n\\times p'}\\) where \\(p' \\leq p\\), whose columns are a strict subset of those in \\(X\\).\n\n\n\n3.3.10 The standard procedure: Variance Stabilizing Transformation\nThe vst (variance stabilizing transformation) procedure is a commonly used method in single-cell RNA-seq analysis for identifying highly variable genes. It is designed to account for the relationship between a gene’s mean expression and its variability, ensuring that variability is measured in a way that is independent of mean expression levels. The procedure involves the following steps:\n\nFitting the Mean-Variance Relationship:\n\nFirst, the relationship between the log-transformed variance and log-transformed mean of gene expression values is modeled using local polynomial regression, commonly referred to as loess. This step captures the expected variance for a given mean expression level. Let \\(\\mu_j\\) denote the mean expression of gene \\(j\\) and \\(\\sigma_j^2\\) its variance. The loess fit provides the expected variance, \\(\\hat{\\sigma}_j^2\\), as a smooth function of \\(\\log(\\mu_j)\\).\n\nStandardizing Gene Expression Values:\n\nThe observed expression values for each gene are then standardized to account for the expected variance: \\[\nZ_{ij} = \\frac{X_{ij} - \\mu_j}{\\sqrt{\\hat{\\sigma}_j^2}},\n\\] where \\(X_{ij}\\) is the observed expression value for gene \\(j\\) in cell \\(i\\), \\(\\mu_j\\) is the observed mean for gene \\(j\\), and \\(\\hat{\\sigma}_j^2\\) is the expected variance given by the fitted loess line. This standardization ensures that variability is measured relative to what is expected for genes with similar mean expression levels.\n\nClipping and Variance Calculation: To prevent outliers from dominating the analysis, the standardized values \\(Z_{ij}\\) are clipped to a maximum value, determined by a parameter such as clip.max. The variance of each gene is then calculated based on the clipped standardized values. Genes with the highest variance are selected as highly variable and prioritized for downstream analyses, such as clustering and trajectory inference.\n\nThis procedure addresses the inherent bias where genes with higher mean expression tend to exhibit greater variance, even if this variance is not biologically meaningful. By standardizing variability against the expected mean-variance relationship, the vst method provides a robust approach to identifying genes that truly capture biological heterogeneity across cells.\n\nRemark (Feature selection, agnostic of the rest of the analysis). For the statistical students, feature selection might seem a bit odd. After all, we’re selecting the genes to use in our analysis (again, typically about 2000 genes among 30,000 genes) before we do any other modeling. This is unlike the Lasso, where the feature selection is done while we’re doing a downstream task (i.e., regression in this case).\nIn a scRNA-seq analysis, we typically use the HVGs upfront, and use only these genes for all remaining downstream analyses. This is mainly for pragmatic reasons – most genes are 0’s in almost all the cells, so there’s no need to involve these genes in any of our downstream analysis.\n\n\n\n3.3.11 How to do this in practice\nSee the Seurat::FindVariableFeatures function, illustrated in Figure 3.10.\n\n\n\n\n\n\n\nFigure 3.10: The feature selection step in the Seurat tutorial, which selects the highly variable 2000 genes (from the 13,714 genes sequenced). See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n3.3.12 A brief note on other approaches\nSee (Zhao et al. 2024) for an overview of feature selection methods. This is a interesting statistical modeling question in the sense that the goal for these methods is often how to distinguish between “technical variance” (i.e., variation attributed to sequencing depth) from “biological variance” (i.e., if a gene has different expressions across cells in the tissue).\n\n\n3.3.13 (Optional) Imputation\nImputation refers to methods used to infer missing or undetected values in single-cell sequencing data, particularly addressing the zeros that dominate scRNA-seq datasets. Early in the development of single-cell technologies, these zeros were often attributed to “dropouts,” where lowly expressed genes failed to be captured due to technical limitations. Zero-inflated models were developed to distinguish between “true” biological zeros – where a gene is genuinely not expressed – and dropout zeros, filling in the latter to provide a more complete representation of gene expression. However, with advancements like unique molecular identifiers (UMIs), scRNA-seq data have become more reliable and less prone to dropout artifacts. As a result, the need for imputation has diminished, and modern workflows often bypass it entirely, favoring raw or minimally processed data that more accurately reflect true biological variation. Imputation remains an optional step, typically reserved for specific analyses where reconstructing missing data is critical. See (Hou et al. 2020) for a review of many methods in this category.\nSee (Kharchenko, Silberstein, and Scadden 2014) for the landmark paper that originally discussed dropouts. However, see Remark 4.2 for why these “dropouts” are no longer common to worry about in your single-cell analysis.\n\n\n3.3.14 How to do this in practice\nI’ve see MAGIC (Markov Affinity-based Graph Imputation of Cells) (Van Dijk et al. 2018) used quite often, if there’s a data imputation being performed. This method imputes missing gene expression values by leveraging similarities among cells using a graph-based approach. MAGIC constructs a nearest-neighbor graph where cells are nodes, and edges represent biological similarity. Through data diffusion, information is shared across similar cells to denoise and recover underlying gene expression patterns. See Figure 3.11.\n\n\n\n\n\n\n\nFigure 3.11: Schematic of the MAGIC method (Van Dijk et al. 2018).\n\n\n\n\n\n\n3.3.15 Dimension reduction\nDimension reduction, particularly methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), serves as a cornerstone in single-cell sequencing analysis. The primary goal is twofold: first, to obtain a low-dimensional representation of the data, which is critical for visualizing complex datasets and enabling computationally efficient downstream analyses such as clustering and trajectory inference. By condensing thousands of genes into a smaller number of principal components, researchers can focus on the dominant patterns of variation that drive biological differences. Second, dimension reduction helps denoise the data by filtering out technical noise and minor variations that are less biologically relevant. This makes the resulting analysis more robust and interpretable, allowing researchers to capture meaningful cellular heterogeneity while mitigating the impact of sparsity and noise inherent to single-cell datasets.\n\nInput/Output. The input to dimension reduction is a matrix \\(X\\in \\mathbb{R}^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes). The output is \\(Z \\in \\mathbb{R}^{n\\times k}\\) where \\(k \\ll p\\) (where there are \\(k\\) latent dimensions, typically between 5 to 50). We call \\(Z\\) the score matrix. Most dimension reduction (typically only linear ones – i.e., ones based on a matrix factorization) also return: (1) gene loading matrix \\(W \\in \\mathbb{R}^{p \\times k}\\), and (2) “denoised” matrix \\(X'\\in \\mathbb{R}^{n\\times p}\\), which is some form of \\(X' = ZW^\\top\\).\n\n\n\n3.3.16 The standard procedure: PCA\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique widely used in single-cell RNA-seq analysis to simplify complex datasets while retaining the most important patterns of variation. By transforming high-dimensional gene expression data into a smaller set of principal components, PCA captures the dominant trends in the data, such as differences between cell types or states. This reduced representation helps to mitigate the noise and sparsity inherent in single-cell data, making downstream tasks like clustering, trajectory inference, and visualization more efficient and interpretable. PCA serves as a foundational step in many single-cell workflows, offering both computational efficiency and biological insight.\n\n\n\n\n\n\n\nFigure 3.12: Illustration of the PCA of a \\(p=2\\)-dimensional dataset reduced to one dimension (\\(K=1\\)). (Left) Showing the maximization or minimization perspective of PCA.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.13: Showing how to interpret PCA as a matrix approximation. Here, the “component” is \\(\\hat{W}\\), and the “loading” is \\(\\hat{Z}=X\\hat{W}\\).\n\n\n\n\nI’ll describe the PCA in a bit more detail here since it’s can be a bit confusing.\n\nTheorem (Equivalent formulations of PCA). Let \\(X \\in \\mathbb{R}^{n\\times p}\\) be a centered matrix ( i.e., \\(\\overline{X}_{\\cdot j} = 0\\) for all \\(j\\in\\{1,\\ldots,p\\}\\)) and \\(K\\) be a pre-chosen latent dimensionality (such that \\(k \\ll p\\)). Let \\(\\hat{\\Sigma} = X^\\top X/n\\) denote the empircial covariance matrix.\nThe following four estimated embeddings \\(\\hat{Z} \\in \\mathbb{R}^{n\\times k}\\) are “equivalent.”\n\nMaximization of information captured by the covariance (shown in Figure 3.12 (left)): Define \\(\\hat{Z} = X\\hat{W}\\) where \\[\n\\hat{W} = \\argmax_{W \\in \\mathbb{R}^{p\\times K}}\\tr\\Big(W^\\top \\hat{\\Sigma} W\\Big),\n\\quad \\text{such that} \\quad W^\\top W = I_K.\n\\tag{3.4}\\]\nMinimization of reconstruction error (via squared Euclidean loss) (shown in Figure 3.12 (left)): Define \\(\\hat{Z} = X \\hat{W}\\) where \\[\n\\hat{W} = \\argmin_{W \\in \\mathbb{R}^{p\\times K}}\\frac{1}{n}\\sum_{i=1}^{n}\\Big\\|X_{i\\cdot} - WW^\\top X_{i\\cdot}\\Big\\|^2_F\n\\quad \\text{such that} \\quad W^\\top W = I_K.\n\\tag{3.5}\\]\nLow-rank approximation of the interpoint distances (via squared Euclidean distances): Let \\(D_X \\in \\mathbb{R}^{n\\times n}_+\\) denote the matrix of squared Euclidean distances among all \\(n\\) samples in \\(X\\), i.e., \\([D_X]_{i_1,i_2} = \\|X_{i_1,\\cdot} - X_{i_2,\\cdot}\\|^2_F\\) for all \\(i_1,i_2 \\in \\{1,\\ldots,n\\}\\). (This is a function of \\(X\\).) Then, construct \\(\\hat{Z}=X\\hat{W}\\) where \\[\n\\hat{W} = \\argmin_{W \\in \\mathbb{R}^{p\\times K}} \\sum_{i_1,i_2 \\in \\{1,\\ldots,n\\}}  \\big[D_X\\big]_{i_1,i_2} - \\big[D_{XW}\\big]_{i_1,i_2} \\quad \\text{such that} \\quad W^\\top W = I_K,\n\\tag{3.6}\\] and \\(D_{XW}  \\in \\mathbb{R}^{n\\times n}_+\\) is the matrix of squared Euclidean distances constructed from \\(XW\\).\n\n\nNotice that Formulation #3 is simply the sum of differences (not squared) since projections are non-expansive, so the interpoint Euclidean distances necessarily decrease. We are not accurately describing the identifiability conditions above to formalize what “equivalence” technically means.\nSome remarks:\n\nRelation to eigen-decompositions: PCA is fundamentally tied to eigen-decompositions due to the Low-Rank Approximation theorem6 (more formally, the Eckart-Young-Mirsky theorem). Specifically, \\(\\hat{W}\\) is the eigenvalues of \\(\\hat{\\Sigma}\\), the covariance matrix. See Figure 3.13 (right).\n\n(If you are familiar with how to manipulate eigen-decompositions, you can convince yourself that Formulation 2 is most directly related to the Low-Rank Approximation.)\n\nInterpretation of Formulation #1: In Formulation 1, we want to project the data \\(X\\) onto a lower-dimensional subspace defined by \\(\\hat{W}\\) such the low-dimensional data \\(\\hat{Z}\\) has as much variance as possible. This is by far the most common explanation of PCA. Part of its appeal is that it’s easy to explain what the “population model’s target quantity” is through this interpretation, but 1) it requires you to intrinsically understand why preserving covariance is something useful when we’re more interested in about the cells themselves, and 2) does not give insight to the more popular extensions of PCA.\n\nAdjustable aspects to obtain other dimension-reduction methods: 1) Choosing a more suitable estimate of the population covariance \\(\\Sigma = \\mathbb{E}[X^\\top X]/n\\) (or its correlation counterpart), or 2) enforcing additional structure (such as sparsity) onto the columns of \\(\\hat{W}\\).\n\nInterpretation of Formulation #2: In Formulation 2, we want to project the data \\(X\\) onto a lower-dimensional subspace defined by \\(\\hat{W}\\) such that the ambient-dimensional data is as close to the original data as possible (i.e., the projection “disturbs” the data the least). This interpretation reveals that PCA is trying to find the subspace that “distorts” each cell’s expression the least. While this won’t be used to motivate more modern embedding methods, it 1) offers a formal relation between PCA and matrix denoising, and 2) is the motivation for autoencoders later on.\n\nAdjustable aspects to obtain other dimension-reduction methods: 1) How to measure the “error” between each cell’s expression and its reconstructed expression, 2) how this reconstruction is constructed given a cell’s low-dimensional representation?\n\nInterpretation of Formulation #3: In Formulation 3, we conceptually think about the cells’ interpoint distances (i.e., the distances between any two pairs of cells). Then, we want to project the data \\(X\\) onto a lower-dimensional subspace defined by \\(\\hat{W}\\) such the low-dimensional data \\(\\hat{Z}\\) preserves the distances between any two pairs of cells as much as possible. This perspective is perhaps the most intuitive, and also most revealing on the shortcomings of PCA. PCA is choosing to preserve the pairwise distances in a very specific fashion, via the squared Euclidean distances. There are a two aspects that reveal the deficiency of PCA:\n\n\nPCA is trying to preserve the squared Euclidean distance, so it’s going to put most of its attention on pairs of points that are very far away from one another.\nWe consider all pairs of points.\n\nThese two aspects combined yield the qualitative statement that “PCA favors a global embedding that preserve far-away points”.7 In some sense, this is the exact opposite of what we want – qualitatively, we probably care more about getting the relative distances of each points and its immediate/local neighbors (so we can accurately see the subtle shifts in cells, as in trajectory inference).\nAdjustable aspects to obtain other dimension-reduction methods: 1) How we’re measuring distance in \\(X\\) or in \\(Z\\) we’re using (perhaps using a distance that adapts to the geometry of the data), 2) how to measure the “distortion” in the pairwise distances, 3) which pairs of cells do we care about preserving their distances?\n\nReferences for the proofs: Relating one and two is mainly an exercise in matrix algebra. To show that Formulation #3 is also PCA, see Theorem 14.4.1 in (Kent, Bibby, and Mardia 1979).\n\n\nRemark (The words we use to call the columns of \\(\\hat{W}\\) after doing a linear-dimension reduction method like PCA). Note that \\(\\hat{W}\\) is a \\(p\\)-by-\\(K\\) matrix. That is, each column of \\(\\hat{W}\\) has one number for each feature (i.e., gene). We typically call each column one of a few things: “gene program” or “pathway” (both more biology-facing) or “eigen-gene” or “topic” (both more statistics-facing). Regardless, these words is to denote that each column of \\(\\hat{W}\\) denotes a set of genes that work together, based on the magnitude and sign that feature has in this column.\n\n\n\n3.3.17 How to do this in practice\nSee the Seurat::RunPCA function, illustrated in Figure 3.14. The resulting visualization is shown in Figure 3.15.\n\n\n\n\n\n\n\nFigure 3.14: The PCA step. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.15: Using the Seurat::DimPlot(pbmc, reduction = \"pca\") function to visualize the first two PCs. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n3.3.18 A brief note on other approaches\nThe distinction between “imputation” and “dimension reduction” gets murky because 1) computationally, many imputation methods rely on a dimension reduction, and 2) statistically, due to the low-rank approximation property of PCA, you can interpret many dimension reductions methods as an imputation method. When I say that “imputation is optional,” what I really mean is that modeling zero-inflation is typically optional.\nReviews of imputation methods such as (Hou et al. 2020) contain overview of dimension-reduction methods. eSVD (Lin, Qiu, and Roeder 2024) and GLM-PCA (Townes et al. 2019) are dimension reduction methods that leverage the fact that the NB is in the exponential family (among many). FastTopics (Carbonetto et al. 2021) is a popular non-negative matrix factorization method (among many) for modeling single-cell data. (We won’t have time to discuss non-negative matrix factorization in this course.) All these mentioned methods are able to perform normalization with dimension reduction simultaneously. Also, dimension-reductions are commonly folded into a method that aims to do multiple tasks simultaneously (for example, see scINSIGHT (Qian et al. 2022), among many examples). See (Ghojogh et al. 2021) for a massive review of common dimension-reduction frameworks.\nHowever, by far the most popular method nowadays is using VAEs, since it is easy to combine dimension reduction with other downstream tasks. We’ll discuss VAEs in more detail in ?sec-vae.\n\nRemark (Personal opinion: Considerations for picking an appropriate dimension-reduction method). The categorizations of “linear” vs. “non-linear” is a bit blurry. Personally, I would call methods like eSVD (Lin, Qiu, and Roeder 2024) and GLM-PCA (Townes et al. 2019) as “linear” (in addition to PCA and fastTopics (Carbonetto et al. 2021)) since you estimate a matrix factorization and there’s a fixed transformation that relates the observed count \\(X_{ij}\\) to the inner product between a cell’s latent vector \\(Z_{i,\\cdot}\\) and a gene’s latent vector \\(W_{j,\\cdot}\\).\nTypically, people refer to non-linear dimension reductions as methods that do not use a matrix factorization (think of diffusion maps (Haghverdi, Buettner, and Theis 2015) or deep-learning methods (Lopez et al. 2018)). Since non-linear methods can capture more complex relations than linear methods, what are some considerations you should make to pick a particular method?\n\nMean-variance relation: PCA is intimately connected to modeling the data with constant Gaussian noise (i.e., every entry of \\(X_{ij}\\) is observed with the same amount of Gaussian error (Tipping and Bishop 1999). However, you might want to use a method where you expect the amount of variability in the data increases as the mean increases (such as a Poisson distribution). See (Lin, Qiu, and Roeder 2022) for more details.\nGene programs: Methods that do not use a matrix factorization often do not give you a built-in way to interpret collections of genes. In contrast, after using a matrix factorization approach like PCA, you can interpret how genes are coordinating through the estimated matrix \\(\\hat{W}\\).\n“Distance” between cells: As you saw in Formulation #3 Equation 3.6, PCA cares about the Euclidean distance between any two cells. Sometimes, such as in a diffusion map (see Palantir for instance (Setty et al. 2019)), you have in mind a more appropriate way to measure how “different” two cells are.\n\n(Notice that by itself, the fact that “non-linear methods can capture non-linear structure” is not really a good enough reason to use a non-linear method. This is because this statement itself is somewhat vacuous. By the low-rank approximation theorem (https://en.wikipedia.org/wiki/Low-rank_approximation), as long as you use enough latent dimensions, any linear method can also capture non-linear structures. The only advantage of non-linear methods in this setting is that the non-linear method can represent this non-linear structure with “less numbers” (i.e., an information-compression perspective).)\nIn general, there is never a “strictly better” option. Every reasonable method has its own niche on what kind of {technology, experiment, data, biological-question} setting it outshines other competitors. Your job as a thoughtful researcher is to: 1) understand the conceptual differences between all these methods, and 2) have a concrete strategy on how you would make an informed decision on which method you want to apply.\n\n\n\n3.3.19 Batch correction/Covariate adjustment\nBatch correction is a vital step in single-cell sequencing analysis that addresses technical variability introduced by differences in experimental conditions, such as processing time, reagent batches, or sequencing runs. These batch effects can obscure true biological signals and create artificial differences between groups of cells. Batch correction methods aim to align data from different batches while preserving meaningful biological variation. Techniques range from simple scaling adjustments to advanced algorithms that model batch effects explicitly. As single-cell studies increasingly combine datasets from multiple experiments or institutions, effective batch correction ensures that comparisons and integrative analyses reflect biological reality rather than technical artifacts. See Figure 3.16 for an illustration of this.\nWe will talk about this a bit more when we discuss integration methods for cell-type labeling Section 4.6.\n\n\n\n\n\n\n\nFigure 3.16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIllustration of what the goal of batch correction is. All these are UMAPs (see Section 3.3.23). The top row colors cells by their batch, and the bottom row colors cells by cell type. (Left) Original data showing batch effects. (Middle) Intermediate processing. (Right) The batch-corrected data. (Tran et al. 2020) ::::\n\nInput/Output. The input to batch correction is (1) either the normalized matrix \\(X \\in \\mathbb{R}^{n\\times p}\\) or the score matrix \\(Z \\in \\mathbb{R}^{n\\times k}\\) and (2) an categorical vector \\(C \\in \\{1,\\ldots,B\\}^n\\) which denotes which cell belongs to which batch. (Certain batch correction methods might require more information.) In a literal sense, both \\(X\\) and \\(Z\\) are technically row-wise concatenations of many datasets.\n\nIf the input is a normalized matrix, the output is a batch-corrected normalized matrix \\(X'   \\in \\mathbb{R}^{n\\times p}\\). If the input is a score matrix \\(Z\\), the output is a batch-corrected score matrix \\(Z'   \\in \\mathbb{R}^{n\\times k}\\).\n\nRemark (What makes this task statistically possible?). There are many sources of “noise” (i.e., things unrelated to the biology we’re trying to study) in a single-cell dataset.\n\nSequencing noise: This comes from the fact that we “randomly” get counts from each cell, and we can’t control how effective each cell is at producing counts. We can think of this as “mean-zero, i.i.d. across all cells” in some sense.\nBatch noise: This is the “noise” that originates from a particular batch. It is probably more statistically appropriate to think of this as a “bias,” because it is not mean-zero. Instead, we conceptually think of this as a shift occurring to all the cells in a batch more-or-less uniformly (but could impact different genes differently).\nBiological confounding noise: Say you want to study how lung tissue changes during aging, but some of your tissue samples come from donors who smoke. The aging biological signature you’re hoping to find would likely be confounded by a smoking signature. This “noise” is also not mean-zero, and it could be different across different cell types. However, given a cell type, this biological confounding effect should be the same across batches. (The trouble is that you might not be aware of all the confounders during an analysis.)\n\nHence, it is possible to “remove” batch effects because: 1) it’s a constant “shift” for all the cells in the same batch, and 2) we always know the “batches,” since it is purely an artifact of how data is generated. Most batch-correction papers are statistically motivated, but do not have formal theoretical properties. This is primarily due to the fact that in practice, it’s very hard to know how to model a batch effect rigorously. However, there are a few papers that try to leverage this intuition formally in a statistical framework, see (Ma et al. 2024).\n\n\nRemark (The term “batch” is very overloaded). There are many steps during a single-cell analysis (see Chapter 3). These are some of the things a “batch” could refer to: 1) cells processed together during droplet/gem formation, 2) cells processed during during PCR amplification, 3) cells processed together during sequencing (this is most common usage of the word “batch”), 4) cells processed using different protocols, 5) cells processed by different labs, 6) cells of different donors/organisms, among many more. (The first three steps are similar in “resolution,” but the last three are more “coarse” in how a batch is defined.)\nTechnically, the statistical logic of most batch-correction methods isn’t suitable for trying to align cells from different donors/organisms since in this setting, the “batches” are based on biology, not a technical artifact. For example, the differences between cells from two donors could be much more complicated than simply cells in two different sequencing batches.\nIn general, be sure to discuss with the people who generated the data (who likely will have a lot more experience on what technical artifacts are worth worrying about) on how to think about batch effects.\n\n\n\n3.3.20 What’s the difference between “batch-correcting” and “adjusting for covariates”?\nTechnically, there are some batch-correction methods whose premise is to treat each batch as simply a covariate to be adjusted for. The primary example is COMBAT (Zhang, Parmigiani, and Johnson 2020). However, most batch-correction methods do not do this. There is no formal reason why. I personally feel it’s because batch effects is such a critical bottleneck for almost every single-cell analysis that it’s imperative to use a method that is as (sensibly) flexible as possible. See my remark later in Remark 3.2. See (Tran et al. 2020; Luecken et al. 2022) for benchmarking multiple batch correction methods.\n\n\n3.3.21 A typical choice: Harmony\nHarmony (Korsunsky et al. 2019) is one of the most commonly used methods to integrate single-cell datasets from different experiments, technologies, or biological conditions into a shared low-dimensional embedding. The method begins with a principal component analysis (PCA) embedding and iteratively adjusts it to remove batch effects while preserving biological variation. Harmony groups cells into clusters using soft clustering, which assigns cells probabilistically to multiple clusters. These clusters account for technical and biological differences while capturing shared cell types and states. Correction factors are then computed for each cluster and applied to individual cells, ensuring that the final embedding reflects intrinsic cellular phenotypes without being confounded by dataset-specific biases. See Figure 3.17.\n\n\n\n\n\n\n\nFigure 3.17: Schematic of the Harmony (Korsunsky et al. 2019).\n\n\n\n\n\n\n3.3.22 How to do this in practice\nThere is no standardize default for batch correction. The usual practice is to: 1) decide upfront (with your collaborators) on how you would deem what is a “successful” batch correction. This usually involves understanding the data generation protocol and the biology question of interest. This will involve a blend of biological logic (i.e., statements that should be true, given the biological context), qualitative evaluations (i.e., based on plots), and numerical metrics (i.e., based on some score where you decide upfront how the score relates to a desirable batch correction). Then, 2) you try many batch correction methods8.\n\nRemark 3.2. Remark (Personal opinion: Batch correction is single-handedly the most precarious step in almost every analysis). While a cautious statistician might be doubtful for “cherry-picking” a good batch-correction method, the reality is that there is no guaranteed procedure to always work. This is because there are so many types of “batches,” and the underlying data generation pipeline, technology, and biology differs from lab to lab. Also, doing a “bad” batch correction is extremely detrimental. Unlike many other steps in a single-cell analysis where most choices of how to perform a step are “reasonable” (but some methods might have slightly better power or slightly more robust), a bad batch correction can single-handedly invalidate almost all other aspects of a single-cell analysis regardless of how sophisticated those methods are.\nAlso: batch correction is limited by the experimental design. No “real” way to validate this if you’re only given datasets – the validations need to be decided during the planning of the experimental design. For example, if you are studying cancer, and you put all the cancer donors’ cells in one batch and all the healthy donors’ cells in another batch, then you have no ability to disentangle the batch effects from the biological effect.\nTo aid validation of batch correction, a common experimental design is to put one reference tissue (where it’s possible to obtain many samples from this one tissue) in every sequencing batch.\n\n\n\n3.3.23 Visualization\nVisualization is a key step in single-cell sequencing analysis, offering a way to explore and interpret complex datasets intuitively. One of the most popular visualization techniques is Uniform Manifold Approximation and Projection (UMAP), which creates a low-dimensional embedding designed to preserve local and global structure in the data. UMAP is particularly effective for capturing non-linear relationships and revealing clusters of similar cells, making it a powerful tool for visualizing cellular heterogeneity. However, UMAP coordinates are less reliable than those obtained through methods like PCA for quantitative analysis. Unlike PCA, which is grounded in linear transformations and retains a direct link to the original data, UMAP embeddings are highly dependent on parameter choices and random initializations, making them less reproducible. Despite these limitations, UMAP excels as a visualization tool, helping researchers intuitively explore relationships between cells and identify patterns that guide deeper analyses.\n\nInput/Output. The input to visualization is typically the score matrix \\(Z\\in \\mathbb{R}^{n\\times k}\\) for \\(n\\) cells and \\(k\\) latent dimensions, and the output is matrix with \\(n\\) cells and 2 (sometimes 3) columns, where the visualization is literally plotting the values in the first column against the second column as a scatterplot.\n\n\n\n3.3.24 The standard procedure: UMAP\nUniform Manifold Approximation and Projection (UMAP) (Becht et al. 2019) is a dimensionality reduction technique designed to capture both local and global structure in high-dimensional data. UMAP builds on principles of manifold learning, aiming to preserve the topology of the original data in a lower-dimensional space.\n\nConstructing the high-dimensional graph: UMAP begins by modeling the local neighborhood relationships in the high-dimensional space:\n\n\nFor each data point \\(i\\), a probability distribution \\(p_{ij}\\) is constructed over its neighbors \\(j\\) based on distances, typically using a kernel function.\nThe resulting graph captures the high-dimensional structure of the data.\n\n\nOptimizing the low-dimensional embedding: In the low-dimensional space, UMAP aims to construct a graph with similar relationships. It models pairwise probabilities \\(q_{ij}\\) for the low-dimensional embedding using a smooth, differentiable cost function: \\[\nC = \\sum_{i,j} \\left( - p_{ij} \\log q_{ij} - (1 - p_{ij}) \\log (1 - q_{ij}) \\right).\n\\] Here:\n\n\n\\(p_{ij}\\) and \\(q_{ij}\\) represent probabilities of connectivity in the high- and low-dimensional spaces, respectively.\nThe first term encourages similar points in the high-dimensional space to remain close, while the second term discourages unrelated points from being artificially clustered.\n\n\nGradient descent: The low-dimensional coordinates are iteratively optimized using gradient descent to minimize \\(C\\), aligning the local relationships in the low-dimensional embedding with those in the original space.\n\nUMAP excels at preserving local neighborhoods while maintaining some global structure, making it particularly effective for visualizing high-dimensional single-cell RNA-seq data. However, the resulting embeddings depend on hyperparameters (e.g., number of neighbors, minimum distance) and are not designed for quantitative analysis, as they do not preserve metric properties of the original data. See https://github.com/jlmelville/uwot for great in-depth discussions on UMAPs.\n\nRemark (Yes, UMAPs are also a “dimension reduction,” but they serve a very different purpose). You can think of UMAPs as a very specific dimension reduction where \\(k\\) is always 2 or 3. However, UMAPs serve a very different role – very few single-cell analyses relies on the UMAP coordinates, but many single-cell methods rely upon the low-dimensional vector \\(Z_{i,\\cdot}\\in\\mathbb{R}^k\\) from a dimension reduction method.\nThe reason is that UMAPs are usually solely for visualizations and nothing else. There is almost no concrete relation between the distance between any two cells based on their UMAP coordinates and their true dissimilarity based on their gene expression profiles. The most common analogy is to think of visualizing the Earth on a 2D map, see Figure 3.18. No matter how you try to draw the world on a sheet of paper, there’s always some unnatural distortion. If you interpret this map very literally, it seems like California is further away from Japan than Spain is, and Antartica is almost as large as all the other continents combined. The point is, every low-dimensional projection always sacrifices some aspects, and when you try projecting high-dimensional data to specifically only 2 dimensions, a lot of large-scale qualities “break.”\nSee (Prater and Lin 2024) for more discussions about how to think about UMAPs, Figure 3.19. There’s a big debate on what is “acceptable” or “not acceptable” to infer from UMAPS. See (Chari and Pachter 2023; Lause, Berens, and Kobak 2024), and a recent large controversy in genetics (https://www.science.org/content/article/huge-genome-study-confronted-concerns-over-race-analysis).\nMy take: UMAPs are a useful visualization tool (in the sense that it’ll be ridiculous to claim we should never visualize our data, and we should just accept every visualization tool will have its own drawbacks). However, you should have quantitative metrics planned out prior to visualization so you’re not solely relying on UMAPs for your analysis. UMAPs are useful for giving you a sense of the data quality and broad cellular relations before investing time to carefully quantify the biology of interest. Your biological evidence should not solely rely on UMAPs.\n\n\n\n\n\n\n\n\nFigure 3.18: Map of the world\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.19: (Top row): UMAP computed from 3 different random seeds, to illustrated the “arbitrary” nature of how large “continents” of cells relate to one another. (Bottom left): t-SNE, which often are a lot more “spread out” than UMAPs in terms of how separated the “continents” are. (Bottom right): PCA, which is reproducible (i.e., no unexplainable extrinsic randomness during the computation), statistically rigorous and transparent (i.e., we know how to mathematically interpret any pair of cells). However, unlike UMAP and t-SNE, we often need many PCs to appropriate represent the data, and we can only visualize 2 PCs at a time. (Prater and Lin 2024)\n\n\n\n\n\n\n3.3.25 Relation to PCA\nUMAPs are most related to the Formulation #3 of PCA Equation 3.6. Specifically, instead of “\\(D_X\\)” and “\\(D_{XW}\\)” (originally measuring the distance between two cells using Euclidean distance), UMAPs instead use an adaptive kernel on a cell-cell graph to measure the similarity between two cells, and the loss function isn’t simply a difference between the two similarities.\n\n\n3.3.26 How to do this in practice\nSee the Seurat::RunUMAP function, illustrated in Figure 3.20. The resulting visualization is shown in Figure 3.21.\n\n\n\n\n\n\n\nFigure 3.20: The UMAP visualization step. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.21: Using the Seurat::DimPlot(pbmc, reduction = \"umap\") function to visualize the first two PCs. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n3.3.27 A brief note on other approaches\nBecause visualizing high-dimensional data will always be a need, and there’s never going to be an “optimal” way to do it, the visualization method of choice is going to change based on the community’s desires every 5-10 years. While currently UMAP is king (which dethroned t-SNE, who was the previous king), there’s definitely methods trying to dethrone UMAP. The method I’ve seen the most popularity to dethrone UMAP so far is PaCMAP, but as of 2024-25, UMAP is still by far the most common visualization method. See (Huang et al. 2022) for a broad benchmarking of such methods.\nThere are two lines of work I’ll mention that might be of interest for the statistical students. One is developing methods to assess how “distorted” the visualization is, see (Johnson, Kath, and Mani 2022; Xia, Lee, and Li 2024). Another is to trying to prove (using a formal statistical model) how well these visualization methods actually “cluster” the cells, see (Arora, Hu, and Kothari 2018; Cai and Ma 2022). ::::::::::::::::::::::::::::::::::::::::\n\n\n\n\nAhlmann-Eltze, Constantin, and Wolfgang Huber. 2023. “Comparison of Transformations for Single-Cell RNA-Seq Data.” Nature Methods, 1–8.\n\n\nArora, Sanjeev, Wei Hu, and Pravesh K Kothari. 2018. “An Analysis of the t-Sne Algorithm for Data Visualization.” In Conference on Learning Theory, 1455–62. PMLR.\n\n\nBecht, Etienne, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. 2019. “Dimensionality Reduction for Visualizing Single-Cell Data Using UMAP.” Nature Biotechnology 37 (1): 38.\n\n\nCai, T Tony, and Rong Ma. 2022. “Theoretical Foundations of t-Sne for Visualizing High-Dimensional Clustered Data.” Journal of Machine Learning Research 23 (301): 1–54.\n\n\nCarbonetto, Peter, Abhishek Sarkar, Zihao Wang, and Matthew Stephens. 2021. “Non-Negative Matrix Factorization Algorithms Greatly Improve Topic Model Fits.” arXiv Preprint arXiv:2105.13440.\n\n\nChari, Tara, and Lior Pachter. 2023. “The Specious Art of Single-Cell Genomics.” PLOS Computational Biology 19 (8): e1011288.\n\n\nChen, Wenan, Yan Li, John Easton, David Finkelstein, Gang Wu, and Xiang Chen. 2018. “UMI-Count Modeling and Differential Expression Analysis for Single-Cell RNA Sequencing.” Genome Biology 19 (1): 70.\n\n\nGhojogh, Benyamin, Ali Ghodsi, Fakhri Karray, and Mark Crowley. 2021. “Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey.” arXiv Preprint arXiv:2101.00734.\n\n\nHafemeister, Christoph, and Rahul Satija. 2019. “Normalization and Variance Stabilization of Single-Cell RNA-seq Data Using Regularized Negative Binomial Regression.” Genome Biology 20 (1): 1–15.\n\n\nHaghverdi, Laleh, Florian Buettner, and Fabian J Theis. 2015. “Diffusion Maps for High-Dimensional Single-Cell Analysis of Differentiation Data.” Bioinformatics 31 (18): 2989–98.\n\n\nHeumos, Lukas, Anna C Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D Lücken, et al. 2023. “Best Practices for Single-Cell Analysis Across Modalities.” Nature Reviews Genetics 24 (8): 550–72.\n\n\nHou, Wenpin, Zhicheng Ji, Hongkai Ji, and Stephanie C Hicks. 2020. “A Systematic Evaluation of Single-Cell RNA-Sequencing Imputation Methods.” Genome Biology 21 (1): 1–30.\n\n\nHuang, Haiyang, Yingfan Wang, Cynthia Rudin, and Edward P Browne. 2022. “Towards a Comprehensive Evaluation of Dimension Reduction Methods for Transcriptomic Data Visualization.” Communications Biology 5 (1): 719.\n\n\nJohnson, Eric M, William Kath, and Madhav Mani. 2022. “EMBEDR: Distinguishing Signal from Noise in Single-Cell Omics Data.” Patterns 3 (3).\n\n\nKent, JT, John Bibby, and KV Mardia. 1979. Multivariate Analysis. Academic Press Amsterdam.\n\n\nKharchenko, Peter V, Lev Silberstein, and David T Scadden. 2014. “Bayesian Approach to Single-Cell Differential Expression Analysis.” Nature Methods 11 (7): 740.\n\n\nKorsunsky, Ilya, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang, Kevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya Raychaudhuri. 2019. “Fast, Sensitive and Accurate Integration of Single-Cell Data with Harmony.” Nature Methods, 1–8.\n\n\nLause, Jan, Philipp Berens, and Dmitry Kobak. 2021. “Analytic Pearson Residuals for Normalization of Single-Cell RNA-Seq UMI Data.” Genome Biology 22: 1–20.\n\n\n———. 2024. “The Art of Seeing the Elephant in the Room: 2D Embeddings of Single-Cell Data Do Make Sense.” bioRxiv.\n\n\nLin, Kevin Z, Yixuan Qiu, and Kathryn Roeder. 2022. “eSVD: Cohort-Level Differential Expression in Multi-Individual Single-Cell RNA-Seq Data Using Exponential-Family Embeddings.” (In Preparation).\n\n\n———. 2024. “eSVD-DE: Cohort-Wide Differential Expression in Single-Cell RNA-Seq Data Using Exponential-Family Embeddings.” BMC Bioinformatics 25 (1): 113.\n\n\nLopez, Romain, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir Yosef. 2018. “Deep Generative Modeling for Single-Cell Transcriptomics.” Nature Methods 15 (12): 1053.\n\n\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.” Genome Biology 15 (12): 550.\n\n\nLuecken, Malte D, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese, Marta Interlandi, Michaela F Müller, Daniel C Strobl, et al. 2022. “Benchmarking Atlas-Level Data Integration in Single-Cell Genomics.” Nature Methods 19 (1): 41–50.\n\n\nLuecken, Malte D, and Fabian J Theis. 2019. “Current Best Practices in Single-Cell RNA-Seq Analysis: A Tutorial.” Molecular Systems Biology 15 (6): e8746.\n\n\nMa, Rong, Eric D Sun, David Donoho, and James Zou. 2024. “Principled and Interpretable Alignability Testing and Integration of Single-Cell Data.” Proceedings of the National Academy of Sciences 121 (10): e2313719121.\n\n\nMaden, Sean K, Sang Ho Kwon, Louise A Huuki-Myers, Leonardo Collado-Torres, Stephanie C Hicks, and Kristen R Maynard. 2023. “Challenges and Opportunities to Computationally Deconvolve Heterogeneous Tissue with Varying Cell Sizes Using Single-Cell RNA-Sequencing Datasets.” Genome Biology 24 (1): 288.\n\n\nMcGinnis, Christopher S, Lyndsay M Murrow, and Zev J Gartner. 2019. “DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing Data Using Artificial Nearest Neighbors.” Cell Systems 8 (4): 329–37.\n\n\nPrater, Katherine E, and Kevin Z Lin. 2024. “All the Single Cells: Single-Cell Transcriptomics/Epigenomics Experimental Design and Analysis Considerations for Glial Biologists.” Glia.\n\n\nQian, Kun, Shiwei Fu, Hongwei Li, and Wei Vivian Li. 2022. “scINSIGHT for Interpreting Single-Cell Gene Expression from Biologically Heterogeneous Data.” Genome Biology 23 (1): 1–23.\n\n\nRisso, Davide, John Ngai, Terence P Speed, and Sandrine Dudoit. 2014. “Normalization of RNA-Seq Data Using Factor Analysis of Control Genes or Samples.” Nature Biotechnology 32 (9): 896–902.\n\n\nSalzberg, Steven L. 2018. “Open Questions: How Many Genes Do We Have?” BMC Biology 16 (1): 94.\n\n\nSarkar, Abhishek, and Matthew Stephens. 2021. “Separating Measurement and Expression Models Clarifies Confusion in Single Cell RNA-Seq Analysis.” Nature Genetics 53 (6): 770–77.\n\n\nSaunders, Lauren M, Sanjay R Srivatsan, Madeleine Duran, Michael W Dorrity, Brent Ewing, Tor H Linbo, Jay Shendure, et al. 2023. “Embryo-Scale Reverse Genetics at Single-Cell Resolution.” Nature 623 (7988): 782–91.\n\n\nSetty, Manu, Vaidotas Kiseliovas, Jacob Levine, Adam Gayoso, Linas Mazutis, and Dana Pe’er. 2019. “Characterization of Cell Fate Probabilities in Single-Cell Data with Palantir.” Nature Biotechnology 37 (4): 451–60.\n\n\nTipping, Michael E, and Christopher M Bishop. 1999. “Probabilistic Principal Component Analysis.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61 (3): 611–22.\n\n\nTownes, F William, Stephanie C Hicks, Martin J Aryee, and Rafael A Irizarry. 2019. “Feature Selection and Dimension Reduction for Single-Cell RNA-seq Based on a Multinomial Model.” Genome Biology 20 (1): 1–16.\n\n\nTran, Hoa Thi Nhu, Kok Siong Ang, Marion Chevrier, Xiaomeng Zhang, Nicole Yee Shin Lee, Michelle Goh, and Jinmiao Chen. 2020. “A Benchmark of Batch-Effect Correction Methods for Single-Cell RNA Sequencing Data.” Genome Biology 21 (1): 1–32.\n\n\nVan Dijk, David, Roshan Sharma, Juozas Nainys, Kristina Yim, Pooja Kathail, Ambrose J Carr, Cassandra Burdziak, et al. 2018. “Recovering Gene Interactions from Single-Cell Data Using Data Diffusion.” Cell 174 (3): 716–29.\n\n\nXi, Nan Miles, and Jingyi Jessica Li. 2021. “Benchmarking Computational Doublet-Detection Methods for Single-Cell RNA Sequencing Data.” Cell Systems 12 (2): 176–94.\n\n\nXia, Lucy, Christy Lee, and Jingyi Jessica Li. 2024. “Statistical Method scDEED for Detecting Dubious 2D Single-Cell Embeddings and Optimizing t-SNE and UMAP Hyperparameters.” Nature Communications 15 (1): 1753.\n\n\nYazar, Seyhan, Jose Alquicira-Hernandez, Kristof Wing, Anne Senabouth, M Grace Gordon, Stacey Andersen, Qinyi Lu, et al. 2022. “Single-Cell eQTL Mapping Identifies Cell Type-Specific Genetic Control of Autoimmune Disease.” Science 376 (6589): eabf3041.\n\n\nYoung, Matthew D, and Sam Behjati. 2020. “SoupX Removes Ambient RNA Contamination from Droplet-Based Single-Cell RNA Sequencing Data.” Gigascience 9 (12): giaa151.\n\n\nZhang, Yuqing, Giovanni Parmigiani, and W Evan Johnson. 2020. “ComBat-Seq: Batch Effect Adjustment for RNA-Seq Count Data.” NAR Genomics and Bioinformatics 2 (3): lqaa078.\n\n\nZhao, Ruzhang, Jiuyao Lu, Weiqiang Zhou, Ni Zhao, and Hongkai Ji. 2024. “A Systematic Evaluation of Highly Variable Gene Selection Methods for Single-Cell RNA-Sequencing.” bioRxiv, 2024–08.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#sec-chapter_3_tech",
    "href": "chapter3_rna.html#sec-chapter_3_tech",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.2 How scRNA-seq works as a technology",
    "text": "4.2 How scRNA-seq works as a technology\n\nRemark (Single-cell vs. single-nuclei). While I use the word “cell,” sometimes in certain biological tissues, the more accurate word is “nuclei.” The computational methods for single-cell RNA-seq data (scRNA-seq) is essentially the same as single-nuclei RNA-seq data (snRNA-seq). What’s different is the biological context. For instance, some biological processes happen only in the nucleus, so when you collect snRNA-seq data, you might be overly enriched to discover those processes. See (Denisenko et al. 2020; Ding et al. 2020) for example.\n\nSee Figure 4.1 for an illustration of how the 10x protocol works. We’ll explain this next (and more importantly, why it’s important for a statistician to know how single-cell sequencing works)3.\n\n\n\n\n\n\n\nFigure 4.1: Illustration of the how scRNA-seq works (this is specific for 10x Chromium 3’, but most single-cell protocols follow this rough strategy). (Swaminath and Russell 2024)\n\n\n\n\n\nTissue dissociation and cell isolation: The tissue of interest is dissociated into a suspension of individual cells using enzymatic or mechanical methods. Single cells are isolated, often using fluorescence-activated cell sorting (FACS) or microfluidic techniques. This step is why some biological systems require “single-nuclei” instead of “single-cell.”\n\n\nRemark (Tissue dissociation is typically invasive). The main non-invasive ways is through blood, skin, and urine/feces. Otherwise, unlike MRIs, this is not easy to access tissues in humans. Hence, human samples from most non-accessible biological systems (i.e., brain, lung, heart, etc.) are usually obtain with prior consent from donors after surgery, biopsy, transplant, etc.\n\n\nDroplet formation: Cells are encapsulated into tiny droplets along with barcoded beads. Each bead contains unique molecular identifiers (UMIs) and cell-specific barcodes to label RNA molecules from individual cells. This step is the how we can keep track and eventually figure out which data came from which cell.4\n\n\nCell lysis and RNA capture: Cells are lysed within the droplets, and the released RNA molecules hybridize to oligonucleotides on the barcoded beads. This step ensures that all RNA from a single cell is tagged with the same cell-specific barcode.\n\n\nRemark (Lysing prevents longitudinal profile). This step is why, by default, scRNA-seq cannot do longitudinal experiments on literally the same cells. You have to destroy the cell to sequence it.\n\n\nReverse transcription, cDNA synthesis, and PCR amplification: The captured RNA is reverse-transcribed into complementary DNA (cDNA), incorporating the barcodes and UMIs to track the origin of each molecule. The cDNA is amplified via PCR to generate enough material for sequencing5. UMIs help correct for amplification biases by identifying and collapsing duplicate reads from the same original RNA molecule. See Figure 4.2 for the benefit the UMI.\n\n\n\n\n\n\n\n\nFigure 4.2: Illustration of the importance of UMIs. From https://help.geneiousbiologics.com/hc/en-us/articles/4781289585300-Understanding-Single-Cell-technologies-Barcodes-and-UMIs.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: How the Poly(dT) on the 10x bead sequence binds to the Poly(A) tail at the 3’ end of an mRNA fragment. From https://www.10xgenomics.com/support/universal-three-prime-gene-expression/documentation/steps/library-prep/chromium-gem-x-single-cell-3-v4-gene-expression-user-guide.\n\n\n\n\n\nRemark (The importance and impact of PCR). Some biological modalities cannot be PCR’d reliably. This is why (as we’ll see later in the course) certain omics are a lot harder to get data of at single-cell resolution. The PCR step is important because you can imagine it’s very hard to sequence every fragment in a cell. There might be many fragments, but you won’t be able to sequence many of them. By amplifying the content in a cell, we can more reliably sequence the contents in that cell. The UMIs become very important because we want to make sure our eventual gene expression counts are the fragments from the original cell, not counts due to PCR amplification.\n\n\nLibrary preparation and high-throughput sequencing: Amplified cDNA is fragmented (if necessary) and adapters are added to prepare the material for sequencing. These adapters are required for sequencing compatibility and indexing. The prepared library is sequenced on a next-generation sequencing platform6, generating short reads7 that include both the transcript sequence and the associated barcodes and UMIs\n\n\nRemark (The impact of “short reads”). Short reads, typically around 50-400 base pairs long8, are the output of most high-throughput sequencing platforms (i.e., a limitation of using Illumina sequencing, for example). While they provide highly accurate and cost-effective data, their limited length can pose challenges in resolving complex regions of the genome, such as repetitive sequences, or distinguishing between transcript isoforms in RNA-seq data. These limitations can obscure biological insights, particularly in applications requiring full-length transcript information or precise structural variation analysis.\n\n\nRemark (Limitation of budget). The sequencing step is a costly step (in terms of money) in a data generation process. The more money you spend, the more sequencing depth you can get. (Of course, this is also dictated by the number of cells in your experiment, the cell quality, the efficacy of the sequencing protocol, etc.) However, if you don’t sequence using a large financial budget, then your count matrix \\(X\\) will have even more 0’s. This means you have less “resolution” to discover biological insights9.\n\n\nMapping and alignment: Sequencing reads are mapped to a reference genome or transcriptome to identify the originating genes. UMIs are used to remove PCR duplicates, ensuring accurate quantification of transcripts. This is a classic bioinformatics task – given a sequencing fragment (i.e., the literal basepair sequence) and a reference genome, figure out where in the reference genome this sequencing fragment should map to. If you are doing a typical 10x scRNA-seq experiment, the software to do this (made by 10x) is called CellRanger10.\n\n\nRemark 4.1. Remark (A lot of fascinating biology is thrown out). During the mapping and alignment step, many sequencing reads are often discarded because they do not align to the reference genome or transcriptome used in the analysis. This includes reads from bacteria and viruses, which could reveal insights into host-microbe interactions or infections (Arabi et al. 2023; Sato et al. 2018). Similarly, long non-coding RNAs (lncRNAs) (Mattick et al. 2023), genetic variation (including allele-specific expression) (Robles-Espinoza et al. 2021), alternative splicing (J. Chen and Weiss 2015), and untranslated regions (UTRs) of mRNAs are frequently overlooked in standard analyses despite their crucial roles in gene regulation, transcript stability, and translation (Arora et al. 2022). While these regions and features are often excluded for simplicity or due to lack of annotation, they represent a wealth of biological information that could lead to fascinating discoveries if properly explored.\n\n\nRemark 4.2. Remark (Why zero-inflation is not currently as popular for scRNA-seq analyses). As I mentioned in Section 3.3.13, modeling zero-inflation is not very common anymore. (This is also why we typically do not use an imputation step beyond the usual dimension reduction.) This was shown for droplet-based sequencing protocols (Svensson 2020), alongside its replies and responses (Cao et al. 2021; Svensson 2021). You might also be interested as well to read more about where the 0’s in a count matrix could come from during the entire sequencing protocol (Jiang et al. 2022), and how UMIs have contributed to not needing to worry about zero-inflation as much anymore (W. Chen et al. 2018; Kim, Zhou, and Chen 2020).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#clustering",
    "href": "chapter3_rna.html#clustering",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.3 Clustering",
    "text": "4.3 Clustering\n\nInput/Output.\nThe input to clustering is a normalized matrix \\(X\\in \\mathbb{R}^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes) and typically a clustering resolution, and the output is a partitioning of all the cells. That is, if there are estimated to be \\(K\\) clusters, each cell is assigned to one of the \\(K\\) clusters.\n\n\n4.3.1 What is a “cell type”? No, seriously. What is it?\nThe concept of a “cell type” is foundational yet complex in biology. Historically, cell types have been defined by shared structural and functional characteristics, making them distinct from other cells in an organism. However, this simplicity belies a deeper challenge: what features truly define a cell type? Modern research has shown that cell types exhibit diverse properties across molecular (i.e., the mRNA fragments and proteins in the cell), morphological (i.e., the shape of the cell), spatial (i.e., where the cell is in the human body), ancestry (i.e., where the cell originated from), and functional (i.e., the purpose of this cell) dimensions, often making it difficult to draw clear boundaries. For example:\n\nExample of obvious distinctions: Obvious differences in cells are often rooted in their structure and function. Heart muscle cells contract using striated fibers, while neurons transmit signals via dendrites and axons, making their structures and functions unmistakably distinct.\nExample of not-obvious distinctions: Neurons in the brain have been categorized based on their shape, connectivity, and gene expression patterns, but these dimensions do not always align, leading to ambiguities in classification.\n\nFor example, in many “atlas” projects (i.e., big scientific collaborators that aim to exhaustively profile many cells to understand the full diversity of cells), there is a very fundamental question: “How many cell types are there?” This is illustrated in Figure 4.411, where after exhaustively profile 4 brains from multiple brain regions, the authors defined 31 “superclusters” (shown as different colors) that separate into 3313 “subclusters.”\n\n\n\n\n\n\n\nFigure 4.4: From (Siletti et al. 2023).\n\n\n\n\nAdvances in single-cell transcriptomics have revolutionized how we define cell types. By profiling thousands of genes simultaneously, researchers can cluster cells into groups based on gene expression patterns, creating detailed cell taxonomies. Yet, even this high-resolution approach raises questions about granularity—should closely related clusters represent distinct cell types or variations within the same type? Additionally, transcriptomic definitions often need validation through other modalities, such as spatial organization, connectivity, or functional roles. See Figure 4.5 for illustrations of other biotechnology assays to define cell types. Despite these challenges, integrating these diverse perspectives is essential for understanding how cells contribute to the structure and function of tissues and organs in both health and disease.\n\n\n\n\n\n\n\nFigure 4.5: Illustrating how to define cell types (in this case, specific to the brain). (Zeng 2022)\n\n\n\n\n\nRemark (Personal opinion: Being stunlocked on how to define “cell type” in your paper is the rite of passage to becoming a cell biologist). The more you work closer to biology, the more you’ll realize that answering “What is a cell type?” transforms from a seemingly useless pedagogical question into insanely difficult question that requires you to dig deep into biology. For instance, aging research is currently at the forefront of a lot of biomedical research. However, consider Figure 4.6. Does a cell switch it’s “cell type” as we age? (Clearly, the answer must be “yes” to some degree, since all our cells originate from an embryo.) However, when we age from 70 to 80 years old, do our cells change their “cell type” as certain cells start losing their intended function?\nThe questions can get even crazier: Suppose I “discover a new cell type” in my high-profile scRNA-seq analysis. What does that mean? How do I convey to another researcher what defines this cell type? How am I confident that this “cell type” can be found in donors not in my dataset? (Our notation of cell type would be quite useless if it only existed in one donor or one dataset.)\n\n\n\n\n\n\n\n\nFigure 4.6: From (Zeng 2022).\n\n\n\n\nWe will focus on how to cluster cells in the remainder of this section, but we’ll discuss how to label the clusters as “cell types” in Section 4.6, and how to infer temporal relations in Section 4.7.\n\n\n4.3.2 Clustering cells into cell types or cell states\nClustering is a central step in single-cell RNA-sequencing analysis, allowing researchers to group cells with similar gene expression profiles into distinct clusters that represent cell types or states. Methods like Louvain or Leiden clustering are commonly used for this purpose, leveraging the low-dimensional representation from Principal Component Analysis (PCA) to identify patterns of cellular heterogeneity. These algorithms operate on a graph-based representation of the data, where cells are nodes and edges reflect similarities in their PCA embeddings. The goal is to partition cells into cohesive groups that capture biologically meaningful variation, such as distinct immune cell types or developmental stages. By clustering cells, researchers can uncover the diversity within a tissue or system and generate hypotheses about the roles of different cell populations in health and disease.\n\n\n4.3.3 The standard procedure: Louvain clustering\nLouvain clustering is a graph-based community detection algorithm widely used in single-cell RNA-seq (scRNA-seq) analysis to group cells into clusters representing cell types or states. The algorithm operates on a nearest-neighbor graph constructed from the low-dimensional representation of the data, such as the first few principal components obtained via Principal Component Analysis (PCA). This methods needs one tuning parameter \\(\\gamma\\), which denotes the “resolution” of the method.\n\nInitialization (Graph construction): The first step in Louvain clustering is to construct a graph \\(G = (V, E)\\), where:\n\\(V\\) represents the set of nodes (cells),\n\\(E\\) represents the edges between nodes, with weights \\(w_{ij}\\) quantifying the similarity between cells \\(i\\) and \\(j\\).\n\nThe edge weights \\(w_{ij}\\) are typically computed using a similarity metric, such as Euclidean distance or cosine similarity, and thresholded to retain only the \\(k\\)-nearest neighbors for each node.\n\nThe objective (Modularity optimization): The goal of Louvain clustering is to maximize the modularity \\(Q\\) of the graph, defined as: \\[\nQ = \\frac{1}{2m} \\sum_{i,j} \\left( w_{ij} - \\gamma \\frac{k_i k_j}{2m} \\right) \\delta(c_i, c_j),\n\\] where:\n\\(w_{ij}\\) is the weight of the edge between nodes \\(i\\) and \\(j\\),\n\\(\\gamma\\) is the resolution of the desired clustering (a larger number means smaller cluster sizes),\n\\(k_i = \\sum_j w_{ij}\\) is the sum of edge weights for node \\(i\\),\n\\(m = \\frac{1}{2} \\sum_{i,j} w_{ij}\\) is the total weight of all edges in the graph,\n\\(c_i\\) is the community assignment for node \\(i\\),\n\\(\\delta(c_i, c_j)\\) is an indicator function that equals 1 if nodes \\(i\\) and \\(j\\) belong to the same community, and 0 otherwise.\n\nThe modularity measures the quality of a partition by comparing the observed edge weights within communities to those expected under a random graph with the same degree distribution. (Conceptually, you can see that \\(k_i k_j/(2m)\\) represents the expected edge weight under a random graph model – a sort of “background distribution” adjustment.)\n\nThe estimation procedure (Iterative refinement): Louvain clustering proceeds iteratively:\n\n\nEach node is initially assigned to its own community.\nNodes are moved to neighboring communities if this increases the modularity \\(Q\\).\nCommunities are aggregated into “super-nodes,” and the process repeats on the coarsened graph.\n\nThe algorithm terminates when no further modularity improvement is possible.\nIn the context of scRNA-seq, Louvain clustering is applied to the nearest-neighbor graph of cells constructed from their PCA embeddings. The resulting clusters correspond to groups of cells with similar gene expression profiles, often representing distinct cell types or states. This method is computationally efficient and well-suited for the large, sparse graphs typical of scRNA-seq data. Louvain clustering’s modularity-based approach provides a flexible framework for uncovering cellular heterogeneity, making it a cornerstone of single-cell analysis pipelines.\n\nRemark (Leiden as an improvement over Louvain). The Leiden algorithm (Traag, Waltman, and Van Eck 2019) improves upon Louvain by addressing issues of cluster quality and stability. While Louvain can produce disconnected or poorly connected clusters, Leiden guarantees that clusters are well-connected and more robust. This makes Leiden particularly suitable for high-dimensional and noisy datasets, such as those in single-cell RNA-seq analyses.\n\n\nRemark (You specify a resolution, not the number of clusters). In many clustering algorithms, particularly graph-based methods like Louvain or Leiden, you specify a resolution parameter that influences the granularity of the clustering. A higher resolution typically results in more, smaller clusters, while a lower resolution produces fewer, larger clusters. The number of clusters is not fixed beforehand but emerges as a result of the resolution and the underlying structure of the data.\n\n\n\n4.3.4 How to do this in practice\nSee the Seurat::FindClusters function, illustrated in Figure 4.7. The resulting visualization is shown in Figure 4.8.\n\n\n\n\n\n\n\nFigure 4.7: The Louvian clustering step. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: Using the Seurat::DimPlot(pbmc, reduction = \"pca\", group.by = \"RNA_snn_res.0.5\") (left) function to visualize the clusters.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Using the Seurat::DimPlot(pbmc, reduction = \"umap\", group.by = \"RNA_snn_res.0.5\") (right) function to visualize the clusters. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\nRemark 4.3. Remark (Clustering is often an exploratory step, which then allows you focus on key populations of interest later on in your research project). As you can see here, the clustering role (together with the previous visualization steps and the cell-type labeling to be discussed in Section 4.6) is to provide you a high-level overview of what your dataset contains. After all, when you get a scRNA-seq data fresh out of a wetlab experiment, you don’t actually know what kind of cells are in your dataset. (No dataset comes automatically labeled with cell types.)\nUsually in your paper, after determining the clusters (and their cell types), you will spend a considerable portion of the paper going in-depth on just a few key cell types.\n\n\nRemark (Personal opinion: Make a bit too many clusters, and manually adjust/merge). Starting with a higher number of clusters can capture finer granularity and prevent important subpopulations from being overlooked. You can then manually combine clusters based on biological knowledge or other criteria, ensuring the final groupings are both meaningful and interpretable.\nWhile this might seem sacrilegious to statistical hard-liners, the reality of scRNA-seq analyses is that: 1) you don’t want technical artifacts (from sequencing) or numerical quirks (based on the literal computational method being used) to dictate certain cell clusters (sometimes certain clusters are way too small to be meaningful), and 2) for the reality mentioned in Remark 4.3, it might not be great use to time to worry about the “perfect clustering straight out of a method.” As long as you can justify your actions biologically, you’ll be in good hands.\n\n\nRemark (Personal opinion: If you’re goal is to make an atlas, consider hierarchical clustering methods). Recall Figure 4.5. If the goal of your project is to delineate all the different types of biological variation in your dataset, you’re probably better off using a method that gives you a hierarchy of cell clusters (rather than one fixed clustering). Methods like PAGA (Wolf et al. 2019) and MRTree (Peng et al. 2021) are some (among many) that are useful for this exact task.\n\n\n\n4.3.5 A brief note on other approaches\nSee (S. Zhang et al. 2020; Yu et al. 2022) on benchmarking of various ways to cluster cells.\n\n\n4.3.6 Clustering cells into “meta-cells”\nClustering cells into “meta-cells” is a strategy primarily aimed at simplifying the analysis of single-cell data or increasing the robustness of downstream analyses. In this approach, individual cells are grouped into larger aggregates, or meta-cells, based on their similarity in gene expression profiles. Unlike clustering aimed at identifying biologically meaningful cell types or states, meta-cells are often created to reduce noise, handle sparsity, or improve computational efficiency by working with fewer, more aggregated units. By combining the counts of multiple cells into a single meta-cell, this technique can also mitigate technical variability, making patterns in the data easier to detect. While meta-cells may not represent distinct biological entities, they serve as a practical tool for gaining insights from large and noisy datasets.\n\n\n4.3.7 How to do this in practice\nThe method I’ve seen most commonly used for this task is MetaCell (Baran et al. 2019). These metacells are constructed using a graph-based partitioning method that leverages k-nearest neighbor (K-nn) similarity graphs to identify groups of cells with highly similar gene expression profiles. By grouping cells into metacells, the method effectively reduces noise while preserving meaningful biological variance, allowing for robust and unbiased exploration of transcriptional states, gradients, and cell types. This is illustrated in Figure 4.10.\n\n\n\n\n\n\n\nFigure 4.10: Schematic of Metacell. (Baran et al. 2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.11: Visualization of the cells and their meta-cells. (Baran et al. 2019)\n\n\n\n\n\n\n4.3.8 A brief note on other approaches\nSee a review in (Bilous et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#differential-expression",
    "href": "chapter3_rna.html#differential-expression",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.4 Differential expression",
    "text": "4.4 Differential expression\nDifferential expression (DE) analysis, illustrated in Figure 4.12, aims to identify genes that are expressed at significantly different levels between two groups of cells. We will discuss two primary scenarios in the following section: 1) either where the groups are estimated from data, or 2) where the groups are determined by the experimental design. This comparison helps uncover genes that may play critical roles in the biological processes or phenotypic changes under investigation. In the context of single-cell RNA-seq, DE analysis must account for the unique challenges of sparsity and variability in the data. The primary goal is to highlight key molecular drivers of the experimental conditions, providing insights into underlying mechanisms or potential targets for therapeutic intervention. This analysis is foundational for linking cellular behavior to experimental manipulations or disease states.\n\n\n\n\n\n\n\nFigure 4.12: Illustration of how to think about DE. From https://www.sc-best-practices.org/conditions/differential_gene_expression.html.\n\n\n\n\nSee https://www.sc-best-practices.org/conditions/differential_gene_expression.html for more background.\n\nInput/Output. The input to differential expression is matrix \\(X\\in \\mathbb{R}^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes) split into two groups, and the output is two numbers per gene. One is the p-value for the gene (denoting how significant that gene deviates from the null distribution where there is no difference in the gene expression between the two groups. A smaller p-value denotes more evidence against the null), and the other is log fold change (denoting, roughly, how much larger the mean expression in one group is larger than the mean expression of the other group). The p-value is a number between \\(0\\) and \\(1\\), and the log-fold change is any real value where both the sign and magnitude are important.\n\n\nRemark (Intent of Differential Expression Analysis). The primary intent of differential expression (DE) analysis in single-cell RNA-seq is not usually to provide definitive evidence for interventions, such as proving a drug’s efficacy. Instead, DE in single-cell studies is often exploratory, aimed at uncovering basic biological insights. It addresses questions like, “We know there is a difference between these two conditions, but what is driving that difference at the cellular level?” The goal is to identify genes that exhibit meaningful changes in expression, contributing to our collective understanding of gene functions and their roles in cellular processes. These findings expand the compendium of knowledge that informs future experiments and hypotheses.\n\n\nRemark (Pleiotropy and the Need for Continued DE Analyses). There are a ton of existing databases that document what the function of each gene is. For instance, you can see collections of genes (https://www.genome.jp/kegg/kegg2.html and https://www.gsea-msigdb.org/gsea/msigdb) or individual genes (https://www.ncbi.nlm.nih.gov/gene and https://www.genecards.org/). You might wonder – given this wealth of knowledge, why bother do any more DE analyses on your own?\nPleiotropy, where a single gene influences multiple, seemingly unrelated biological processes, underscores the importance of continued differential expression (DE) analyses even in the presence of well-annotated pathways like KEGG. While these resources provide invaluable insights into known gene functions and pathways, they often fail to capture the full complexity of gene roles across diverse cellular contexts, conditions, or tissues. DE analysis allows us to uncover novel functions or unexpected roles for genes, especially in underexplored biological systems or conditions. Pleiotropic effects are context-dependent, and without systematically exploring gene expression across varied conditions, we risk oversimplifying or missing critical aspects of cellular and molecular biology.\n\n\n4.4.1 Hold on… isn’t DE just a t-test?\nSo why is DE even an interesting problem? Couldn’t we just use a t-test (parametric, asymptotic), Wilcoxon rank-sum test (nonparametric, asymptotic), or permutation test (nonparametric, exact)? Yes, you can.\nSo why is there a huge field studying DE in genomics, and why are there so many methods? Ultimately, the focus is on achieving greater statistical power or applying the methods more broadly across diverse datasets and experimental conditions. It boils down to engineering the underlying statistical model to do the following two things effectively:\n\nExtract the “meaningful” signal: While we could run any of the aforementioned tests on the observed single-cell data, the results might be misleading due to the presence of unwanted covariates. For example, significant genes might simply reflect technical artifacts, such as the correlation between gene expression and the number of cells with non-zero counts for that gene.\n\nAs we’ve discussed, these are cases where technical noise (i.e., noise induced by the measurement or sequencing process, and theoretically removable with perfect technology) has not been properly accounted for. The challenge, then, is to design a statistical model that “removes” this noise, isolating the “meaningful” biological signal. Without this step, DE results may reflect experimental artifacts rather than true biological differences.\n\nSharing information across genes to increase power, while controlling Type-1 error: Single-cell data is often much sparser than bulk sequencing data, leading to fewer detectable signals. This raises the question: can we pool information across genes to enhance the statistical power of our tests? For example, one might consider smoothing the expression matrix to reduce noise.\n\n(Recall that 70%+ of a single-cell dataset is 0’s. This means you should expect little power by default. Hence, different DE methods deploy different strategies to improve the power as much as possible.)\nHowever, pooling or smoothing introduces a delicate tradeoff. While it can increase power by borrowing strength from other genes, it risks inflating false positives if overdone. If the smoothing is too aggressive, what might have been a single DE gene could appear as a broader, artificial signal across multiple genes. Designing models that balance this tradeoff, while maintaining statistical validity, is a key challenge in single-cell DE analyses.\nHence, DE can be approached with classical tests, the nuances of single-cell data – ranging from technical noise to sparse data and confounding factors – necessitate specialized models and methods. The ultimate goal is to extract biologically meaningful insights while ensuring robust statistical inference.\n\n\n4.4.2 DE between two data-driven grouping of cells\nPerforming differential expression (DE) analysis between two data-driven groupings of cells, such as clusters identified during analysis, poses unique statistical challenges. A key issue is circularity: the same genes used to define the cell clusters may bias the DE results, complicating the interpretation of identified marker genes. This lack of statistical independence can undermine the validity of formal inference. However, despite these limitations, this type of DE analysis holds substantial exploratory value. It provides a practical way to highlight key genes that distinguish cell types or states. These genes are often called marker genes. The marker genes identified in this manner can still serve as useful entry points for understanding the biological roles of different cell groups, even if their statistical rigor is limited.\n\n\n4.4.3 Wilcoxon Rank-Sum Test for Differential Expression Testing\nThe Wilcoxon rank-sum test is a non-parametric statistical test used to assess whether the distributions of a gene’s expression values differ significantly between two groups of cells. It is particularly well-suited for single-cell RNA-seq data that has been normalized, as it does not assume the data follows a specific distribution, such as normality. The test is based on the ranks of the data rather than the raw expression values, making it robust to outliers and noise.\n\nRemark (Wilcoxon test after normalization and feature selection). The test below is typically done after we’ve normalized and found HVGs. As we’ll discuss in Section 4.4.15 next, this is typically before we any imputation or PCA (and we are not worried about batch effects in this scenario).\n\nBelow, we explain the Wilcoxon rank-sum test, generically applied to gene \\(j\\) (i.e., the \\(j\\)-th column of the normalized scRNA-seq matrix \\(X\\)). We will slightly overload the notation just to keep the notation of the Wilcoxon test itself simple.\n\nSetting up the hypothesis: Let the expression values in two groups of cells be \\(X_1, X_2, \\ldots, X_m\\) (group 1) and \\(Y_1, Y_2, \\ldots, Y_n\\) (group 2). The null and alternative hypotheses are: \\[\nH_0: \\text{The distributions of } X \\text{ and } Y \\text{ are identical.}\n\\] \\[\nH_a: \\text{The distribution of } X \\text{ is shifted relative to } Y.\n\\]\nRanking the data: All observations from both groups are combined and ranked from smallest to largest. Tied values are assigned the average of their ranks. Let \\(R(X_i)\\) represent the rank of \\(X_i\\) in the combined data.\nTest statistic: The test statistic \\(W\\) is calculated as the sum of ranks for one of the groups, typically group 1: \\[\nW = \\sum_{i=1}^m R(X_i).\n\\] Under the null hypothesis, \\(W\\) is expected to follow a known distribution based on the ranks.\nExpected Value and Variance Under \\(H_0\\): Under the null hypothesis, the expected value and variance of \\(W\\) are given by: \\[\n\\mathbb{E}(W) = \\frac{m(m + n + 1)}{2},\n\\] \\[\n\\text{Var}(W) = \\frac{mn(m + n + 1)}{12}.\n\\]\np-Value calculation:\n\nThe p-value is computed by comparing the observed \\(W\\) to its null distribution. For large sample sizes, \\(W\\) can be approximated by a normal distribution: \\[\nZ = \\frac{W - \\mathbb{E}(W) }{\\sqrt{\\text{Var}(W)}}.\n\\] The p-value is then derived from the standard normal distribution: \\[\np = 2 \\cdot P(Z &gt; |z|),\n\\] where \\(z\\) is the observed value of \\(Z\\).\nIn single-cell RNA-seq data, the Wilcoxon rank-sum test is used to compare the expression of each gene between two groups of cells, such as treated versus untreated cells or two clusters identified through clustering. Its non-parametric nature makes it robust to the sparsity and variability inherent in single-cell datasets, providing a reliable method for detecting genes with differential expression.\n\nRemark (Reminder: Invalid p-value in a literal statistical sense). Since the cell groups were defined by clustering, you are likely to find significant genes regardless. This is the “double-dipping” phenomenon. After all, the cells were organized into separate clusters because there were differences between the two cell groups.\nAnother way to describe this is that the null hypothesis is mis-calibrated. A typical Wilcoxon test is assuming under the null hypothesis that the two distributions \\(X\\) and \\(Y\\) are identical. This is not the case when you’re applying a Wilcoxon test on two cell clusters. This results in p-values that are more significant than they are actually.\nHowever, this is does not mean it’s useless to perform hypothesis testing in this situation. After all, our goal is to simply find marker genes to describe a cell cluster, and using hypothesis testing gives us a convenient tool to determine which genes are the most meaningful ones to describe what this cluster represents.\n\n\n\n4.4.4 How to do this in practice\nSee the Seurat::FindMarkers function, with more details described in https://satijalab.org/seurat/articles/de_vignette. This is illustrated in Figure 4.13.\n\n\n\n\n\n\n\nFigure 4.13: The differential expression step, where this example is done to find the “marker genes” for Cluster 2. By default, this is using the Wilcoxon test. You typically don’t need to run logfc.threshold = 0, min.pct = 0, min.cells.feature = 0, min.cells.group = 0. This is primarily done to ensure all the highly variable genes are tested. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html and https://satijalab.org/seurat/articles/de_vignette.\n\n\n\n\n\n\n4.4.5 What if you want “valid” p-value?\nThere are a few instances where you truly valid p-values. That is, you want a way to perform hypothesis testing that doesn’t inflate the p-values (i.e., does not make the p-values more significant than they should be). Two papers you can look into towards this end are count splitting (Neufeld et al. 2022) or modifying the null distribution (Song et al. 2023).\n\n\n4.4.6 A brief note on other approaches\nThere are an endless number of review and benchmarking papers for this topic. See (Das, Rai, and Rai 2022) for an overview of many methods, and (Dal Molin, Baruzzo, and Di Camillo 2017; Jaakkola et al. 2017; T. Wang et al. 2019; Li et al. 2022) for benchmarkings. This will also help you get a sense of how diversity of DE methods. Note that all the methods we mentioned so far are about differential mean (i.e., there is a shift in the average expression, relative to the variance). There are also mentions dedicated to differential variance (see (J. Liu, Kreimer, and Li 2023) for example) and differential distribution (see (Schefzik, Flesch, and Goncalves 2021) for example).\n\n\n4.4.7 (A brief reminder about multiple testing)\nIn many scientific studies, particularly in genomics and high-throughput experiments, researchers perform thousands of statistical tests simultaneously. For example, in differential expression analysis of RNA-seq data, each gene is tested for association with a condition or phenotype. However, performing multiple tests increases the likelihood of false positives—tests that appear significant purely by chance. This challenge necessitates methods to control error rates, ensuring that conclusions drawn from these analyses are reliable. While traditional corrections like the Bonferroni method are overly stringent and reduce power, modern approaches such as the Benjamini-Hochberg procedure provide a balance between identifying true positives and controlling the rate of false discoveries.\n\n\n4.4.8 Benjamini-Hochberg procedure\nThe Benjamini-Hochberg (BH) procedure is a widely used method to control the False Discovery Rate (FDR), defined as the expected proportion of false positives among all significant results. The procedure is particularly suited for large-scale testing scenarios where identifying as many true positives as possible is critical, while still limiting false discoveries.\n\nOrdered p-values: Let \\(p_1, p_2, \\ldots, p_m\\) represent the p-values for \\(m\\) hypothesis tests. First, these p-values are ordered in ascending order: \\[\np_{(1)} \\leq p_{(2)} \\leq \\cdots \\leq p_{(m)}.\n\\] The index \\((i)\\) denotes the position of the p-value in the sorted list.\nCritical threshold: For a chosen FDR level \\(\\alpha\\), the BH procedure identifies the largest \\(k\\) such that: \\[\np_{(k)} \\leq \\frac{k}{m} \\alpha.\n\\] Here:\n\\(k\\) is the rank of the p-value,\n\\(m\\) is the total number of hypotheses tested,\n\\(\\alpha\\) is the desired FDR threshold (e.g., 0.05).\nSignificant results: Once \\(k\\) is identified, all hypotheses corresponding to \\(p_{(1)}, p_{(2)}, \\ldots, p_{(k)}\\) are considered significant. This ensures that the expected proportion of false discoveries among these selected hypotheses is controlled at \\(\\alpha\\).\n\nThe BH procedure achieves a balance between discovery and error control. Unlike family-wise error rate (FWER) methods such as Bonferroni, which aim to control the probability of any false positives, the BH method allows for a controlled proportion of false positives. This flexibility makes it more powerful in scenarios with a large number of tests, where the likelihood of finding true discoveries would otherwise be severely diminished.\nThe Benjamini-Hochberg procedure is a cornerstone of modern statistical analysis in high-dimensional data contexts. By controlling the FDR, it provides a practical and interpretable way to identify meaningful signals while accounting for the multiple testing problem. This approach is particularly valuable in genomics, proteomics, and other fields involving large-scale hypothesis testing.\n\n\n4.4.9 How to do this in practice\nMost DE methods compute the adjusted p-values for you. However, if you need to to do it manually, you can use the stats::p.adjust function with the setting method=\"BH\".\n\n\n4.4.10 A brief note on other approaches\nBH can control a particular family of dependency structure called PRDS (Benjamini and Yekutieli 2001).\n\n\n4.4.11 The volcano plot\nA volcano plot is a scatterplot used in differential expression analysis to visualize the relationship between statistical significance and fold change for each gene. Genes are plotted with the magnitude of fold change on the x-axis and the negative logarithm of the p-value on the y-axis, making it easy to identify genes that are both significantly and highly differentially expressed. This intuitive layout highlights genes of interest as points far from the origin, often with up- and downregulated genes in separate clusters. See Figure 4.12 for an example of a volcano plot, and Figure 4.14 on how to make one.\n\n\n\n\n\n\n\nFigure 4.14: Using the EnhancedVolcano::EnhancedVolcano(cluster2.markers, lab = rownames(cluster2.markers), x = 'avg_log2FC', y = 'p_val') function to make the volcano plot. See the tutorial in https://bioconductor.org/packages/devel/bioc/vignettes/EnhancedVolcano/inst/doc/EnhancedVolcano.html.\n\n\n\n\n\n\n4.4.12 DE in human cohorts\nDifferential expression (DE) analysis in human cohorts introduces additional complexity due to the presence of numerous confounding variables. Factors such as age, sex, smoking status, genetic background, and even environmental exposures can influence gene expression and obscure the true biological signal related to the condition of interest. Effective DE analysis in this context requires robust statistical frameworks to adjust for these confounders, ensuring that observed differences in gene expression are genuinely associated with the biological or clinical variable being studied. While these adjustments add layers of complexity, they are critical for generating biologically meaningful and reproducible insights that can translate to broader human populations.\nThe broad strategies are:\n\nPseudo-bulk analysis (the most common approach): Pseudo-bulk analysis aggregates single-cell data from each individual into a “bulk” profile by summing or averaging gene expression counts across cells of the same type. This approach treats each individual as a unit of analysis, enabling the application of well-established bulk RNA-seq differential expression methods, such as edgeR (Robinson and Smyth 2007) or DESeq2 (Love, Huber, and Anders 2014). (This methods were created for bulk-sequencing data.) By effectively reducing the dimensionality of the data, pseudo-bulk analysis accounts for inter-individual variability, which is critical in human cohorts. This method is particularly advantageous in case-control studies, where differences in gene expression between cases and controls can be analyzed while maintaining statistical power and controlling for confounders like age, sex, or sequencing depth.\n\nPseudo-bulk samples are created by aggregating single-cell gene expression data across cells within each sample/replicate/donor (depending on the context of the analysis). For donor \\(r\\), the pseudo-bulk expression count \\(\\tilde{X}_{rj}\\) for gene \\(j\\) is obtained by summing the observed counts across all \\(n_{r}\\) cells belonging to that donor: \\[\n\\tilde{X}_{rj} = \\sum_{i \\in \\text{donor }r} X_{ij},\n\\] where \\(X_{ij}\\) is the observed count for gene \\(j\\) in cell \\(i\\) that came from donor \\(r\\). This aggregation reduces the sparsity inherent in single-cell data. Since some of the donors are “cases” and some are “controls”, pseudob-bulking allowing the use of standard bulk RNA-seq differential expression methods while preserving biological replicate structure.\n\nMixture modeling: Mixture modeling is a more nuanced approach that acknowledges the heterogeneity within individual samples by modeling gene expression distributions across all cells. Instead of aggregating data, this approach identifies subpopulations of cells with distinct expression profiles and estimates the contribution of each subpopulation to the overall gene expression. In case-control studies, mixture models can distinguish changes in cellular composition (e.g., shifts in cell type proportions) from changes in gene expression within a given cell type. Mixture modeling is computationally intensive but offers a detailed view of the biological processes underlying disease states in human cohorts. This is mainly using the lme4 R package (or packages that build upon this, like MAST::zlm), such as in (Y. Liu et al. 2023), or NEBULA (He et al. 2021).\n\nBriefly, in its simplest form, a mixture model accounts for the hierarchical structure of the data, where cells are nested within donors. Consider a single gene \\(j\\). The observed expression \\(X_{i(r),j}\\) for cell \\(i\\) from donor \\(r\\) is modeled as: \\[\nX_{i(r),j} \\sim P(X \\mid \\mu_r, \\sigma^2),\n\\] where \\(\\mu_r\\) is the donor-specific mean expression, and \\(\\sigma^2\\) captures the residual variability across cells within the donor. The donor-specific means \\(\\mu_r\\) are modeled as: \\[\n\\mu_r  \\sim \\mathcal{N}(\\beta_0 + \\beta_1 z_r, \\tau^2),\n\\] where \\(z_r\\) is an indicator for case (\\(z_r = 1\\)) or control (\\(z_r = 0\\)) status, \\(\\beta_1\\) is the effect of the condition, and \\(\\tau^2\\) captures the variability in expression across donors. This hierarchical mixture approach explicitly separates within-donor variability from between-donor differences, ensuring that the analysis correctly attributes variability to its appropriate source.\n\nDistributional analysis: Another strategy is to leverage the fact that you have a distribution of cells for each donor. Hence, you can compute the mean and variance of the gene expression for each donor, and compare the donors through their mean and variance. One example of this is eSVD-De (Lin, Qiu, and Roeder 2024). However, this can also be done fully non-parametrically, such as in IDEAS (M. Zhang et al. 2022).\n\n\n\n4.4.13 The standard procedure: DESeq2 (Pseudo-bulk analysis)\nDESeq2 (Love, Huber, and Anders 2014) models read counts \\(X_{ij}\\) for gene \\(j\\) in sample \\(i\\) using a negative binomial (NB) distribution12: \\[\nX_{ij} \\sim \\text{NB}(\\mu_{ij}, r_j),\n\\] where \\(X_{ij}\\) is the total counts in sample \\(i\\) for gene \\(j\\) (which is the sum across all the cells in sample \\(j\\)), and \\(\\mu_{ij} = s_i q_{ij}\\). Here, \\(s_j\\) is a normalization factor accounting for sequencing depth, and \\(q_{ij}\\) represents the normalized mean expression for gene \\(j\\) in sample \\(i\\). The NB distribution’s overdispersion is captured by \\(r_j\\), which varies across genes.\nThe design matrix \\(Z\\) specifies the \\(r\\) covariates of the samples (one of which is the experimental condition). Then, the log of the normalized mean expression \\(q_{ij}\\) is modeled as: \\[\n\\log q_{ij} = \\sum_{r} Z_{ir} \\beta_{jr},\n\\] where \\(\\beta_{ir}\\) are coefficients relating the covariates to the gene expression.\nDispersion parameters \\(r_j\\) are estimated using a shrinkage approach, where gene-wise estimates are pulled toward a trend fitted across genes with similar mean expression. Hypothesis testing for differential expression is based on Wald tests for the corresponding value of \\(\\beta_{jr}\\).\n\n\n4.4.14 A brief note on other approaches\nSee (Squair et al. 2021) and (Junttila, Smolander, and Elo 2022) for more references.\n\n\n4.4.15 Aside: DE testing on normalized vs. count data\nThere are two fundamentally different approaches to performing differential expression (DE) analysis in single-cell RNA-seq data:\n\nDE on the Original Count Data: This approach involves applying statistical methods directly to the raw count data obtained from sequencing. It accounts for the inherent technical noise and sparsity characteristic of single-cell data by using models tailored to handle count distributions (e.g., negative binomial models). This method preserves the original variability and avoids introducing biases from data manipulation. Most of the different DE methods occur in this category (which we’ll discuss more in Section 4.4.12).\n\n\nRemark 4.4. Remark (Personal guideline on when to use). In general, a lot of biological replicates (see Section 4.4.12) or have many covariates to adjust for, this is a more appealing option. Generally, you want to default to this option when you really want to have “valid” p-values (i.e., not just markers for convenient usage). That is, you expect your markers to be investigated in detail in followup work. This is because by modeling the original count nature, if you have an appropriate statistical model, you are more likely to have validity.\n\n\nDE on Processed Data: In this paradigm, the data undergoes preprocessing steps such as normalization, scaling, or transformation before DE analysis. Typically, when you’re using strategy, most people defer to the Wilcoxon test.\n\n\nRemark (Personal guideline on when to use). In general, this option is most appealing when you want something “quick and dirty,” such as getting some interesting genes in a preliminary analysis to look at. This is because you’re off-loading all your concerns about statistical modeling to how the data was processed (i.e., the normalization, batch correction, etc.).\n\nCaution on Processed Data: Do not impute or denoise your data by pooling information across genes or cells prior to DE analysis – unless you are fully aware of the consequences. Imputation or aggressive smoothing can introduce artificial correlations and inflate the signal, potentially leading to false positives and erroneous conclusions. This is because the “signal” from “true DE genes” is smoothed to impact the null genes. These processes can mask true biological variability and create spurious patterns that reflect the imputation model rather than the underlying biology. See Figure 4.15 for an illustration of this. There are some exceptions of this though – see eSVD-DE (Lin, Qiu, and Roeder 2024) and lvm-DE (Boyeau et al. 2023) for example.\n\n\n\n\n\n\n\nFigure 4.15: (Left) DE using the raw or normalized counts, which is what you should use. (Right) DE using the smoothed counts where the data is “smoothed” across all the genes. (Prater and Lin 2024)\n\n\n\n\nIn summary, while both paradigms aim to identify differentially expressed genes, they rely on different assumptions and have different risks associated with them. Performing DE on the original count data is generally safer and more reliable, provided that appropriate statistical models are employed. If you choose to perform DE on processed data, proceed with caution and be mindful of the potential pitfalls associated with data manipulation.\nSee (H. C. Nguyen et al. 2023) for empirical evidence on how DE interacts with batch-correction across different methods.\n\n\n4.4.16 Pooling of overdispersion parameters is usually the most DE methods “share information across genes,” if at all\nMost DE methods analyze one gene at a time, where each gene’s analysis does not influence another gene. As we mentioned above in Remark 4.4, we typically do not perform DE methods on the smoothed data. However, if any information is shared across genes, usually it is at most the overdispersion parameters \\(r_j\\) that is often “averaged/shrunk” across all the genes. See DESeq2 (Love, Huber, and Anders 2014).\n\n\n4.4.17 “DE” for a continuous covariate\nAs we’ll see later in Section 4.7, sometimes you want to see if a gene is correlated with a continuous covariate (often the estimated developmental stage of a cell). Typically, the way this is done is: fit a regression model (typically a form of GLM) to predict the continuous covariate based on a gene’s expression, and then perform a hypothesis test on whether the gene was relevant for the prediction. See (hou2023statistical?) and its related papers for how methods do this.\n\nRemark (Personal opinion: The distinct between categorical and continuous phenotypes). If you study a clinical phenotype (typically about the organism or overall tissue), you typically want to treat the phenotype as binary (or categorical). After all, a doctor cannot declare someone to “kinda” have cancer. You either have it or you don’t. (Think about how chaotic medication prescriptions, health insurance, follow-up treatments, etc. would be if doctors gave “kinda” assessments.) In medical practice, doctors are constantly updating the guidelines on how to diagnose samples accordingly discrete buckets of categorizations.\nIf you study a basic biology phenotype, you typically want to treat the phenotype as continuous variable. After all, nothing about a cell is ever discrete. Nothing is truly either “on” or “off”, and most ways to understand cells is along a gradient. For example, if you study DNA damage, it’s not very useful to simply know when a cell’s DNA is damaged. (It probably always is – however, most DNA damage present in a cell is probably minor and infrequent.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#pathway-analysis",
    "href": "chapter3_rna.html#pathway-analysis",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.5 Pathway analysis",
    "text": "4.5 Pathway analysis\nPathway analysis is a critical step in single-cell RNA-sequencing studies, providing a systems-level understanding of the biological processes and pathways underlying gene expression changes. Tools like Gene Ontology (GO) and Gene Set Enrichment Analysis (GSEA) are commonly used to identify functional themes within a set of differentially expressed or highly variable genes. Gene Ontology categorizes genes into hierarchical groups based on their roles in biological processes, molecular functions, or cellular components, offering a structured framework for interpreting gene expression patterns. GSEA takes this further by testing whether predefined gene sets, representing pathways or functional categories, are enriched in a given cell cluster or condition. These approaches are particularly valuable in single-cell analysis, where individual genes may have noisy or sparse expression patterns. By focusing on groups of related genes, pathway analysis mitigates noise, enhances biological interpretability, and highlights broader functional insights that single genes alone may not reveal.\n\nInput/Output. The input to a pathway analysis is: 1) a vector of \\(p\\) values, one for each gene, and 2) a database that contains many pathways (and which genes belong to that pathway). Importantly, this must be a named vector (i.e., you know which value corresponds to which gene). The output is one p-value per pathway which denotes how significant the genes in the pathway are not randomly distributed based on their value. That is, a significant pathway is one where many of genes are have simultaneously very large (or very small) values.\n\n\n4.5.1 The standard procedure: GSEA\nGene Set Enrichment Analysis (GSEA) is a widely used computational method designed to determine whether predefined sets of genes exhibit statistically significant differences in expression between two biological states. Unlike single-gene analyses, which focus on individual genes, GSEA evaluates groups of related genes (gene sets) to identify coordinated changes in expression patterns. This approach provides a more systems-level understanding of the underlying biology, particularly in settings where individual genes show subtle expression changes.\nThe goal of GSEA is to test whether a predefined gene set \\(S\\), such as a pathway or functional group, is overrepresented among the genes most differentially expressed between two conditions. GSEA leverages ranked gene lists, focusing on the distribution of gene set members within the list to detect enrichment. See Figure 4.16 for an illustration of how the method works visually.\n\nInput data: GSEA requires the following inputs:\n\n\nA ranked list of genes \\(G = \\{g_1, g_2, \\ldots, g_N\\}\\), sorted by their differential expression scores (e.g., fold change, t-statistics, or signal-to-noise ratio) between two conditions.\nA predefined gene set \\(S \\subseteq G\\), typically representing a biological pathway, molecular function, or cellular process.\n\n\nThe test statistic (Enrichment score, i.e., ES): The core of GSEA is the calculation of an enrichment score (ES), which reflects the degree to which the genes in \\(S\\) are concentrated at the top (or bottom) of the ranked list \\(G\\). The ES is computed by walking down the ranked list and increasing a running-sum statistic when encountering a gene in \\(S\\), or decreasing it otherwise: \\[\n\\text{ES} = \\max_k \\left| P_S(k) - P_G(k) \\right|,\n\\] where:\n\\(P_S(k)\\): Cumulative sum of weights for genes in \\(S\\) up to rank \\(k\\).\n\\(P_G(k)\\): Cumulative sum of weights for all genes up to rank \\(k\\).\n\nThe weights often depend on the absolute value of the gene’s differential expression score, emphasizing genes with larger effects.\n\nThe null distribution: To assess the significance of the observed ES, GSEA employs a permutation-based strategy:\n\n\nPermute the condition labels to generate randomized ranked lists.\nRecalculate the ES for the gene set \\(S\\) in each randomized list.\nCompare the observed ES to the null distribution of ES values from the permutations to compute a p-value.\n\n\nMultiple testing adjustment: Since many gene sets are tested in a typical GSEA analysis, corrections for multiple hypothesis testing are essential. GSEA uses the false discovery rate (FDR) to control for false positives, reporting gene sets with significant enrichment after correction.\n\n\n\n\n\n\n\n\nFigure 4.16: The typical “random walk” illustration of a GSEA. (Subramanian et al. 2005)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.17: The typical “random walk” illustration of a GSEA. (Subramanian et al. 2005)\n\n\n\n\nGSEA outputs enriched gene sets ranked by their statistical significance and direction of change (positive or negative ES). These results highlight biological pathways or processes that are upregulated or downregulated between conditions. For example: (1) In cancer studies, GSEA might identify pathways like “cell cycle” or “apoptosis” as significantly enriched. (2) In immune response research, pathways such as “inflammatory response” or “antigen processing” may be highlighted.\nSome advantages of GSEA:\n\nSensitivity to Subtle Changes: GSEA detects coordinated changes in groups of genes, even when individual genes show modest differential expression.\nBiological Context: By focusing on gene sets, GSEA provides results that are easier to interpret in the context of known biological processes or pathways.\nRobustness: GSEA is less affected by noise or variability in individual genes, as it aggregates signals across entire gene sets.\n\nDespite its strengths, GSEA has limitations:\n\nGene Set Overlap: Many gene sets share overlapping genes, complicating interpretation.\nRanking Dependence: Results depend heavily on the ranking metric used.\nPredefined Gene Sets: GSEA relies on the quality and relevance of predefined gene sets, which may not fully capture novel or context-specific biology.\n\n\n\n4.5.2 A brief note on other approaches\nExtensions to GSEA address some of these issues, including pathway-specific weighting, time-series GSEA, and single-cell adaptations of the method. One example to illustrate this is iDEA (Ma et al. 2020). These methods aren’t hugely popular since this procedure is quite literally often one of the last stages of a single-cell analyses – using a fancy method here usually requires you to have faith that every analyses beforehand was performed reliably. (For instance, if you don’t think you performed batch correction properly, it doesn’t matter if you use the fanciest variant of GSEA – you’ll probably still find nothing biologically relevant.) See (Mubeen et al. 2022) for an overview of many methods.\n\n\n4.5.3 How to do this in practice\nThe most popular package to perform a GSEA analysis is clusterProfiler::gseGO. See https://yulab-smu.top/biomedical-knowledge-mining-book/clusterprofiler-go.html for tutorials on how to do this. If you’re working on human data, the database used for GSEA is often the org.Hs.eg.db package, see https://bioconductor.org/packages/release/data/annotation/html/org.Hs.eg.db.html. See the following code snippet and the outputs in Figure 4.18 and Figure 4.20 as an example.\n\nteststat_vec &lt;- cluster2.markers$avg_log2FC names(teststat_vec) &lt;- rownames(cluster2.markers) teststat_vec &lt;- sort(teststat_vec, decreasing = TRUE) set.seed(10) gse &lt;- clusterProfiler::gseGO( teststat_vec, ont = “BP”, # what kind of pathways are you interested in keyType = “SYMBOL”, OrgDb = “org.Hs.eg.db”, pvalueCutoff = 1, # p-value threshold for pathways minGSSize = 10, # minimum gene set size maxGSSize = 500 # maximum gene set size ) head(as.data.frame(gse))\n\n\n\n\n\n\n\n\nFigure 4.18: Output of head(as.data.frame(gse))\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.19: Output of head(as.data.frame(gse))\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.20: Output of clusterProfiler::dotplot(gse, showCategory=30)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#sec-celltype_labeling",
    "href": "chapter3_rna.html#sec-celltype_labeling",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.6 Cell-type labeling, transfer learning",
    "text": "4.6 Cell-type labeling, transfer learning\nCell-type labeling is a critical step in single-cell RNA-sequencing analysis, where the goal is to assign meaningful biological identities to cell clusters or groups. Traditionally, this process relies on manual annotation using known marker genes, which can be time-consuming and subjective. Transfer learning has emerged as a powerful alternative, leveraging reference datasets with well-annotated cell types to predict labels for new datasets. By applying machine learning models trained on these references, transfer learning enables automated, scalable, and more consistent cell-type annotation. This approach is particularly valuable for studies with large datasets or when analyzing data from underexplored tissues or conditions. Despite its utility, challenges remain, such as aligning datasets across technical platforms or accounting for differences in cell-state diversity, but the method continues to refine and enhance the accuracy and efficiency of cell-type labeling.\n\nInput/Output. The input to cell-type labeling is two matrices, a reference dataset \\(X^{(\\text{ref})} \\in \\mathbb{R}^{n_1\\times p}\\) for \\(n_1\\) cells and \\(p\\) features (i.e., genes) where all \\(n_1\\) cells are labeled (and hence, why it’s called a “reference” dataset), and another dataset which is unlabeled and is the interest of your study, \\(X^{(\\text{target})} \\in \\mathbb{R}^{n_2\\times p}\\) for \\(n_2\\) cells and the same \\(p\\) features. If there are \\(K\\) clusters in \\(X^{(\\text{ref})}\\), the goal is to label each of the \\(n_2\\) cells in \\(X^{(\\text{target})}\\) with one (or more, or possible none) of the \\(K\\) clusters.\n\n\n4.6.1 Strategy 1: Good’ol marker genes\nCell-type identification using marker genes is a typical choice in single-cell RNA-seq analysis. This is typically performed after clustering cells into groups with similar gene expression profiles. This process involves leveraging existing literature to associate marker genes identified for each cluster with known cell types. While this approach requires significant manual effort and domain expertise – making it a human-in-the-loop process – this can be both its strength and its limitation. The manual aspect ensures transparency and allows researchers to contextualize results within the biology of the experiment, drawing on their understanding of the literature and experimental design.\nSome drawbacks of this approach: 1) It can be labor-intensive, dependent on the quality of prior knowledge, and requires the data analyst to have quite some expertise with the biological system. After all, you will need to figure out the most reliable set of genes to use – this will inevitably require you know have read quite a few papers to see which markers are commonly used by the community. 2) A more limiting drawback is that you will not be able to label “new” cell types in your dataset that do not have already established marker genes. After all, if you are “missing” the marker genes for a particular cell type, you might not even realize that that cell type is in your dataset.\nDespite these challenges, this method remains the most widely used because it minimizes the risk of major errors going unnoticed, making it a reliable approach for annotating cell types.\n\n\n\n\n\n\n\nFigure 4.21: The cell-type labeling step. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.22: The markers for various cell types in this system.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.23: Using the DimPlot(pbmc, reduction = \"umap\", label = TRUE, pt.size = 0.5) + Seurat::NoLegend() function to visualize the data with the cell-type labels. (By default, the “labels” come from Seurat::Idents, which is set to the latest clustering by default. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n4.6.2 In relation to scoring a gene-set\nRecall in Section 3.3.3 that we often create a score for the percentage of expression originating from the mitochondria genes for cell-filtering. The Seurat::PercentageFeatureSet function to do this does exactly what it sounds like – it computes the percentage of counts originating from the genes in the gene set (relative to all the counts in the cell).\nThere are fancier methods to compute this enrichment score (i.e., a score where a higher number denotes that that gene set is “more active” in a cell). Two popular methods for this account for the “background distribution” (UCell, (Andreatta and Carmona 2021)) or the gene regulatory network (AUCell, (Aibar et al. 2017)).\n\n\n4.6.3 Strategy 2: Deploying a classifier/Transfer learning\nIn the earlier days of single-cell analyses, there were quite a few methods that had you build a cell-type classifier on a reference dataset which had cell-type labels (typically developed by a large consortia, where you trust the cell-type labels) and you deploy the classifier on your dataset of interest. The classifier here predicts the cell type based on the gene expression. For instance, you can see (Abdelaal et al. 2019) which has relevant references. While these are probably the most statistically transparent, it usually takes a bit of time to learn the details of these methods to understand how statistically sound they are.\nThere are a few aspects of statistical interest in this direction:\n\nClassification with abstain/rejection: It is possible to design a classifier with a “reject” or “abstain” option. This way, if the classifier is unsure about the cell type label, then it can refuse to give a cell-type prediction. For instance, see scPred (Alquicira-Hernandez et al. 2019) and Hierarchical Reject (Theunissen et al. 2024) for one way this can be done.\nConformal inference: Conformal inference is a large statistical theoretical field. Essentially, instead of predicting one cell type, the method returns a set of potential cell types. (This is a different way to design methods with an abstain option, since a cell with an empty prediction set would essentially “abstain.”) See (Khatri and Bonn 2022) for an example.\nTransfer learning: This is the more commonly option done nowadays in this category. The term “transfer learning” refers to training a classifier on the reference dataset and then slightly modifying it (i.e., “transfer”) in order to your dataset of interest. Some examples of this is scArches (Lotfollahi et al. 2022) and its extension in treeArches (Michielsen et al. 2023).\n\nSome drawbacks: The main drawback is due to batch effects. After all, “by definition,” the dataset you’re analyzing is very likely coming from a different research lab/consortium that produced the reference dataset. While statistical theory offers great ways to diagnose a classifer assuming the training and test data originate from the same source, it is much harder to take advantage of this statistical theory if you blatantly know this is not the case.\n\n\n4.6.4 Strategy 3: Data integration\nDataset integration has become a widely used approach for cell-type labeling, especially when marker genes are not readily available. This strategy involves integrating a new dataset with an existing reference, often using methods that align the datasets in a shared space. Once integrated, cells in the new dataset are labeled based on their nearest neighbors in the reference. While this approach is powerful, it has limitations. A key advantage of this method is its visual interpretability; researchers can assess the alignment of the datasets in a low-dimensional embedding, adding a layer of protection against mislabeling or other errors. See Figure 4.24 for a schematic.\n\n\n\n\n\n\n\nFigure 4.24: Illustration of how to conceptualize data integration. (Stuart et al. 2019)\n\n\n\n\nA few notes:\n\nWe sometimes use the verb “align” two datasets or “project” one dataset onto another. Note that we’re just using these verbs conceptually, not in a literal statistical sense.\nMany methods used for batch correction (see Section 3.3.19) can be repurposed for this task.\nBe warned though: integration can sometimes “collapse” distinct points in the data, potentially masking subtle differences or even preventing the discovery of new cell types.\n\nFor some canonical examples of these methods: Using CCA ((Butler et al. 2018) and its followup, (Stuart et al. 2019)), using cell-cell graphs (MNN (Haghverdi et al. 2018) and Harmony (Korsunsky et al. 2019)), and regression (COMBAT-seq (Yuqing Zhang, Parmigiani, and Johnson 2020)). However, by far the most common way to do this currently is through deep-learning (which can be interpreted as a form of transfer learning, see scANVI (Xu et al. 2021) for example).\nJust to give you small taste of some of these above methods:\n\nAnchor pairs via CCA: (Stuart et al. 2019) essentially is a 3 step process: 1) first find many pairs of cells (one in your dataset, one in the reference) that are similar based on CCA, and then 2) use those pairs of cells to learn a transformation on how to convert the cells in your dataset into the cells in the reference dataset. Then, apply this transformation to all cells in your dataset.\nClustering and shrinking via Harmony: (Korsunsky et al. 2019) essentially is a 2 step process: 1) first cluster all the cells in both datasets together (yours with the reference dataset), where you want a clustering where each cluster represents a good mixture of cells from each dataset, then 2) “shrink” all the cells in each cluster towards its cluster center. Essentially, you treat the differences between cluster as “biological signal,” while the variability within a cluster as “technical noise.”\nDeep-learning (i.e., a really fancy regression), via scANVI: (Xu et al. 2021) essentially use one framework to: 1) learn a way to project all the cells in both datasets into one low-dimensional space where the low-dimensional space is “independent” of the dataset label (i.e., given a cell’s low-dimensional space, you can’t predict which dataset the cell came from), and then 2) combine a cell’s low-dimensional space with its dataset label to reconstruct the original gene expression (which is to ensure that the low-dimensional space was a meaningful representation).\n\n\n\n4.6.5 How to do this in practice\nThere’s not standardize way to do this, but I would personally recommend data integration using Harmony, where you integrate your (unlabeled) dataset of interest with a reference dataset with cell-type labels. See Section 3.3.19.\n\n\n4.6.6 A brief note on other approaches\nSee (Abdelaal et al. 2019; Mölbert and Haghverdi 2023) for benchmarking of cell-type labeling methods.\nNote that cell-type labeling is popularly being integrating into web-based all-in-one (i.e., “one-stop-shop”) platforms that automatically label your cells (see Azimuth (https://azimuth.hubmapconsortium.org/) and CellTypist (https://www.celltypist.org/) as an example). This is because cell-type labeling through writing your own code can be quite fickle that requires a lot of biological expertise to know what is “sensible”, so it’s beneficial to offload this concern to a platform. See (Tran et al. 2020; Luecken et al. 2022; Lance et al. 2022) (among many) that benchmark the dataset integration methods.\n\nRemark (The big challenges in cell-type integration). Broadly speaking, there are three major challenges in cell-type integration:\n\nHow do you pick the appropriate reference dataset?: (Mölbert and Haghverdi 2023) discusses the impact of the reference dataset itself. (Of course, using a different reference dataset will change your cell type labeling, so it’s important to plan beforehand how you will defend the critique of whether you even have an appropriate reference dataset.)\nHow “heavy-handed” do you want your integration to be?: If you have longitudinal samples (i.e., tissue from the same donors at different points, say, based on followup visits after therapy), these samples might be on different sequencing batches. This presents a unique conundrum – how can you differentiate between a “batch effect” and a “true biological effect due to different timepoints”? You might be interested to see papers such as CellANOVA for this (Z. Zhang et al. 2024). Even if you don’t have longitudinal samples, sometimes certain integration methods to overboard and merge too much together. For example, one of the reasons some researchers prefer reciprocal PCA to do integration (see https://satijalab.org/seurat/articles/integration_rpca.html) is precisely because it’s more “conservative” (i.e., is less aggressive in trying to remove batch effects). How will you, as the data analyst, determine how much is too much?\nHow do you know if your dataset contains cell types not in the reference?: If your dataset of interest has many cell types that aren’t in the reference, your data integration method might unknowingly wipe all this signal away since it is doing its job – it’s trying to integrate the two datasets together, under the pretense that both datasets are “similar.” This is where different methods start to exhibit different behavior. You might be interested in looking at methods such as popV (Ergen et al. 2024) that try to build a “uncertainty score.” (Many modern dataset integration methods now have this feature. However, you should always take these scores with a grain of salt since you can’t be too sure how calibrated these scores are.) CASi (Y. Wang et al. 2024) offers a different perspective on how to answer “in a longitudinal experiment, how do I even know if a new celltype emerges?”\n\nMy personal opinion? The best batch correction strategy is to do careful experimental design. One common strategy is to have a “control” tissue that you put into every sequencing run. That way, even across your different sequencing batches, you will have one set of cells that should be the same across every batch. This helps you get some additional clues on when a batch correction method is performing well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#sec-trajectory",
    "href": "chapter3_rna.html#sec-trajectory",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.7 Trajectory analysis",
    "text": "4.7 Trajectory analysis\nTrajectory analysis aims to reconstruct the dynamic processes underlying cellular transitions, such as differentiation or response to stimuli, by organizing cells along a pseudotime axis. This is particularly valuable in single-cell RNA-sequencing studies, where cells are often sampled as a snapshot, rather than longitudinally, making it challenging to infer temporal relationships. Methods for trajectory analysis typically rely on one of two approaches: geometric relationships or biophysical models. Geometric methods use structures like cell-cell nearest-neighbor graphs to infer trajectories and identify branching points, capturing transitions and bifurcations in cell states. Alternatively, biophysical models leverage biological priors, such as splicing kinetics, gene-regulatory network dynamics, or metabolic states, to assign cells to pseudotime. Despite the complexities of inferring trajectories from static data, these approaches have proven effective in uncovering developmental pathways, understanding disease progression, and identifying rare transitional cell states. Trajectory analysis thus provides a framework to explore cellular dynamics in the absence of direct time-course experiments.\n\nInput/Output. The input to trajectory analysis is typically a matrix \\(X\\in \\mathbb{R}^{n\\times k}\\) for \\(n\\) cells and \\(k\\) latent dimensions. Depending on which trajectory method you’re using, you might need more inputs (such as cell clusters, specific cells that denote the start or end of the trajectory, etc.). The output is typically two things: 1) the pseudotime for each cell (i.e., a number between \\(0\\) and \\(1\\) for each cell), and 2) the trajectories (i.e., the order of which cells or cell clusters develop/reprogram into which other cells or cell clusters).\n\n\n4.7.1 A typical choice: Monocle\nMonocle (Trapnell et al. 2014) is a computational framework designed to infer the trajectory of single-cell gene expression profiles as they progress through a dynamic biological process, such as differentiation or reprogramming. It constructs a “pseudotime” ordering, which assigns a continuous value to each cell based on its transcriptional state, representing its progression along the biological process. This method is illustrated as a schematic in Figure 4.25.\nThe method begins by representing the gene expression profile of each cell as a point in a high-dimensional space \\(\\mathbb{R}^d\\), where \\(d\\) is the number of genes measured. To reduce noise and computational complexity, Monocle uses dimensionality reduction techniques, such as Independent Component Analysis (ICA), to project the data into a lower-dimensional space.\nNext, Monocle constructs a Minimum Spanning Tree (MST) in this reduced space. Each cell is treated as a node, and edges represent transcriptional similarity between cells, with weights corresponding to distances in the reduced space. The MST serves as the backbone for the trajectory and is used to identify the longest connected path, representing the main pseudotemporal progression.\nMathematically, pseudotime \\(\\tau_i\\) for a cell \\(i\\) is defined along this trajectory as: \\[\n\\tau_i = \\sum_{j=1}^{i} d(c_j, c_{j+1}),\n\\] where \\(d(c_j, c_{j+1})\\) is the geodesic distance between successive cells \\(c_j\\) and \\(c_{j+1}\\) along the path.\n\n\n\n\n\n\n\nFigure 4.25: From (Trapnell et al. 2014).\n\n\n\n\n\n\n4.7.2 How to do this in practice\nThere is no standardized way for trajectory analysis. However, Monocle13 is quite popular because: 1) it was one of the first ways developed to do trajectory analyses, 2) it is quite modular (so different researchers can switch out specific aspects for another), 3) each step is quite transparent mathematically, and 4) the software is well maintained and runs reasonably quickly.\n\n\n4.7.3 A highly active area of research: How much biology does your premise assume is true?\nMethods such as Monocle intrinsically assume that two cells in gene expression is a good proxy of both cells at a similar biological stage. In general, this is a huge assumption, and it has been shown to not hold in various settings. For instance, see (Saunders et al. 2023), or multi-omics in Chapter 5 and lineage-tracing in Chapter 8. (It’s possible that the transcriptome is not the most accurate omic to track cell development/response.)\nHowever, other methods based on the transcriptome could rely on different assumptions on how the biology relates to the math. Just to give some examples, Slingshot (Street et al. 2018) assumes the biological premise of Monocle and adds that the trajectory must be strictly branching. CytoTRACE (Gulati et al. 2020) instead assumes that as cells develop, there are less genes that are expressed. RNA velocity methods such as velocyto (La Manno et al. 2018) assume there is spliced/unspliced relation in mRNA fragments that can be used to learn a cell’s developmental stage, which offers a potentially more mechanistic perspective on how to learn the cell trajectories.\nA brief note on other approaches: See (Saelens et al. 2019; Cannoodt et al. 2021; Yifan Zhang et al. 2023) for large benchmarkings of various RNA velocity methods. See (Ancheta et al. 2024) for a benchmarking specific to RNA velocity.\n\n\n4.7.4 Talk to me about statistics: What makes this topic criminally difficult?\nTrajectory inference is a challenging statistical problem because it combines elements of both unsupervised and supervised learning in a highly complex setting. On one hand, it is unsupervised because you do not know the true pseudotime or branching structure of the data – these must be inferred from the observed high-dimensional gene expression profiles. On the other hand, it has a supervised flavor, as the goal is often to reconstruct a biologically meaningful trajectory that aligns with an underlying developmental or dynamic process. This dual nature makes the problem particularly tricky.\nSeveral assumptions exacerbate the difficulty (see (Saelens et al. 2019)):\n\nUnknown time and branching structure: The exact ordering of cells and the number or nature of branches in the trajectory are unknown. Inferring these from noisy and sparse single-cell data is highly nontrivial.\nAssumption of a trajectory: You are implicitly assuming that the data follows a trajectory, but not all biological processes have a well-defined or continuous progression. If the underlying process is more stochastic or does not follow a clear path, the trajectory inference may be misleading.\nEqual representation across pseudotime: Many methods assume an approximately equal number of cells sampled across all stages of the trajectory, but real datasets often have uneven representation, which can bias the inferred pseudotime or branching structure.\n\n\nRemark (Personal opinion: Lack of flushed-out statistical theory). A typical statistical theorem goes: 1) I have a specific data model in mind that tells me how data is generated, 2) I have an specific aspect about this model that I do not get to observe, but I wish to estimate from data, and then 3) based on a novel estimator, I can prove that as I get more data, under some reasonable assumptions, I can estimate the unknown aspects better and better.\nIn general, I have not seen this convincingly been done at all for almost any trajectory analysis for single-cell methods. This is due to the complexity of the biological assumptions you have justify, all the modeling assumptions you have justify (shown above), and then all the crazy proof techniques you would need to carefully manage to show how you can cast a natively unsupervised statistical problem into a supervised one.\nThis isn’t to say trajectory methods are not trustworthy. Certainly, these methods have uncovered tremendous biological insights. My comment here is that a lot of our understanding of trajectory methods is driven by the biological context, not so much by our mathematical insight.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#sec-beyond_scrna",
    "href": "chapter3_rna.html#sec-beyond_scrna",
    "title": "4  Single-cell RNA-sequencing",
    "section": "4.8 Concluding thoughts about scRNA-seq: Why bother with anything else?",
    "text": "4.8 Concluding thoughts about scRNA-seq: Why bother with anything else?\n\n4.8.1 Limited biological insights from scRNA-seq matrices\nRecall Remark 4.1, which highlights how much biological information is discarded during typical scRNA-seq workflows, particularly by tools like CellRanger. While the raw reads may contain rich and valuable insights – such as information on splicing, untranslated regions (UTRs), and alternative polyadenylation sites (APAs) – these are often overlooked in downstream analyses. If your analysis starts solely from the count matrix, you might miss significant aspects of cellular biology. For a broader perspective on what scRNA-seq still lacks, both biologically and methodologically, see review papers like (Lahnemann et al. 2020). Additionally, for those interested in computational challenges in single-cell analysis, resources like https://openproblems.bio/events/ provide an excellent starting point.\n\n\n4.8.2 Limited biological understanding of mRNA fragments\nIt’s important to remember that scRNA-seq primarily focuses on associating gene expression with biological phenomena. While we have a detailed map of gene expression, there is often limited context or understanding of how these genes interact functionally. mRNA fragments don’t act in isolation; their roles are inherently connected to proteins, cellular structures, and interactions with other cells. Moreover, scRNA-seq lacks direct insights into the functional impact of gene expression changes (as opposed to massively parallelized reporter assays), ignores temporal dynamics (as time information is lost when cells are lysed, as opposed to live-cell imaging), and omits structural context like cellular shape and protein localization (as opposed to other imaging technologies).\n\n\n4.8.3 Translational limitation of scRNA-seq, and bulk deconvolution\nAs we showed in the footnotes of Section 4.2, scRNA-seq is extremely expensive. This makes any scRNA-seq technology quite unlikely to become a routine tool in clinical settings like hospitals. This is one of the main appeals of bulk deconvolution methods (where you can estimate cell-type proportions for many tissues for many donors, see (Cobos et al. 2023; H. Nguyen et al. 2024; Maden et al. 2023)), since bulk RNA sequencing is a lot cheaper than scRNA sequencing.\n\n\n4.8.4 Not all RNA is mRNA\nIf you have not studied biology before, it might shock you to realize that mRNA is only one specific type of RNA molecules. See Figure 4.26 for all the different types of mRNA molecules. mRNA is simply the only RNA molecules to become translated into proteins. In fact, this is only from 1%-5% of all RNA molecules in a cell14. There is incredible biology being done to investigate the other roles of the other RNA molecules, for instance lncRNA (Statello et al. 2021) and microRNA (Shang et al. 2023).\n\n\n\n\n\n\n\nFigure 4.26: From https://www.youtube.com/watch?v=FThA4Vxs3v4\n\n\n\n\n\n\n4.8.5 Importance of other omics\nTo truly understand the complexity of cellular systems, we need to integrate information from multiple modalities. This idea can be illustrated by the “blind men and the elephant” analogy, Figure 4.27: each single-cell modality provides only a partial perspective of the whole, and no single technology can offer a complete picture. For instance, scRNA-seq captures transcriptional snapshots, while other methods focus on proteins, chromatin, or spatial organization. Combining these modalities is essential to gain a holistic view of cellular function and dynamics.\n\n\n\n\n\n\n\nFigure 4.27: The “blind men and the elephant” analogy. Each omic gives you one perspective of the cell, and it’s important to put all these omics together to get the full picture.\n\n\n\n\n\nRemark (Personal opinion: The abundance of computational tools for scRNA-seq doesn’t necessarily reflect its importance as an omic, but rather its standardization and well-organized data structure). The explosion of computational tools for scRNA-seq, particularly from 3’ short-read technologies, is less about it being the most biologically critical modality and more about the ease of working with the data. The technology is highly standardized, producing consistent and structured outputs, such as count matrices, that are straightforward to analyze. This standardization has enabled rapid tool development and widespread adoption, creating a feedback loop where more tools emerge as the data becomes more accessible. See (Eisenstein 2020) for an opinion piece.\n\n\nRemark (The quadruple threat in cell biology). Suppose you collect data, analyze it, and realize your findings don’t align with current biological knowledge. Depending on your expertise, you might blame one of these four major aspects:\n\nYour computational/statistical methods aren’t good: This is often the perspective of a computational/statistical person, where you blame the way you analyzed the data. This might imply there is room for better analytical tools.\nYou didn’t collect data using the appropriate technology: This is often the perspective of a bio-engineering person, where you blame the technology used or the omic being assayed. This might imply there’s a better technology that can be developed to measure something much more directly relevant to your biological question.\nYou didn’t collect enough data from a diverse set of tissues/donors: This is often the perspective of a large consortium, where you blame the limited number of samples being used to draw this conclusion. Perhaps by pooling together manpower, funding, and expertise, the collective group of researchers can have enough power to learn the desired biology.\nThe biology is much more complicated than you realize: This is often the perspective of a biologist – how are you so sure your finding isn’t a fluke, but instead, it’s biological novelty? Is there a biological premise on why your findings might be reflective of reality?\n\nThis is showcase: 1) the thrill and unending expanse of cell biology, and 2) cell biology is now so complicated that it is unrealistic to learn it all. We (as researchers) need to specialize in a particular area, gain a preliminary understanding of all the other aspects of cell biology, and find great collaborators who complement your own expertise. For instance, developing better computational methods is not very impactful if the community knows that the scientific limitation isn’t the computational methods.\n\n\n\n\n\nAbdelaal, Tamim, Lieke Michielsen, Davy Cats, Dylan Hoogduin, Hailiang Mei, Marcel JT Reinders, and Ahmed Mahfouz. 2019. “A Comparison of Automatic Cell Identification Methods for Single-Cell RNA Sequencing Data.” Genome Biology 20 (1): 1–19.\n\n\nAibar, Sara, Carmen Bravo González-Blas, Thomas Moerman, Vân Anh Huynh-Thu, Hana Imrichova, Gert Hulselmans, Florian Rambow, et al. 2017. “SCENIC: Single-Cell Regulatory Network Inference and Clustering.” Nature Methods 14 (11): 1083–86.\n\n\nAlquicira-Hernandez, Jose, Anuja Sathe, Hanlee P Ji, Quan Nguyen, and Joseph E Powell. 2019. “scPred: Accurate Supervised Method for Cell-Type Classification from Single-Cell RNA-Seq Data.” Genome Biology 20: 1–17.\n\n\nAncheta, Sarah, Leah Dorman, Guillaume Le Treut, Abel Gurung, Loic Alain Royer, Alejandro Granados, and Merlin Lange. 2024. “Challenges and Progress in RNA Velocity: Comparative Analysis Across Multiple Biological Contexts.” bioRxiv, 2024–06.\n\n\nAndreatta, Massimo, and Santiago J Carmona. 2021. “UCell: Robust and Scalable Single-Cell Gene Signature Scoring.” Computational and Structural Biotechnology Journal 19: 3796–98.\n\n\nArabi, Tarek Ziad, Aliyah Abdulmohsen Alabdulqader, Belal Nedal Sabbah, and Abderrahman Ouban. 2023. “Brain-Inhabiting Bacteria and Neurodegenerative Diseases: The ‘Brain Microbiome’ Theory.” Frontiers in Aging Neuroscience 15: 1240945.\n\n\nArora, Ankita, Raeann Goering, Hei Yong G Lo, Joelle Lo, Charlie Moffatt, and J Matthew Taliaferro. 2022. “The Role of Alternative Polyadenylation in the Regulation of Subcellular RNA Localization.” Frontiers in Genetics 12: 818668.\n\n\nBaran, Yael, Akhiad Bercovich, Arnau Sebe-Pedros, Yaniv Lubling, Amir Giladi, Elad Chomsky, Zohar Meir, Michael Hoichman, Aviezer Lifshitz, and Amos Tanay. 2019. “MetaCell: Analysis of Single-Cell RNA-Seq Data Using k-Nn Graph Partitions.” Genome Biology 20 (1): 1–19.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The Control of the False Discovery Rate in Multiple Testing Under Dependency.” Annals of Statistics, 1165–88.\n\n\nBilous, Mariia, Léonard Hérault, Aurélie AG Gabriel, Matei Teleman, and David Gfeller. 2024. “Building and Analyzing Metacells in Single-Cell Genomics Data.” Molecular Systems Biology, 1–23.\n\n\nBoyeau, Pierre, Jeffrey Regier, Adam Gayoso, Michael I Jordan, Romain Lopez, and Nir Yosef. 2023. “An Empirical Bayes Method for Differential Expression Analysis of Single Cells with Deep Generative Models.” Proceedings of the National Academy of Sciences 120 (21): e2209124120.\n\n\nButler, Andrew, Paul Hoffman, Peter Smibert, Efthymia Papalexi, and Rahul Satija. 2018. “Integrating Single-Cell Transcriptomic Data Across Different Conditions, Technologies, and Species.” Nature Biotechnology 36 (5): 411–20.\n\n\nCannoodt, Robrecht, Wouter Saelens, Louise Deconinck, and Yvan Saeys. 2021. “Spearheading Future Omics Analyses Using Dyngen, a Multi-Modal Simulator of Single Cells.” Nature Communications 12 (1): 3942.\n\n\nCao, Yingying, Simo Kitanovski, Ralf Küppers, and Daniel Hoffmann. 2021. “UMI or Not UMI, That Is the Question for scRNA-Seq Zero-Inflation.” Nature Biotechnology, 1–2.\n\n\nChen, J, and WA Weiss. 2015. “Alternative Splicing in Cancer: Implications for Biology and Therapy.” Oncogene 34 (1): 1–14.\n\n\nChen, Wenan, Yan Li, John Easton, David Finkelstein, Gang Wu, and Xiang Chen. 2018. “UMI-Count Modeling and Differential Expression Analysis for Single-Cell RNA Sequencing.” Genome Biology 19 (1): 70.\n\n\nCobos, Francisco Avila, Mohammad Javad Najaf Panah, Jessica Epps, Xiaochen Long, Tsz-Kwong Man, Hua-Sheng Chiu, Elad Chomsky, et al. 2023. “Effective Methods for Bulk RNA-Seq Deconvolution Using scnRNA-Seq Transcriptomes.” Genome Biology 24 (1): 177.\n\n\nDal Molin, Alessandra, Giacomo Baruzzo, and Barbara Di Camillo. 2017. “Single-Cell RNA-Sequencing: Assessment of Differential Expression Analysis Methods.” Frontiers in Genetics 8: 62.\n\n\nDas, Samarendra, Anil Rai, and Shesh N Rai. 2022. “Differential Expression Analysis of Single-Cell Rna-Seq Data: Current Statistical Approaches and Outstanding Challenges.” Entropy 24 (7): 995.\n\n\nDenisenko, Elena, Belinda B Guo, Matthew Jones, Rui Hou, Leanne De Kock, Timo Lassmann, Daniel Poppe, et al. 2020. “Systematic Assessment of Tissue Dissociation and Storage Biases in Single-Cell and Single-Nucleus RNA-Seq Workflows.” Genome Biology 21 (1): 1–25.\n\n\nDing, Jiarui, Xian Adiconis, Sean K Simmons, Monika S Kowalczyk, Cynthia C Hession, Nemanja D Marjanovic, Travis K Hughes, et al. 2020. “Systematic Comparison of Single-Cell and Single-Nucleus RNA-Sequencing Methods.” Nature Biotechnology 38 (6): 737–46.\n\n\nEisenstein, Michael. 2020. “The Secret Life of Cells.” Nature Methods 17: 7–10.\n\n\nErgen, Can, Galen Xing, Chenling Xu, Martin Kim, Michael Jayasuriya, Erin McGeever, Angela Oliveira Pisco, Aaron Streets, and Nir Yosef. 2024. “Consensus Prediction of Cell Type Labels in Single-Cell Data with popV.” Nature Genetics, 1–8.\n\n\nGrishkevich, Vladislav, and Itai Yanai. 2014. “Gene Length and Expression Level Shape Genomic Novelties.” Genome Research 24 (9): 1497–1503.\n\n\nGulati, Gunsagar S, Shaheen S Sikandar, Daniel J Wesche, Anoop Manjunath, Anjan Bharadwaj, Mark J Berger, Francisco Ilagan, et al. 2020. “Single-Cell Transcriptional Diversity Is a Hallmark of Developmental Potential.” Science 367 (6476): 405–11.\n\n\nHaghverdi, Laleh, Aaron TL Lun, Michael D Morgan, and John C Marioni. 2018. “Batch Effects in Single-Cell RNA-Sequencing Data Are Corrected by Matching Mutual Nearest Neighbors.” Nature Biotechnology 36 (5): 421–27.\n\n\nHe, Liang, Jose Davila-Velderrain, Tomokazu S Sumida, David A Hafler, Manolis Kellis, and Alexander M Kulminski. 2021. “NEBULA Is a Fast Negative Binomial Mixed Model for Differential or Co-Expression Analysis of Large-Scale Multi-Subject Single-Cell Data.” Communications Biology 4 (1): 629.\n\n\nJaakkola, Maria K, Fatemeh Seyednasrollah, Arfa Mehmood, and Laura L Elo. 2017. “Comparison of Methods to Detect Differentially Expressed Genes Between Single-Cell Populations.” Briefings in Bioinformatics 18 (5): 735–43.\n\n\nJiang, Ruochen, Tianyi Sun, Dongyuan Song, and Jingyi Jessica Li. 2022. “Statistics or Biology: The Zero-Inflation Controversy about scRNA-Seq Data.” Genome Biology 23 (1): 31.\n\n\nJunttila, Sini, Joahnnes Smolander, and Laura L Elo. 2022. “Benchmarking Methods for Detecting Differential States Between Conditions from Multi-Subject Single-Cell RNA-Seq Data.” Briefings in Bioinformatics 23 (5): bbac286.\n\n\nKhatri, Robin, and Stefan Bonn. 2022. “Uncertainty Estimation for Single-Cell Label Transfer.” In Conformal and Probabilistic Prediction with Applications, 109–28. PMLR.\n\n\nKim, Tae Hyun, Xiang Zhou, and Mengjie Chen. 2020. “Demystifying ‘Drop-Outs’ in Single-Cell UMI Data.” Genome Biology 21 (1): 196.\n\n\nKorsunsky, Ilya, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang, Kevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya Raychaudhuri. 2019. “Fast, Sensitive and Accurate Integration of Single-Cell Data with Harmony.” Nature Methods, 1–8.\n\n\nLa Manno, Gioele, Ruslan Soldatov, Amit Zeisel, Emelie Braun, Hannah Hochgerner, Viktor Petukhov, Katja Lidschreiber, et al. 2018. “RNA Velocity of Single Cells.” Nature 560 (7719): 494–98.\n\n\nLahnemann, David, Johannes Koster, Ewa Szczurek, Davis J McCarthy, Stephanie C Hicks, Mark D Robinson, Catalina A Vallejos, et al. 2020. “Eleven Grand Challenges in Single-Cell Data Science.” Genome Biology 21 (1): 1–35.\n\n\nLance, Christopher, Malte D Luecken, Daniel B Burkhardt, Robrecht Cannoodt, Pia Rautenstrauch, Anna Laddach, Aidyn Ubingazhibov, et al. 2022. “Multimodal Single Cell Data Integration Challenge: Results and Lessons Learned.” BioRxiv, 2022–04.\n\n\nLeung, Szi Kay, Aaron R Jeffries, Isabel Castanho, Ben T Jordan, Karen Moore, Jonathan P Davies, Emma L Dempster, et al. 2021. “Full-Length Transcript Sequencing of Human and Mouse Cerebral Cortex Identifies Widespread Isoform Diversity and Alternative Splicing.” Cell Reports 37 (7).\n\n\nLi, Yumei, Xinzhou Ge, Fanglue Peng, Wei Li, and Jingyi Jessica Li. 2022. “Exaggerated False Positives by Popular Differential Expression Methods When Analyzing Human Population Samples.” Genome Biology 23 (1): 1–13.\n\n\nLin, Kevin Z, Yixuan Qiu, and Kathryn Roeder. 2024. “eSVD-DE: Cohort-Wide Differential Expression in Single-Cell RNA-Seq Data Using Exponential-Family Embeddings.” BMC Bioinformatics 25 (1): 113.\n\n\nLiu, Jiayi, Anat Kreimer, and Wei Vivian Li. 2023. “Differential Variability Analysis of Single-Cell Gene Expression Data.” Briefings in Bioinformatics 24 (5): bbad294.\n\n\nLiu, Yunqing, Jiayi Zhao, Taylor S Adams, Ningya Wang, Jonas C Schupp, Weimiao Wu, John E McDonough, et al. 2023. “iDESC: Identifying Differential Expression in Single-Cell RNA Sequencing Data with Multiple Subjects.” BMC Bioinformatics 24 (1): 318.\n\n\nLopes, Inês, Gulam Altab, Priyanka Raina, and JoãO Pedro De Magalhães. 2021. “Gene Size Matters: An Analysis of Gene Length in the Human Genome.” Frontiers in Genetics 12: 559998.\n\n\nLotfollahi, Mohammad, Mohsen Naghipourfar, Malte D Luecken, Matin Khajavi, Maren Büttner, Marco Wagenstetter, Žiga Avsec, et al. 2022. “Mapping Single-Cell Data to Reference Atlases by Transfer Learning.” Nature Biotechnology 40 (1): 121–30.\n\n\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.” Genome Biology 15 (12): 550.\n\n\nLuecken, Malte D, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese, Marta Interlandi, Michaela F Müller, Daniel C Strobl, et al. 2022. “Benchmarking Atlas-Level Data Integration in Single-Cell Genomics.” Nature Methods 19 (1): 41–50.\n\n\nMa, Ying, Shiquan Sun, Xuequn Shang, Evan T Keller, Mengjie Chen, and Xiang Zhou. 2020. “Integrative Differential Expression and Gene Set Enrichment Analysis Using Summary Statistics for scRNA-Seq Studies.” Nature Communications 11 (1): 1–13.\n\n\nMaden, Sean K, Sang Ho Kwon, Louise A Huuki-Myers, Leonardo Collado-Torres, Stephanie C Hicks, and Kristen R Maynard. 2023. “Challenges and Opportunities to Computationally Deconvolve Heterogeneous Tissue with Varying Cell Sizes Using Single-Cell RNA-Sequencing Datasets.” Genome Biology 24 (1): 288.\n\n\nMattick, John S, Paulo P Amaral, Piero Carninci, Susan Carpenter, Howard Y Chang, Ling-Ling Chen, Runsheng Chen, et al. 2023. “Long Non-Coding RNAs: Definitions, Functions, Challenges and Recommendations.” Nature Reviews Molecular Cell Biology 24 (6): 430–47.\n\n\nMichielsen, Lieke, Mohammad Lotfollahi, Daniel Strobl, Lisa Sikkema, Marcel JT Reinders, Fabian J Theis, and Ahmed Mahfouz. 2023. “Single-Cell Reference Mapping to Construct and Extend Cell-Type Hierarchies.” NAR Genomics and Bioinformatics 5 (3): lqad070.\n\n\nMölbert, Carla, and Laleh Haghverdi. 2023. “Adjustments to the Reference Dataset Design Improve Cell Type Label Transfer.” Frontiers in Bioinformatics 3: 1150099.\n\n\nMubeen, Sarah, Alpha Tom Kodamullil, Martin Hofmann-Apitius, and Daniel Domingo-Fernández. 2022. “On the Influence of Several Factors on Pathway Enrichment Analysis.” Briefings in Bioinformatics 23 (3): bbac143.\n\n\nNeufeld, Anna, Lucy L Gao, Joshua Popp, Alexis Battle, and Daniela Witten. 2022. “Inference After Latent Variable Estimation for Single-Cell RNA Sequencing Data.” arXiv Preprint arXiv:2207.00554.\n\n\nNguyen, Hai CT, Bukyung Baik, Sora Yoon, Taesung Park, and Dougu Nam. 2023. “Benchmarking Integration of Single-Cell Differential Expression.” Nature Communications 14 (1): 1570.\n\n\nNguyen, Hung, Ha Nguyen, Duc Tran, Sorin Draghici, and Tin Nguyen. 2024. “Fourteen Years of Cellular Deconvolution: Methodology, Applications, Technical Evaluation and Outstanding Challenges.” Nucleic Acids Research 52 (9): 4761–83.\n\n\nPeng, Minshi, Brie Wamsley, Andrew G Elkins, Daniel M Geschwind, Yuting Wei, and Kathryn Roeder. 2021. “Cell Type Hierarchy Reconstruction via Reconciliation of Multi-Resolution Cluster Tree.” bioRxiv.\n\n\nPrater, Katherine E, and Kevin Z Lin. 2024. “All the Single Cells: Single-Cell Transcriptomics/Epigenomics Experimental Design and Analysis Considerations for Glial Biologists.” Glia.\n\n\nRobinson, Mark D, and Gordon K Smyth. 2007. “Moderated Statistical Tests for Assessing Differences in Tag Abundance.” Bioinformatics 23 (21): 2881–87.\n\n\nRobles-Espinoza, Carla Daniela, Pejman Mohammadi, Ximena Bonilla, and Maria Gutierrez-Arcelus. 2021. “Allele-Specific Expression: Applications in Cancer and Technical Considerations.” Current Opinion in Genetics & Development 66: 10–19.\n\n\nSaelens, Wouter, Robrecht Cannoodt, Helena Todorov, and Yvan Saeys. 2019. “A Comparison of Single-Cell Trajectory Inference Methods.” Nature Biotechnology 37 (5): 547–54.\n\n\nSato, Yuma, Shumpei Watanabe, Yoshinari Fukuda, Takao Hashiguchi, Yusuke Yanagi, and Shinji Ohno. 2018. “Cell-to-Cell Measles Virus Spread Between Human Neurons Is Dependent on Hemagglutinin and Hyperfusogenic Fusion Protein.” Journal of Virology 92 (6): 10–1128.\n\n\nSaunders, Lauren M, Sanjay R Srivatsan, Madeleine Duran, Michael W Dorrity, Brent Ewing, Tor H Linbo, Jay Shendure, et al. 2023. “Embryo-Scale Reverse Genetics at Single-Cell Resolution.” Nature 623 (7988): 782–91.\n\n\nSchefzik, Roman, Julian Flesch, and Angela Goncalves. 2021. “Fast Identification of Differential Distributions in Single-Cell RNA-Sequencing Data with waddR.” Bioinformatics 37 (19): 3204–11.\n\n\nShang, Renfu, Seungjae Lee, Gayan Senavirathne, and Eric C Lai. 2023. “microRNAs in Action: Biogenesis, Function and Regulation.” Nature Reviews Genetics 24 (12): 816–33.\n\n\nSiletti, Kimberly, Rebecca Hodge, Alejandro Mossi Albiach, Ka Wai Lee, Song-Lin Ding, Lijuan Hu, Peter Lönnerberg, et al. 2023. “Transcriptomic Diversity of Cell Types Across the Adult Human Brain.” Science 382 (6667): eadd7046.\n\n\nSong, Dongyuan, Kexin Li, Xinzhou Ge, and Jingyi Jessica Li. 2023. “ClusterDE: A Post-Clustering Differential Expression (DE) Method Robust to False-Positive Inflation Caused by Double Dipping.” Research Square.\n\n\nSquair, Jordan W, Matthieu Gautier, Claudi Kathe, Mark A Anderson, Nicholas D James, Thomas H Hutson, Rémi Hudelle, et al. 2021. “Confronting False Discoveries in Single-Cell Differential Expression.” Nature Communications 12 (1): 5692.\n\n\nStatello, Luisa, Chun-Jie Guo, Ling-Ling Chen, and Maite Huarte. 2021. “Gene Regulation by Long Non-Coding RNAs and Its Biological Functions.” Nature Reviews Molecular Cell Biology 22 (2): 96–118.\n\n\nStreet, Kelly, Davide Risso, Russell B Fletcher, Diya Das, John Ngai, Nir Yosef, Elizabeth Purdom, and Sandrine Dudoit. 2018. “Slingshot: Cell Lineage and Pseudotime Inference for Single-Cell Transcriptomics.” BMC Genomics 19 (1): 477.\n\n\nStuart, Tim, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi, William M Mauck III, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. 2019. “Comprehensive Integration of Single-Cell Data.” Cell 177 (7): 1888–1902.\n\n\nSubramanian, Aravind, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert, Michael A Gillette, Amanda Paulovich, et al. 2005. “Gene Set Enrichment Analysis: A Knowledge-Based Approach for Interpreting Genome-Wide Expression Profiles.” Proceedings of the National Academy of Sciences 102 (43): 15545–50.\n\n\nSvensson, Valentine. 2020. “Droplet scRNA-Seq Is Not Zero-Inflated.” Nature Biotechnology 38 (2): 147–50.\n\n\n———. 2021. “Reply to: UMI or Not UMI, That Is the Question for scRNA-Seq Zero-Inflation.” Nature Biotechnology 39 (2): 160–60.\n\n\nSwaminath, Sharmada, and Alistair B Russell. 2024. “The Use of Single-Cell RNA-Seq to Study Heterogeneity at Varying Levels of Virus–Host Interactions.” PLoS Pathogens 20 (1): e1011898.\n\n\nTheunissen, Lauren, Thomas Mortier, Yvan Saeys, and Willem Waegeman. 2024. “Uncertainty-Aware Single-Cell Annotation with a Hierarchical Reject Option.” Bioinformatics 40 (3): btae128.\n\n\nTraag, Vincent A, Ludo Waltman, and Nees Jan Van Eck. 2019. “From Louvain to Leiden: Guaranteeing Well-Connected Communities.” Scientific Reports 9 (1): 1–12.\n\n\nTran, Hoa Thi Nhu, Kok Siong Ang, Marion Chevrier, Xiaomeng Zhang, Nicole Yee Shin Lee, Michelle Goh, and Jinmiao Chen. 2020. “A Benchmark of Batch-Effect Correction Methods for Single-Cell RNA Sequencing Data.” Genome Biology 21 (1): 1–32.\n\n\nTrapnell, Cole, Davide Cacchiarelli, Jonna Grimsby, Prapti Pokharel, Shuqiang Li, Michael Morse, Niall J Lennon, Kenneth J Livak, Tarjei S Mikkelsen, and John L Rinn. 2014. “The Dynamics and Regulators of Cell Fate Decisions Are Revealed by Pseudotemporal Ordering of Single Cells.” Nature Biotechnology 32 (4): 381.\n\n\nWang, Tianyu, Boyang Li, Craig E Nelson, and Sheida Nabavi. 2019. “Comparative Analysis of Differential Gene Expression Analysis Tools for Single-Cell RNA Sequencing Data.” BMC Bioinformatics 20 (1): 1–16.\n\n\nWang, Yizhuo, Christopher R Flowers, Michael Wang, Xuelin Huang, and Ziyi Li. 2024. “CASi: A Framework for Cross-Timepoint Analysis of Single-Cell RNA Sequencing Data.” Scientific Reports 14 (1): 10633.\n\n\nWolf, F Alexander, Fiona K Hamey, Mireya Plass, Jordi Solana, Joakim S Dahlin, Berthold Göttgens, Nikolaus Rajewsky, Lukas Simon, and Fabian J Theis. 2019. “PAGA: Graph Abstraction Reconciles Clustering with Trajectory Inference Through a Topology Preserving Map of Single Cells.” Genome Biology 20 (1): 59.\n\n\nXu, Chenling, Romain Lopez, Edouard Mehlman, Jeffrey Regier, Michael I Jordan, and Nir Yosef. 2021. “Probabilistic Harmonization and Annotation of Single-Cell Transcriptomics Data with Deep Generative Models.” Molecular Systems Biology 17 (1): e9620.\n\n\nYu, Lijia, Yue Cao, Jean YH Yang, and Pengyi Yang. 2022. “Benchmarking Clustering Algorithms on Estimating the Number of Cell Types from Single-Cell RNA-Sequencing Data.” Genome Biology 23 (1): 49.\n\n\nZeng, Hongkui. 2022. “What Is a Cell Type and How to Define It?” Cell 185 (15): 2739–55.\n\n\nZhang, Mengqi, Si Liu, Zhen Miao, Fang Han, Raphael Gottardo, and Wei Sun. 2022. “IDEAS: Individual Level Differential Expression Analysis for Single-Cell RNA-Seq Data.” Genome Biology 23 (1): 1–17.\n\n\nZhang, Shixiong, Xiangtao Li, Qiuzhen Lin, and Ka-Chun Wong. 2020. “Review of Single-Cell RNA-Seq Data Clustering for Cell Type Identification and Characterization.” arXiv Preprint arXiv:2001.01006.\n\n\nZhang, Yifan, Duc Tran, Tin Nguyen, Sergiu M Dascalu, and Frederick C Harris Jr. 2023. “A Robust and Accurate Single-Cell Data Trajectory Inference Method Using Ensemble Pseudotime.” BMC Bioinformatics 24 (1): 55.\n\n\nZhang, Yuqing, Giovanni Parmigiani, and W Evan Johnson. 2020. “ComBat-Seq: Batch Effect Adjustment for RNA-Seq Count Data.” NAR Genomics and Bioinformatics 2 (3): lqaa078.\n\n\nZhang, Zhaojun, Divij Mathew, Tristan L Lim, Kaishu Mason, Clara Morral Martinez, Sijia Huang, E John Wherry, et al. 2024. “Recovery of Biological Signals Lost in Single-Cell Batch Integration with CellANOVA.” Nature Biotechnology, 1–17.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  }
]