[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "scOmicNotes",
    "section": "",
    "text": "This is a Quarto book made from the content of BIOST 545 (Biostatistical Methods for Big Omics Data), taught by Dr. Kevin Lin at the University of Washington, Winter 2025. The lecture notes in BIOST 545 are meant to ease you into reading these notes, and the notes are meant to be a gateway to many references where the course’s goal is for you 1) to determine for yourself what areas you find interesting and are relevant to your research goals, and 2) use the references as a portal to find many ideas to bolster your own research. This Quarto book is assembled with the help of many student volunteers at the University of Washington: [TBD].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why study cell biology in public health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Efron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes\nMethods and False Discovery Rates for Microarrays.” Genetic\nEpidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and\nKenneth J Livak. 2021. “Applying High-Dimensional Single-Cell\nTechnologies to the Analysis of Cancer Immunotherapy.” Nature\nReviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle\nGaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular\nClassification of Cancer: Class Discovery and Class Prediction by Gene\nExpression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023.\n“Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and\nTrials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N\nShokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse\nProteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease\nBrain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde,\nand Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable\nResponses to Sequential and Prolonged Treatment.” Cell\nSystems 15 (3): 213–26.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the\nMolecular and Cellular Mechanisms of Aging.” Journal of\nInvestigative Dermatology 141 (4): 951–60.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith\nKnight. 2005. “Sparsity and Smoothness via the Fused\nLasso.” Journal of the Royal Statistical Society Series B:\nStatistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME\nBowdish. 2015. “An Introduction to Automated Flow Cytometry Gating\nTools and Their Implementation.” Frontiers in Immunology\n6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n.\n2015. “Microglial Cell Dysregulation in Brain Aging and\nNeurodegeneration.” Frontiers in Aging Neuroscience 7:\n124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao\nZhu, et al. 2022. “Single-Cell Technologies: From Research to\nApplication.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human\nBrain Development.” Nature Reviews Genetics 25 (1):\n26–45.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "intro.html#what-are-omics",
    "href": "intro.html#what-are-omics",
    "title": "2  Introduction",
    "section": "3.1 What are “omics”?",
    "text": "3.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n3.1.1 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 3.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 3.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 3.3.\n\n\n\n\n\n\n\n\nFigure 3.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#how-omics-comes-into-the-picture",
    "href": "intro.html#how-omics-comes-into-the-picture",
    "title": "2  Introduction",
    "section": "3.2 How “omics” comes into the picture",
    "text": "3.2 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 3.4 and Figure 3.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 3.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 3.7.\n\n\n\n\n\n\n\nFigure 3.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised—mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an ``average/typical’’ scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.\n\n\n\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "href": "intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "title": "2  Introduction",
    "section": "2.2 What do we hope to learn from single-cell data?",
    "text": "2.2 What do we hope to learn from single-cell data?\n\n2.2.1 Basic biology\nSingle-cell data offer a transformative lens to study the fundamental processes of life at unparalleled resolution. Unlike bulk data, which averages signals across populations of cells, single-cell technologies allow researchers to examine the diversity and complexity of individual cells within a tissue or organism. This level of detail provides insights into key biological phenomena, such as cellular differentiation during development, the plasticity of cell states in response to environmental cues, and the organization of complex tissues. For instance, single-cell RNA-sequencing (scRNA-seq) has uncovered new cell types in the brain and immune system, challenging traditional classifications and offering a more nuanced understanding of cellular identities and functions. These insights are essential for constructing more accurate models of how life operates at a cellular level.\nMany of the examples shown in Figs. Figure 2.1–Figure 2.2 are basic biology questions. Two additional examples are shown in Figure 2.8 and Figure 2.9.\n\n\n\n\n\n\n\nFigure 2.8: How do cancer cells survive successive therapies? (Schaff et al. 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: What deteriorates in a cell as it ages (Silva and Schumacher 2021)\n\n\n\n\n\n\n2.2.2 Benchside-to-bedside applications\nSingle-cell data have enormous potential to revolutionize clinical practice by bridging molecular biology and medicine. After learning basic biology, the next stage is to use our newfound understanding to advance treatments. (We typically call this translation research, to denote translating our basic biology knowledge to therapeutic improvements.) This is also called benchside (for the wet-bench, i.e. laboratory setting) to bedside (for the hospital setting).\nBy mapping cellular heterogeneity in diseased and healthy tissues, researchers can identify specific cell populations driving disease progression and therapeutic resistance. For example, in cancer, single-cell analyses have uncovered rare tumor subclones that evade treatment, providing critical targets for drug development. Similarly, in autoimmune diseases like rheumatoid arthritis, single-cell profiling of synovial tissues has identified inflammatory cell states that correlate with disease severity and treatment response. Beyond diagnostics, this technology enables precision medicine by tailoring treatments to the molecular profiles of individual patients. As single-cell approaches continue to evolve, they are poised to refine drug-discovery pipelines, improve vaccine design, and ultimately transform how diseases are diagnosed and treated.\nThere are a few ways this typically happens. Figure 2.10 shows one example, where single-cell research helps identify the specific cell types and specific edits needed to improve cellular function. Figure 2.11 shows another example, where understanding the cellular functions, we can improve how conventional methods can be used to measure more accurate biomarkers. Cancer research has been (by far) the biggest beneficiary of single-cell research, and Figure 2.12 illustrates how single-cell improves cancer therapies.\n\n\n\n\n\n\n\nFigure 2.10: There are current single-cell therapies that involve extracting blood from a donor, altering the cells outside the body, and then infusing the altered blood back into the donor. (Source: https://www.cancer.gov/news-events/cancer-currents-blog/2020/crispr-cancer-research-treatment)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: After learning the causal mechanisms for specific cell types for Alzheimer’s disease that can be detected from blood draws, we can refine existing biomarkers to monitor Alzheimer’s. (Hansson et al. 2023)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Example of how insights from single-cell studies transform cancer therapies. (Gohil et al. 2021)\n\n\n\n\n\n\n\n2.2.3 What existed prior to single-cell data?\nWestern blots and flow cytometry.\nBefore the advent of single-cell technologies, biological research relied heavily on methods like western blots and flow cytometry to study cells and molecules. Western blotting (shown in Figure 2.13 and Figure 2.14), developed in the late 1970 s, enabled researchers to detect and quantify specific proteins in a sample, providing insights into cellular pathways and protein-expression levels. However, this technique required lysing entire tissues or cell populations, averaging the signals from thousands or millions of cells. Flow cytometry (shown in Figure 2.15 and Figure 2.16), emerging in the 1980s, represented a major step forward by allowing researchers to analyse individual cells’ physical and chemical characteristics in suspension. While flow cytometry offered single-cell resolution, it was limited to analysing predefined markers and could not capture the full complexity of cellular states or gene expression.\n\n\n\n\n\n\n\nFigure 2.13: Illustration of the Western-blot technique. (Source: https://www.bio-rad.com/en-us/applications-technologies/western-blotting-electrophoresis-techniques)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Example Western-blot data. (Kepchia et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Illustration of flow-cytometry technique. (Source: https://www.streck.com/blog/principles-of-flow-cytometry/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: Example flow-cytometry data. (Verschoor et al. 2015)\n\n\n\n\nMicroarray technology.\nThe 1990s marked the rise of microarray technology (shown in Figure 2.17 and Figure 2.18), which allowed scientists to measure the expression levels of thousands of genes simultaneously. Microarrays revolutionised transcriptomics by enabling high-throughput studies of gene activity in various conditions, tissues, and diseases. Despite its transformative impact, microarray analysis was fundamentally a bulk method, averaging signals across all cells in a sample. It is also based on light intensity, which can be tricky to extract consistently. These limitations obscured cellular heterogeneity, especially in complex tissues where distinct cell types or states contribute uniquely to biological processes or disease mechanisms.\n\n\n\n\n\n\n\nFigure 2.17: Illustration of microarray technique. (Source: https://www.onlinebiologynotes.com/dna-microarray-principle-types-and-steps-involved-in-cdna-microarrays/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.18: Example microarray image prior to quantification. (Source: https://online.stat.psu.edu/stat555/node/28/)\n\n\n\n\n\nRemark (Personal opinion: the close history between microarrays and high-dimensional statistics).\nHistorically, the rise of microarray data spurred advances in high-dimensional statistics — e.g. the Lasso (Tibshirani et al. 2005), gene-expression classification for leukaemia (Golub et al. 1999), and empirical-Bayes multiple testing (Efron and Tibshirani 2002).\n\nBulk sequencing technologies.\nIn the 2000s, bulk sequencing technologies for DNA and RNA emerged, further advancing the study of genomes and transcriptomes. RNA-sequencing (RNA-seq) became a powerful tool for capturing the entire transcriptome with greater accuracy and dynamic range than microarrays. Similarly, DNA sequencing enabled comprehensive studies of genetic variation, from point mutations to structural alterations. However, like microarrays, bulk sequencing aggregated signals across many cells, masking rare cell populations and the heterogeneity critical to understanding dynamic processes such as tumour evolution or immune responses. These bulk techniques laid the groundwork for single-cell methods by driving innovations in high-throughput sequencing and data analysis, which would later be adapted for single-cell resolution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "href": "intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "title": "2  Introduction",
    "section": "2.3 When did single-cell data become popular, and how has the technology advanced?",
    "text": "2.3 When did single-cell data become popular, and how has the technology advanced?\n\n2.3.1 The rise of single-cell data\nSingle-cell data began gaining prominence in the early 2010s, fuelled by advances in microfluidics and next-generation sequencing. Single-cell RNA-sequencing (scRNA-seq), pioneered around 2009 – 2011, was among the first methods to achieve widespread adoption. It enabled measurement of gene expression in individual cells, uncovering heterogeneity that bulk analyses masked. Early applications revealed new cell types and states, reshaped our understanding of development, and identified rare populations in cancers and neurodegenerative disorders. Popularity grew as throughput increased, costs fell, and workflows became standardised.\nFigure 2.19 and Figure 2.20 contrasts bulk and single-cell sequencing. Figure 2.21 shows how single-cell data tease apart different sources of heterogeneity.\n\n\n\n\n\n\n\nFigure 2.19: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.21: Single-cell insights disentangle underlying biological mechanisms. (Lei et al. 2021)\n\n\n\n\n\n\n2.3.2 Expansion into other omics and spatial technologies\nBuilding on single-cell transcriptomics, the field rapidly expanded into other omics. Single-cell proteomics allows detailed analysis of protein expression and signalling pathways. Single-cell ATAC-seq profiles chromatin accessibility; Hi-C and related methods reveal 3D genome architecture. Spatial transcriptomics connects gene expression with tissue context. CRISPR-based single-cell screens enable high-throughput perturbations, and lineage-tracing barcodes add a temporal dimension, charting cell ancestry in development and disease. Together, these advances transformed single-cell biology into a multi-dimensional, integrative discipline.\nMany of these technologies appear in Figure 2.22.\n\n\n\n\n\n\n\nFigure 2.22: Illustration (circa 2020) of technologies that pair various omics at single-cell resolution. (Teichmann and Efremova 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "href": "intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "title": "2  Introduction",
    "section": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?",
    "text": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?\nCell biology is vast – especially for students trained primarily in statistics or biostatistics. Many disciplines intersect with cell biology. For example:\n\nStatistics / Biostatistics – statistical models for complex biological processes; translation between maths and biology\nComputational biology – scalable computation, leveraging public data\nBioinformatics / Genetics – tools that draw on large‐consortium resources\nEpidemiology – population-level data and policy recommendations\nBioengineering – new laboratory technologies for cheaper/faster measurement or imaging\nBiology – mechanistic studies in model organisms\nBiochemistry / Molecular biology – structure, function, interactions of specific molecules\nWet-bench medicine – disease mechanisms via tissues, models, cell lines\nClinical-facing medicine – patient treatment and real-world sample collection\nPharmacology – integrating evidence to design new drugs and therapies\n\nGiven so many players, what does a biostatistician contribute?\n\n2.4.1 How a biostatistician perceives the world\nGive me a concrete (ideally cleaned) dataset: the larger the better—and I will analyse it from many angles.\nCausality: A mathematically stricter notion than correlation, usually via (1) counterfactual reasoning, or (2) a directed-acyclic-graph picture of how variables relate.\n\nRemark (Personal opinion: statistical causality for cell biology is extremely difficult). Obstacles: (i) tracking the same cell over time is impossible because sequencing lyses it; (ii) longitudinal human tissue samples are rare. Strong modelling assumptions can help but must withstand biological scrutiny. The bottleneck is often data, not maths—ambitious statisticians who learn enough biology still have a fighting chance.\n\n\nRemark (Single-cell methods are largely an “associative” world): Most single-cell analyses discover mechanisms that are statistically correlational; the causal proof comes from experiments and biology.\n\nThe research inquiry starts and ends with a method: (How to integrate modalities? learn a gene-regulatory network? perform valid post-clustering tests?) Start with a statistical model and a parameter of interest \\(\\theta^*\\), then typically:\n\nDevelop a novel estimator of \\(\\theta^*\\), explaining why current methods fail (e.g. lack robustness, accuracy, power, or are heuristic). Focus on statistical logic: A clear, simple mathematical intuition should show the gap and how the new method fills it.\nProve theorems showing the estimate \\(\\hat\\theta\\) converges to \\(\\theta^*\\) under stated assumptions. Focus on consistency & convergence: More data should provably yield more accurate results (often the highlight of a statistics paper).\nSimulations demonstrating that when the true \\(\\theta^*\\) is known, \\(\\hat\\theta\\) beats competing estimators across many settings. Illustration via benchmarking: Empirically recover the correct answer more often than existing methods.\nReal-data demonstration showing results align with known biology or provide biologically sensible new insights. Focus on practicality: The method must work in real scenarios mirroring its target audience.\n\nMindset: deliver a reliable tool that others can trust as-is. Human validations are often impractical; guard-rails and diagnostics are vital.\nWhy biostatisticians need wet-lab biologists / clinicians: We rarely generate data ourselves, so collaborators supply (i) exciting data with novel questions, (ii) biological context for sensible assumptions, and (iii) experimental validation of statistical findings.\n\n\n2.4.2 How a wet-lab biologist / clinician perceives the world\nExperiments, experiments, experiments: Carefully controlled—even if small—to make downstream analysis straightforward.\nCausality comes from a chain of experiments. Suppose we study a gene’s role in disease:\n\nTemporal evidence, such as change in gene expression preceding a change in cell phenotype. A causal mechanism should occur before the phenotype.\nBiological logic providing explanation of the underlying mechanism (binding factors, protein function, evolutionary rationale, etc.). For example, there must be a coherent pathway from gene → protein → phenotype.\nUniversality of how the described association persists across cell lines or organisms. A causal mechanism should be discoverable in other systems (extent depends on how general the logic is). This is offten the highlight of a biology paper.\nValidation, such as knocking out the gene alters the outcome, whereas similar genes do not. For example, perturbing the specific gene (not its close counterparts) changes the outcome.\n\nInquiry starts and ends with a biological hypothesis: Large intellectual effort goes into proposing explanations and designing experiments to rule them in or out.\nMindset: Assemble overwhelming evidence for a mechanism, combining careful experiments and biological logic.\nWhy wet-lab scientists need biostatisticians: Data are now complex and plentiful; exhaustive experiments for every hypothesis are infeasible. Statistical methods can (i) account for data & biological complexity and (ii) prioritise hypotheses worth experimental investment.\n\nSo how does a biostatistician develop computational / statistical methods for cell biology?\n\n\nBiological context – What is the biological system and the “north-star” question? Which premises are accepted, which ones are to be tested, and why is it important to understand this mechanism better?\nTechnology, experiment, data – How are data collected and why did you choose this particular {technology, experiment, data} trio? What technical artefacts arise?\nBoundaries of current tools – Simple analyses first: what “breaks” in existing workflows? Is there preliminary evidence a new computational method would do better?\nStatistical model – What is the insight that a different computational method could interrogate the biology better? (Here the statistics training begins.)\nDevelop a method & show robustness – Robustness can be defined numerically (i.e., noise tolerance) or biologically (i.e., applicable across contexts/environments). Often, the biological question you study lacks a ground truth, so validity arguments lean on biological logic.\nUncover new / refined biology – Does the method advance our biological understanding? How confident are we that findings generalise beyond the original {technology, experiment, data} trio? (This is usually the crown-jewel of your computational biology paper – it’s not necessary your specific analyses, but the potential that your tool can be used on other biological studies beyond what you’ve demonstrated your method on.)\n\n\n\n\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes Methods and False Discovery Rates for Microarrays.” Genetic Epidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and Kenneth J Livak. 2021. “Applying High-Dimensional Single-Cell Technologies to the Analysis of Cancer Immunotherapy.” Nature Reviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023. “Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and Trials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N Shokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse Proteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease Brain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun Yu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in Cancer Research: Progress and Perspectives.” Journal of Hematology & Oncology 14 (1): 91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde, and Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable Responses to Sequential and Prolonged Treatment.” Cell Systems 15 (3): 213–26.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to Tailor Treatments.” Science Translational Medicine 9 (408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the Molecular and Cellular Mechanisms of Aging.” Journal of Investigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year 2019: Single-Cell Multimodal Omics.” Nature Methods 17 (1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. “Sparsity and Smoothness via the Fused Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME Bowdish. 2015. “An Introduction to Automated Flow Cytometry Gating Tools and Their Implementation.” Frontiers in Immunology 6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-study-cell-biology-in-public-health",
    "href": "intro.html#why-study-cell-biology-in-public-health",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n\n2.1.2 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 2.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 2.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)\n\n\n\n\n\n\n2.1.3 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 2.4 and Figure 2.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 2.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised – its instead mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an “average/typical” scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html",
    "href": "chapter1_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why study cell biology in public health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#why-study-cell-biology-in-public-health",
    "href": "chapter1_intro.html#why-study-cell-biology-in-public-health",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n\n2.1.2 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 2.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 2.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)\n\n\n\n\n\n\n2.1.3 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 2.4 and Figure 2.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 2.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised – its instead mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an “average/typical” scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "href": "chapter1_intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "title": "2  Introduction",
    "section": "2.2 What do we hope to learn from single-cell data?",
    "text": "2.2 What do we hope to learn from single-cell data?\n\n2.2.1 Basic biology\nSingle-cell data offer a transformative lens to study the fundamental processes of life at unparalleled resolution. Unlike bulk data, which averages signals across populations of cells, single-cell technologies allow researchers to examine the diversity and complexity of individual cells within a tissue or organism. This level of detail provides insights into key biological phenomena, such as cellular differentiation during development, the plasticity of cell states in response to environmental cues, and the organization of complex tissues. For instance, single-cell RNA-sequencing (scRNA-seq) has uncovered new cell types in the brain and immune system, challenging traditional classifications and offering a more nuanced understanding of cellular identities and functions. These insights are essential for constructing more accurate models of how life operates at a cellular level.\nMany of the examples shown in Figs. Figure 2.1–Figure 2.2 are basic biology questions. Two additional examples are shown in Figure 2.8 and Figure 2.9.\n\n\n\n\n\n\n\nFigure 2.8: How do cancer cells survive successive therapies? (Schaff et al. 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: What deteriorates in a cell as it ages (Silva and Schumacher 2021)\n\n\n\n\n\n\n2.2.2 Benchside-to-bedside applications\nSingle-cell data have enormous potential to revolutionize clinical practice by bridging molecular biology and medicine. After learning basic biology, the next stage is to use our newfound understanding to advance treatments. (We typically call this translation research, to denote translating our basic biology knowledge to therapeutic improvements.) This is also called benchside (for the wet-bench, i.e. laboratory setting) to bedside (for the hospital setting).\nBy mapping cellular heterogeneity in diseased and healthy tissues, researchers can identify specific cell populations driving disease progression and therapeutic resistance. For example, in cancer, single-cell analyses have uncovered rare tumor subclones that evade treatment, providing critical targets for drug development. Similarly, in autoimmune diseases like rheumatoid arthritis, single-cell profiling of synovial tissues has identified inflammatory cell states that correlate with disease severity and treatment response. Beyond diagnostics, this technology enables precision medicine by tailoring treatments to the molecular profiles of individual patients. As single-cell approaches continue to evolve, they are poised to refine drug-discovery pipelines, improve vaccine design, and ultimately transform how diseases are diagnosed and treated.\nThere are a few ways this typically happens. Figure 2.10 shows one example, where single-cell research helps identify the specific cell types and specific edits needed to improve cellular function. Figure 2.11 shows another example, where understanding the cellular functions, we can improve how conventional methods can be used to measure more accurate biomarkers. Cancer research has been (by far) the biggest beneficiary of single-cell research, and Figure 2.12 illustrates how single-cell improves cancer therapies.\n\n\n\n\n\n\n\nFigure 2.10: There are current single-cell therapies that involve extracting blood from a donor, altering the cells outside the body, and then infusing the altered blood back into the donor. (Source: https://www.cancer.gov/news-events/cancer-currents-blog/2020/crispr-cancer-research-treatment)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: After learning the causal mechanisms for specific cell types for Alzheimer’s disease that can be detected from blood draws, we can refine existing biomarkers to monitor Alzheimer’s. (Hansson et al. 2023)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Example of how insights from single-cell studies transform cancer therapies. (Gohil et al. 2021)\n\n\n\n\n\n\n\n2.2.3 What existed prior to single-cell data?\nWestern blots and flow cytometry.\nBefore the advent of single-cell technologies, biological research relied heavily on methods like western blots and flow cytometry to study cells and molecules. Western blotting (shown in Figure 2.13 and Figure 2.14), developed in the late 1970 s, enabled researchers to detect and quantify specific proteins in a sample, providing insights into cellular pathways and protein-expression levels. However, this technique required lysing entire tissues or cell populations, averaging the signals from thousands or millions of cells. Flow cytometry (shown in Figure 2.15 and Figure 2.16), emerging in the 1980s, represented a major step forward by allowing researchers to analyse individual cells’ physical and chemical characteristics in suspension. While flow cytometry offered single-cell resolution, it was limited to analysing predefined markers and could not capture the full complexity of cellular states or gene expression.\n\n\n\n\n\n\n\nFigure 2.13: Illustration of the Western-blot technique. (Source: https://www.bio-rad.com/en-us/applications-technologies/western-blotting-electrophoresis-techniques)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Example Western-blot data. (Kepchia et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Illustration of flow-cytometry technique. (Source: https://www.streck.com/blog/principles-of-flow-cytometry/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: Example flow-cytometry data. (Verschoor et al. 2015)\n\n\n\n\nMicroarray technology.\nThe 1990s marked the rise of microarray technology (shown in Figure 2.17 and Figure 2.18), which allowed scientists to measure the expression levels of thousands of genes simultaneously. Microarrays revolutionised transcriptomics by enabling high-throughput studies of gene activity in various conditions, tissues, and diseases. Despite its transformative impact, microarray analysis was fundamentally a bulk method, averaging signals across all cells in a sample. It is also based on light intensity, which can be tricky to extract consistently. These limitations obscured cellular heterogeneity, especially in complex tissues where distinct cell types or states contribute uniquely to biological processes or disease mechanisms.\n\n\n\n\n\n\n\nFigure 2.17: Illustration of microarray technique. (Source: https://www.onlinebiologynotes.com/dna-microarray-principle-types-and-steps-involved-in-cdna-microarrays/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.18: Example microarray image prior to quantification. (Source: https://online.stat.psu.edu/stat555/node/28/)\n\n\n\n\n\nRemark (Personal opinion: the close history between microarrays and high-dimensional statistics).\nHistorically, the rise of microarray data spurred advances in high-dimensional statistics — e.g. the Lasso (Tibshirani et al. 2005), gene-expression classification for leukaemia (Golub et al. 1999), and empirical-Bayes multiple testing (Efron and Tibshirani 2002).\n\nBulk sequencing technologies.\nIn the 2000s, bulk sequencing technologies for DNA and RNA emerged, further advancing the study of genomes and transcriptomes. RNA-sequencing (RNA-seq) became a powerful tool for capturing the entire transcriptome with greater accuracy and dynamic range than microarrays. Similarly, DNA sequencing enabled comprehensive studies of genetic variation, from point mutations to structural alterations. However, like microarrays, bulk sequencing aggregated signals across many cells, masking rare cell populations and the heterogeneity critical to understanding dynamic processes such as tumour evolution or immune responses. These bulk techniques laid the groundwork for single-cell methods by driving innovations in high-throughput sequencing and data analysis, which would later be adapted for single-cell resolution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "href": "chapter1_intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "title": "2  Introduction",
    "section": "2.3 When did single-cell data become popular, and how has the technology advanced?",
    "text": "2.3 When did single-cell data become popular, and how has the technology advanced?\n\n2.3.1 The rise of single-cell data\nSingle-cell data began gaining prominence in the early 2010s, fuelled by advances in microfluidics and next-generation sequencing. Single-cell RNA-sequencing (scRNA-seq), pioneered around 2009 – 2011, was among the first methods to achieve widespread adoption. It enabled measurement of gene expression in individual cells, uncovering heterogeneity that bulk analyses masked. Early applications revealed new cell types and states, reshaped our understanding of development, and identified rare populations in cancers and neurodegenerative disorders. Popularity grew as throughput increased, costs fell, and workflows became standardised.\nFigure 2.19 and Figure 2.20 contrasts bulk and single-cell sequencing. Figure 2.21 shows how single-cell data tease apart different sources of heterogeneity.\n\n\n\n\n\n\n\nFigure 2.19: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.21: Single-cell insights disentangle underlying biological mechanisms. (Lei et al. 2021)\n\n\n\n\n\n\n2.3.2 Expansion into other omics and spatial technologies\nBuilding on single-cell transcriptomics, the field rapidly expanded into other omics. Single-cell proteomics allows detailed analysis of protein expression and signalling pathways. Single-cell ATAC-seq profiles chromatin accessibility; Hi-C and related methods reveal 3D genome architecture. Spatial transcriptomics connects gene expression with tissue context. CRISPR-based single-cell screens enable high-throughput perturbations, and lineage-tracing barcodes add a temporal dimension, charting cell ancestry in development and disease. Together, these advances transformed single-cell biology into a multi-dimensional, integrative discipline.\nMany of these technologies appear in Figure 2.22.\n\n\n\n\n\n\n\nFigure 2.22: Illustration (circa 2020) of technologies that pair various omics at single-cell resolution. (Teichmann and Efremova 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "href": "chapter1_intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "title": "2  Introduction",
    "section": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?",
    "text": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?\nCell biology is vast – especially for students trained primarily in statistics or biostatistics. Many disciplines intersect with cell biology. For example:\n\nStatistics / Biostatistics – statistical models for complex biological processes; translation between maths and biology\nComputational biology – scalable computation, leveraging public data\nBioinformatics / Genetics – tools that draw on large‐consortium resources\nEpidemiology – population-level data and policy recommendations\nBioengineering – new laboratory technologies for cheaper/faster measurement or imaging\nBiology – mechanistic studies in model organisms\nBiochemistry / Molecular biology – structure, function, interactions of specific molecules\nWet-bench medicine – disease mechanisms via tissues, models, cell lines\nClinical-facing medicine – patient treatment and real-world sample collection\nPharmacology – integrating evidence to design new drugs and therapies\n\nGiven so many players, what does a biostatistician contribute?\n\n2.4.1 How a biostatistician perceives the world\nGive me a concrete (ideally cleaned) dataset: the larger the better—and I will analyse it from many angles.\nCausality: A mathematically stricter notion than correlation, usually via (1) counterfactual reasoning, or (2) a directed-acyclic-graph picture of how variables relate.\n\nRemark (Personal opinion: statistical causality for cell biology is extremely difficult). Obstacles: (i) tracking the same cell over time is impossible because sequencing lyses it; (ii) longitudinal human tissue samples are rare. Strong modelling assumptions can help but must withstand biological scrutiny. The bottleneck is often data, not maths—ambitious statisticians who learn enough biology still have a fighting chance.\n\n\nRemark (Single-cell methods are largely an “associative” world): Most single-cell analyses discover mechanisms that are statistically correlational; the causal proof comes from experiments and biology.\n\nThe research inquiry starts and ends with a method: (How to integrate modalities? learn a gene-regulatory network? perform valid post-clustering tests?) Start with a statistical model and a parameter of interest \\(\\theta^*\\), then typically:\n\nDevelop a novel estimator of \\(\\theta^*\\), explaining why current methods fail (e.g. lack robustness, accuracy, power, or are heuristic). Focus on statistical logic: A clear, simple mathematical intuition should show the gap and how the new method fills it.\nProve theorems showing the estimate \\(\\hat\\theta\\) converges to \\(\\theta^*\\) under stated assumptions. Focus on consistency & convergence: More data should provably yield more accurate results (often the highlight of a statistics paper).\nSimulations demonstrating that when the true \\(\\theta^*\\) is known, \\(\\hat\\theta\\) beats competing estimators across many settings. Illustration via benchmarking: Empirically recover the correct answer more often than existing methods.\nReal-data demonstration showing results align with known biology or provide biologically sensible new insights. Focus on practicality: The method must work in real scenarios mirroring its target audience.\n\nMindset: deliver a reliable tool that others can trust as-is. Human validations are often impractical; guard-rails and diagnostics are vital.\nWhy biostatisticians need wet-lab biologists / clinicians: We rarely generate data ourselves, so collaborators supply (i) exciting data with novel questions, (ii) biological context for sensible assumptions, and (iii) experimental validation of statistical findings.\n\n\n2.4.2 How a wet-lab biologist / clinician perceives the world\nExperiments, experiments, experiments: Carefully controlled—even if small—to make downstream analysis straightforward.\nCausality comes from a chain of experiments. Suppose we study a gene’s role in disease:\n\nTemporal evidence, such as change in gene expression preceding a change in cell phenotype. A causal mechanism should occur before the phenotype.\nBiological logic providing explanation of the underlying mechanism (binding factors, protein function, evolutionary rationale, etc.). For example, there must be a coherent pathway from gene → protein → phenotype.\nUniversality of how the described association persists across cell lines or organisms. A causal mechanism should be discoverable in other systems (extent depends on how general the logic is). This is offten the highlight of a biology paper.\nValidation, such as knocking out the gene alters the outcome, whereas similar genes do not. For example, perturbing the specific gene (not its close counterparts) changes the outcome.\n\nInquiry starts and ends with a biological hypothesis: Large intellectual effort goes into proposing explanations and designing experiments to rule them in or out.\nMindset: Assemble overwhelming evidence for a mechanism, combining careful experiments and biological logic.\nWhy wet-lab scientists need biostatisticians: Data are now complex and plentiful; exhaustive experiments for every hypothesis are infeasible. Statistical methods can (i) account for data & biological complexity and (ii) prioritise hypotheses worth experimental investment.\n\nSo how does a biostatistician develop computational / statistical methods for cell biology?\n\n\nBiological context – What is the biological system and the “north-star” question? Which premises are accepted, which ones are to be tested, and why is it important to understand this mechanism better?\nTechnology, experiment, data – How are data collected and why did you choose this particular {technology, experiment, data} trio? What technical artefacts arise?\nBoundaries of current tools – Simple analyses first: what “breaks” in existing workflows? Is there preliminary evidence a new computational method would do better?\nStatistical model – What is the insight that a different computational method could interrogate the biology better? (Here the statistics training begins.)\nDevelop a method & show robustness – Robustness can be defined numerically (i.e., noise tolerance) or biologically (i.e., applicable across contexts/environments). Often, the biological question you study lacks a ground truth, so validity arguments lean on biological logic.\nUncover new / refined biology – Does the method advance our biological understanding? How confident are we that findings generalise beyond the original {technology, experiment, data} trio? (This is usually the crown-jewel of your computational biology paper – it’s not necessary your specific analyses, but the potential that your tool can be used on other biological studies beyond what you’ve demonstrated your method on.)\n\n\n\n\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes Methods and False Discovery Rates for Microarrays.” Genetic Epidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and Kenneth J Livak. 2021. “Applying High-Dimensional Single-Cell Technologies to the Analysis of Cancer Immunotherapy.” Nature Reviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023. “Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and Trials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N Shokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse Proteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease Brain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun Yu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in Cancer Research: Progress and Perspectives.” Journal of Hematology & Oncology 14 (1): 91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde, and Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable Responses to Sequential and Prolonged Treatment.” Cell Systems 15 (3): 213–26.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to Tailor Treatments.” Science Translational Medicine 9 (408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the Molecular and Cellular Mechanisms of Aging.” Journal of Investigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year 2019: Single-Cell Multimodal Omics.” Nature Methods 17 (1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. “Sparsity and Smoothness via the Fused Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME Bowdish. 2015. “An Introduction to Automated Flow Cytometry Gating Tools and Their Implementation.” Frontiers in Immunology 6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html",
    "href": "chapter2_sequencing.html",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "3.1 The sparse count matrix and the negative binomial\nASDF ASDF ASDF ASDF At the heart of single-cell sequencing data lies the sparse count matrix, where rows represent genes, columns represent cells, and entries capture the number of sequencing reads (or molecules) detected for each gene in each cell. This matrix is typically sparse because most genes are not expressed in any given cell, leading to a prevalence of zeros. The data’s sparsity reflects both the biological reality of selective gene expression and technical limitations such as sequencing depth. To model the variability in these counts, the negative binomial distribution is often employed. This distribution accommodates overdispersion, where the observed variability in gene expression counts exceeds what would be expected under simpler models like the Poisson distribution. By accounting for both biological heterogeneity and technical noise, the negative binomial provides a robust framework for analyzing sparse single-cell data.\nLet \\(X \\in \\mathbb{R}^{n\\times p}\\) denote the single-cell data matrix with \\(n\\) cells (rows) and \\(p\\) features (columns). In a single-cell RNA-seq data matrix, the \\(p\\) features represent the \\(p\\) genes.\nHere are some basic statistics about these matrices (based mainly from my experience):\nBased on these observations, the negative binomial distribution is very commonly used.\nThe negative binomial (NB) distribution is a widely used model in single-cell RNA-seq analysis due to its ability to handle overdispersion, which is common in gene expression data. Overdispersion occurs when the variance of the data exceeds the mean, a phenomenon that cannot be captured by the Poisson distribution. The NB distribution introduces an additional parameter to model this extra variability, making it more flexible for single-cell data.\nLet us focus on the count for cell \\(i\\) and gene \\(j\\), i.e., the value \\(X_{ij}\\). The probability mass function (pmf) of the NB distribution for a random variable ( X_{ij} ) is given by:\n\\[\nP(X_{ij} = k) = \\binom{k + r_j - 1}{k} p_{ij}^r (1 - p_{ij})^k, \\quad k = 0, 1, 2, \\ldots\n\\tag{3.1}\\]\nwhere ( r_j &gt; 0 ) is the dispersion (or “overdispersion”) parameter and ( p_{ij} (0, 1) ) is the probability of success. This is the “standard” parameterization, mentioned in https://en.wikipedia.org/wiki/Negative_binomial_distribution. This specific parameterization is actually not very commonly used.\nAlternatively, the NB distribution is most commonly parameterized in terms of the mean \\(\\mu_{ij}\\) and the dispersion parameter \\(r_j\\), which is often preferred in single-cell analysis:\n\\[\n\\text{Mean: } \\mu_{ij}, \\quad \\text{Variance: } \\mu_{ij} + \\frac{\\mu_{ij}^2}{r_j} = \\mu_{ij}\\left(1 + \\frac{\\mu_{ij}}{r_j}\\right).\n\\tag{3.2}\\]\n(Compare these relations to the Poisson distribution, where both the mean and variance would be \\(\\mu_{ij}\\).)\nTo relate equation Equation 3.1 to Equation 3.2, observe that we can derive,\n\\[\n\\mu_{ij} = \\frac{r_j (1 - p_{ij})}{p_{ij}}.\n\\]\nWe typically use a different overdispersion parameter \\(r_j\\) for each gene \\(j \\in \\{1,\\ldots,p\\}\\). Here, ( r_j ) controls the degree of overdispersion. When ( r_j ), the variance approaches the mean, and the NB distribution converges to the Poisson distribution. For single-cell RNA-seq data, the introduction of ( r_j ) allows the model to capture both the biological variability between cells and technical noise, making it robust to the sparse and overdispersed nature of gene expression data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-sparse-count-matrix-and-the-negative-binomial",
    "href": "chapter2_sequencing.html#the-sparse-count-matrix-and-the-negative-binomial",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "For \\(n\\), we usually have 10,000 to 500,000 cells. These cells originate from a specific set of samples/donors/organisms1. See ?fig-lupus and ?fig-zebrafish for larger examples of single-cell datasets.\n\nFor \\(p\\), we usually have about 30,000 genes (but as you’ll see, we usually limit the analysis to 2000 to 5000 genes chosen from this full set).2\n\nIn a typical scRNA-seq matrix, more than 70% of the elements are exactly 0. (And among the remaining 30%, typically half are exactly 1. The maximum count can be in the hundreds, i.e., the distribution is extremely right-skewed.) This is illustrated in Figure 3.1 and Figure 3.2. We’ll see later in ?sec-scrnaseq_tech where these ``counts’’ come from.\n\n\n\n\n\n\n\n\n\nFigure 3.1: Illustration of data from https://satijalab.org/seurat/articles/pbmc3k_tutorial.html showing a snippet of the count matrix\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Illustration of data from https://satijalab.org/seurat/articles/pbmc3k_tutorial.html showing the percentage of cells with non-zero counts for each gene\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy would genes even have different overdispersions?\nWe’re assuming in most statistical models that: 1) genes are modeled as a negative binomial random variable, and 2) genes have different overdispersion parameters \\(r_j\\). Why is this reasonable? Is there a biomolecular reason for this?\nWe draw upon (sarkar2021separating?), explaining how the NB distribution is justified (both theoretically and empirically) for scRNA-seq data.\nAfter reading this paper, you’ll see that the overdispersion originates from the “biological noise” (i.e., the Gamma distribution). This differs from gene to gene because gene expression is subject to many aspects: transcriptomic bursting, the proximity between the gene and the promoter, the mRNA stability and degradation, copy number variation, the cell cycle stage, etc.\n\n\n3.1.0.1 How do you estimate the overdispersion parameter in practice?\nThe easiest way is to use MASS::glm.nb in R.\nHowever, this is very noisy for single-cell data (see Remark 3.1).\nSince we are estimate one dispersion parameter per gene, many methods use an Empirical Bayes framework to “smooth” the overdispersion parameters (love2014moderated?).\nMore sophisticated methods (such as SCTransform (hafemeister2019normalization?)) use the Bioconductor R package called glmGamPoi.\nDeep-learning methods simply incorporate estimating the dispersion parameter into the architecture and objective function, see scVI (lopez2018deep?).\nAlso, note that which we are assuming here that the overdispersion parameter \\(r_j\\) is shared across all the cells for gene \\(j\\), there are methods that also use different overdispersion parameters for different cell populations. See (chen2018umi?) or the dispersion parameter in scVI (see https://docs.scvi-tools.org/en/stable/api/reference/scvi.model.SCVI.html#scvi.model.SCVI). In general, using different overdispersion parameters for the same gene across different cell populations is not common, so my suggestion is to: 1) have a diagnostic in mind on how would you know if need to use different overdispersion parameters for the same gene, and then 2) try analyzing the genes where each gene only has one overdispersion parameter and see if you have enough concrete evidence that such a model was too simplistic.\n\nPersonal opinion: My preferred parameterization\nThe formulation Equation 3.2 by far is the most common way people write the NB distribution.\nWe typically call \\(r_j\\) the “overdispersion” parameter, since the inclusion of \\(r_j\\) in our modeling is typically to denote that there is more variance than a Poisson.\nI personally don’t like it because I find it confusing that a larger \\(r_j\\) denotes a distribution with smaller variance.\nHence, I usually write the variance as \\(\\mu_{ij}(1+\\alpha_j \\mu_{ij})\\) (where \\(\\alpha_j = 1/r_j\\)). This way, \\(\\alpha_j = 0\\) is a Poisson distribution. However, even though this makes more sense to me, it’s not commonly used. (It’s because this parameterization makes it confusing to work with the exponential-family distributions mathematically.)\nThe main takeaway is to always pay close attention to each paper’s NB parameterization. You’re looking for the mean-variance relation written somewhere in the paper to ensure you understand the author’s notation.\n\n\nRemark 3.1. What makes the negative binomial tricky Let me use an analogy that is based on the more familiar coin toss. Suppose 30 people each are given a weighted coin where the probability of heads is \\(10^{-9}\\) (basically, it’s impossible to get a heads).\nEveryone coerce and agree to a secret number \\(N\\), and everyone independently flips their own coin \\(N\\) times and records the number of heads.\nI (the moderator) do not know \\(N\\), but I get to see how many heads each person got during this experiment.\nMy goal is to estimate \\(N\\). What makes this problem very difficult?\nIf \\(N\\) were 10, or 100, or maybe even 10,000, almost everyone would get 0 heads. That is, we cannot reliably estimate \\(N\\) since the log-likelihood function is very flat. The issue is because count data has a finite resolution — 0 is always the smallest possible value, and 1 is always the second smallest value. Hence, unlike continuous values (which have “infinite resolution” in some sense), non-negative integers “lose” information the smaller the range is.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#footnotes",
    "href": "chapter2_sequencing.html#footnotes",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "You can have a simple hierarchical model in your head – each human donor contributes one (or more) tissue sample. Each tissue sample contains many cells.↩︎\nHow many genes are there in the human body? This is not a well-defined questions. If you want to only ask about protein-coding genes, there’s probably about 24,000 of them (salzberg2018open?). However, most gene callers (such as CellRanger https://www.10xgenomics.com/support/software/cell-ranger/latest, which is what we usually use for 10x single-cell data) also label genes that don’t translate. We’ll talk about this in ?sec-beyond_scrna.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html",
    "href": "chapter3_rna.html",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "4.1 Some important nouns and verbs",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#footnotes",
    "href": "chapter3_rna.html#footnotes",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "Technically, genomics (as a field) is a broader categorization and encapsulates genetics. But usually when people say they work on “genomics,” they are colloquially implying they work on biology that is not genetics (otherwise, they usually say they’re a geneticist).↩︎\nThis term is not very often used anymore. It was an umbrella label popularized in the mid-2000s. We instead typically refer to a more specific category of technology. For example, most of the data in this scRNA-seq chapter are referred to as “short-read 3′ single-cell RNA-sequencing.” (You could throw in “droplet-based” and “UMI” to be even more precise, but that’s usually not needed in most casual contexts.)↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#some-important-nouns-and-verbs",
    "href": "chapter3_rna.html#some-important-nouns-and-verbs",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "Sequencing (verb): The process of determining the order of nucleotides (A, T, C, G) in a DNA or RNA molecule, providing the primary structure of these biomolecules. Sequencing technologies have evolved to be high-throughput, enabling the analysis of entire genomes or transcriptomes.\nSequencing depth, read depth, library size (noun; all synonyms): These terms refer to the number of times a specific nucleotide or region of the genome is sequenced in an experiment. Greater depth provides more accurate detection of rare variants or lowly expressed genes but requires increased computational and financial resources.\nChromosome, DNA, RNA, protein (noun):\n\nChromosome: A large, organized structure of DNA and associated proteins that contains many genes and regulatory elements.\n\nDNA: The molecule that encodes genetic information in a double-helical structure.\n\nRNA: A single-stranded molecule transcribed from DNA that can act as a messenger (mRNA), a structural component (rRNA), or a regulator (e.g., miRNA). When we talk about scRNA-seq, we are usually referring to exclusively measuring mRNA.\n\nProtein: The functional biomolecule synthesized from RNA via translation, performing structural, enzymatic, and regulatory roles in cells.\n\nGenome vs. gene vs. intergenic region (noun):\n\nGenome: The complete set of DNA in an organism, encompassing all of its genetic material, including coding genes, non-coding regions, and regulatory elements. The genome is the blueprint that defines the biological potential of the organism.\n\nGene: A specific sequence within the genome that encodes a functional product, typically a protein or functional RNA. Genes include regions such as exons (coding sequences), introns (non-coding regions within a gene), and regulatory sequences (e.g., promoters and enhancers) that control gene expression. We will see more about the architecture of a gene in ?sec-genetics_basics.\n\nIntergenic region: The stretches of DNA between genes that do not directly code for proteins or RNA. Intergenic regions were once considered “junk DNA,” but they often contain regulatory elements, such as enhancers and silencers, that influence the expression of nearby or distant genes. These regions also play roles in chromatin organization and genome stability.\n\nGenetics vs. genomics (noun): Genetics typically focuses on the role of the DNA among large populations (of people, of species, etc.), while genomics can encapsulate any omic, and does not necessarily imply studies across a large population1.\n“Next generation sequencing” (noun)2: A collection of high-throughput technologies that allow for the parallel sequencing of millions of DNA or RNA molecules. It has revolutionized biology by enabling large-scale studies of genomes, transcriptomes, and epigenomes.\nRead fragment (noun): A short sequence of DNA or RNA produced as an output from high-throughput sequencing. Fragments are typically between 50 bp and 300 bp long, depending on the sequencing technology, and they represent segments of the original molecule being sequenced.\nReference genome (noun): A curated, complete assembly of the genomic sequence for a species, used as a template to align and interpret sequencing reads. It serves as a baseline for identifying genetic variations, such as mutations or structural changes, and for annotating functional elements.\nCoding genes vs. non-coding genes (noun):\n\nCoding genes: Genes that contain instructions for producing proteins. They are transcribed into mRNA, which is then translated into functional proteins that perform structural, enzymatic, or regulatory roles in cells.\n\nNon-coding genes: Genes that do not produce proteins but instead generate functional RNA molecules, such as rRNA, tRNA, miRNA, or lncRNA, which regulate gene expression, maintain genomic stability, or perform other cellular functions. Non-coding genes highlight the complexity of gene regulation and cellular processes beyond protein synthesis.\n\nEpigenetics vs. epigenomics (noun): Epigenetics studies modifications to DNA and histones (e.g., methylation, acetylation) that regulate gene expression without altering the DNA sequence. Epigenomics examines these modifications across the entire genome.\nTranscriptome (noun): The complete set of RNA transcripts expressed in a cell or tissue at a given time, reflecting dynamic gene activity.\nProteome (noun): The full complement of proteins expressed in a cell, tissue, or organism, representing functional output.\nSingle-cell sequencing (noun): Sequencing technologies applied at the resolution of individual cells, allowing for the study of heterogeneity in gene expression, epigenetics, or genetic variation across cell populations.\nBulk sequencing (noun): Sequencing technologies that aggregate material (e.g., RNA, DNA) from many cells, providing an average profile of the population but masking individual cell variability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html",
    "href": "chapter4_protein.html",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "5.1 Review of the central dogma\nThe central dogma of molecular biology outlines the flow of genetic information within a cell: DNA is transcribed into RNA, and RNA is then translated into protein (see Figure 5.1). Specifically, coding genes in DNA are transcribed into messenger RNA (mRNA), which serves as a template for protein synthesis. During translation, mRNA sequences are read in sets of three nucleotides, known as codons, each corresponding to a specific amino acid. These amino acids are then linked together to form proteins, which carry out a vast array of functions within the cell. While this process provides a foundational framework, it is a dramatic over-simplification. As we’ll explore later in the course, the correlation between a gene and its corresponding protein levels is often surprisingly low, highlighting the complexity of gene expression regulation.\nUnderstanding proteins is crucial because they are the primary effectors of cellular function. Most cellular activities — whether structural, enzymatic, or signaling—are mediated by proteins. While RNA intermediates, such as mRNA, play important roles in carrying genetic information, the majority of RNA fragments never leave the cell, with a few exceptions like extracellular RNA in communication. Proteins, however, directly influence both intracellular processes and extracellular interactions. Given the weak correlation between genes and proteins and the central role proteins play in biological function, studying proteins arguably provides a more direct and meaningful insight into cellular and organismal behavior. This dual perspective on the central dogma will frame much of our exploration in this course.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#review-of-the-central-dogma",
    "href": "chapter4_protein.html#review-of-the-central-dogma",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "Figure 5.1: What I call the ``AP Biology textbook’’ figure. Screenshot taken from https://www.slideshare.net/slideshow/lecture-on-dna-to-proteins-the-central-dogma-of-molecular-biology/38811421.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: From (buccitelli2020mrnas?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: From (buccitelli2020mrnas?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#other-ways-to-study-proteins-that-were-not-going-to-discuss-here",
    "href": "chapter4_protein.html#other-ways-to-study-proteins-that-were-not-going-to-discuss-here",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.2 Other ways to study proteins that we’re not going to discuss here",
    "text": "5.2 Other ways to study proteins that we’re not going to discuss here\n\n5.2.1 So You Heard About AlphaFold…\nAlphaFold (see Figure 5.4) represents a revolutionary advancement in computational biology, designed to predict the three-dimensional structure of proteins from their amino acid sequences2. Historically, determining protein shapes required experimental techniques like X-ray crystallography, cryo-electron microscopy (EM) (see Figure 5.5), or nuclear magnetic resonance (NMR), which are resource-intensive and time-consuming. AlphaFold uses deep learning and structural biology insights to achieve high accuracy.\nHowever, significant challenges remain. There are still many open questions on how specific genetic modifications impact protein folding, how proteins dynamically change their conformation, or how they interact with other molecules such as DNA or other proteins. Additionally, ongoing developments in using large language models are showing promise in predicting not only shape but also potential functions directly from amino acid sequences.\n\n\n\n\n\n\n\nFigure 5.4: From (jumper2021highly?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: From https://myscope.training/CRYO_Introducing_Single_Particle_Analysis, as an example of what cryo-EM data “looks like,” just to give you a brief glimpse on how people study protein structure.\n\n\n\n\n\n\n5.2.2 Other Methods: Flow Cytometry, Spatial Proteomics, and FISH\nWhile AlphaFold focuses on protein structure, methods like flow cytometry and spatial proteomics explore proteins in their functional and cellular contexts. Flow cytometry, sometimes considered the “original” single-cell data method, measures the expression of surface and intracellular proteins across thousands of cells, providing rich insights into cellular heterogeneity. Spatial proteomics and techniques like fluorescence in situ hybridization (FISH) take this further by localizing proteins and RNA within tissue contexts, enabling researchers to map molecular interactions in their native environments. These approaches highlight the versatility of protein studies, from understanding their structure to dissecting their function and distribution in complex systems. While not the focus of this course, these methods are invaluable in expanding our understanding of proteins and their roles in biology.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#footnotes",
    "href": "chapter4_protein.html#footnotes",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "Proteins typically degrade much slower than mRNA fragments. See https://book.bionumbers.org/how-fast-do-rnas-and-proteins-degrade. For this reason, you might hypothesize that “cellular memory” is stored via proteins, not mRNA.↩︎\nSee https://www.youtube.com/watch?v=P_fHJIYENdI for a fun YouTube video for more about this.↩︎\nEach feature in the protein modality in CITE-seq data is often called ADT. It might sometimes be called a protein marker as well.↩︎\nNotice there is no “library size” for protein.↩︎\nEach feature in the protein modality in CITE-seq data is often called ADT. It might sometimes be called a protein marker as well.↩︎\nNotice there is no “library size” for protein.↩︎\nTechnically, \\(z_n\\) is a Logistic Normal, which means it’s a distribution over the simplex. See more details in https://docs.scvi-tools.org/en/latest/user_guide/models/totalvi.html.↩︎\nIt’s not too important to understand what this means, but you can see (kingma2019introduction?) for a formal definition.↩︎\nSome details about this “complicated math”: The MAP is generally intractable to compute due to a very annoying normalization constant. So instead of computing MAP directly, we maximize the ELBO, which is a lower-bound of the likelihood. This is really the same trick used in deriving the EM algorithm. See https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29 or (lotfollahi2019scgen?) for a derivation of the ELBO, and (kingma2013auto?) for the formal objective function.↩︎\nEach feature in the protein modality in CITE-seq data is often called ADT. It might sometimes be called a protein marker as well.↩︎\nNotice there is no “library size” for protein.↩︎\nTechnically, \\(z_n\\) is a Logistic Normal, which means it’s a distribution over the simplex. See more details in https://docs.scvi-tools.org/en/latest/user_guide/models/totalvi.html.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html",
    "href": "chapter5_epigenetics.html",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "6.1 Primer on the genome, epigenetics, and enhancers\nThe genome is the complete set of DNA within an organism, encoding the instructions for life. While every cell in an organism typically contains the same genome, different cells exhibit distinct phenotypes and functions. This diversity arises not from changes in the underlying DNA sequence but from epigenetic regulation—heritable modifications that influence gene expression without altering the DNA itself. Epigenetics includes processes like DNA methylation, histone modification, and chromatin accessibility, all of which contribute to the dynamic regulation of gene activity in response to developmental cues and environmental signals.\nEnhancers play a critical role in this regulatory landscape. These are DNA sequences that, while not coding for proteins themselves, can dramatically increase the transcription of target genes. Enhancers act by binding specific transcription factors, proteins that recognize and attach to DNA sequences to regulate gene expression. Some transcription factors require assistance from chaperone proteins, which ensure their proper folding and functionality, or pioneer proteins, which can access and open tightly packed chromatin to allow other factors to bind. This interplay highlights the complexity of the regulatory machinery that governs cellular function. See Figure 6.1 and Figure 6.2 to appreciate how complex this machinery is.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#primer-on-the-genome-epigenetics-and-enhancers",
    "href": "chapter5_epigenetics.html#primer-on-the-genome-epigenetics-and-enhancers",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "Figure 6.1: From (ito2022factors?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: From (claringbould2021enhancers?).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response",
    "href": "chapter4_protein.html#just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.3 Just a very brief blurb about the passive and adaptive immune response",
    "text": "5.3 Just a very brief blurb about the passive and adaptive immune response\nThe immune system functions as a highly coordinated defense network, designed to detect foreign invaders such as pathogens and viruses, mount an appropriate response, and retain a memory of the invader for faster recognition in the future. This response involves two primary arms: the innate (passive) immune response, which provides a rapid but non-specific reaction, and the adaptive immune response, which is slower but highly specific and long-lasting (Figure 5.6). The innate immune system acts as the first line of defense, utilizing macrophages and dendritic cells to patrol tissues, engulf pathogens, and present antigens to activate the adaptive response. These antigen-presenting cells serve as surveillance officers, alerting the adaptive immune system to potential threats.\n\n\n\n\n\n\n\nFigure 5.6: From (primorac2022adaptive?).\n\n\n\n\nOnce the adaptive immune system is activated, a complex cellular expansion process takes place. T-cells and B-cells, the key players of adaptive immunity, undergo rapid division to generate an army of pathogen-specific defenders (Figure 5.7; Figure 5.8). B-cells produce antibodies that neutralize the invader, while T-cells can either directly kill infected cells or help orchestrate the immune response. Once the threat is eliminated, the expanded immune cell population undergoes programmed cell death to prevent excessive immune activity, leaving behind a small number of memory cells that provide long-term immunity. This cycle of surveillance, expansion, and contraction resembles a law enforcement system that transitions from routine surveillance (innate immunity) to full military mobilization (adaptive immunity) when an invasion occurs, ensuring both immediate and long-term protection.\n\n\n\n\n\n\n\nFigure 5.7: (Top): From https://www.youtube.com/watch?v=Kd-cTfVYwf8.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: (Bottom): From (joseph2022trained?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#sec-cite-seq_tech",
    "href": "chapter4_protein.html#sec-cite-seq_tech",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.4 CITE-seq: Sequencing mRNA alongside cell surface markers, simulteaneously",
    "text": "5.4 CITE-seq: Sequencing mRNA alongside cell surface markers, simulteaneously\nCITE-seq (Cellular Indexing of Transcriptomes and Epitopes by sequencing) (stoeckius2017simultaneous?) extends the capabilities of single-cell RNA-seq by simultaneously measuring messenger RNA (mRNA) and surface protein expression in the same cell. This is achieved by combining traditional single-cell RNA-seq protocols with oligonucleotide-labeled antibodies that bind to specific surface proteins. These oligonucleotides are then sequenced alongside the mRNA, providing a comprehensive view of both transcriptional and protein-level information for each cell.3 See Figure 5.9 for a comparison between the cell populations that are separable with flow cytometry verses the surface proteins in a CITE-seq dataset.\n\n\n\n\n\n\n\nFigure 5.9: From (stoeckius2017simultaneous?).\n\n\n\n\nFirst, some vocabulary: - Antibody and its epitope: An antibody is a Y-shaped protein produced by the immune system that binds specifically to an antigen, which is often a protein or peptide. The part of the antigen that the antibody recognizes and binds to is called the epitope. This interaction is highly specific, allowing antibodies to target unique molecular features of cells, pathogens, or other biological molecules. See (Figure 5.10; Figure 5.11; Figure 5.12).4\n\n\n\n\n\n\n\nFigure 5.10: (Top) From https://sciencenotes.org/antigen-definition-function-and-types.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.11: (Bottom Left) From https://www.sigmaaldrich.com/US/en/technical-documents/technical-article/protein-biology/elisa/antigens-epitopes-antibodies.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.12: (Bottom Right) From (sanchez2017fundamentals?).\n\n\n\n\n\nSurface markers (CDs): Cell surface markers, often referred to as CD (cluster of differentiation) markers, are proteins expressed on the surface of cells that serve as distinguishing features of specific cell types or states. These markers are commonly used in immunology and single-cell analyses to classify and study immune cells. For example, CD4 and CD8 are well-known markers for helper T cells and cytotoxic T cells, respectively. The ability to target CD markers with labeled antibodies is central to techniques like flow cytometry and CITE-seq. See Figure 5.13 for a table of common CDs used to identify immune cells.\n\n\n\n\n\n\n\n\nFigure 5.13: From https://www.cellsignal.com/pathways/immune-cell-markers-human.\n\n\n\n\nHere is a brief description of the CITE-seq steps (see Figure 5.14):\n\nTissue dissociation and cell isolation: This step remains identical to traditional single-cell RNA-seq. The tissue of interest is dissociated into a suspension of single cells, and cells are isolated via techniques like fluorescence-activated cell sorting (FACS) or microfluidics. The primary difference comes afterward, when specific oligonucleotide-conjugated antibodies are added to the cell suspension.\nAntibody labeling: In this unique step, oligonucleotide-labeled antibodies are used to bind to surface proteins on each cell. Each antibody is tagged with a DNA barcode unique to the specific protein it targets. These DNA barcodes are critical for linking sequencing data back to the corresponding surface proteins.\nDroplet formation: Similar to RNA-seq, cells are encapsulated into droplets along with barcoded beads. These beads capture both the mRNA and the antibody-bound oligonucleotides. This simultaneous capture is what makes CITE-seq distinct, as it preserves information about both RNA transcripts and protein markers.\nCell lysis and RNA/antibody capture: Cells are lysed within droplets, releasing both RNA and antibody-conjugated oligonucleotides. The RNA hybridizes to barcoded beads as in traditional RNA-seq. Simultaneously, the antibody-derived oligonucleotides are captured using complementary sequences on the same beads.\nReverse transcription, cDNA synthesis, and amplification: The RNA is reverse-transcribed into complementary DNA (cDNA), while the antibody-derived oligonucleotides are also amplified. This ensures sufficient material for sequencing both modalities.\nLibrary preparation and sequencing: Separate libraries are prepared for the RNA and antibody-derived oligonucleotides, but they are sequenced together on the same high-throughput platform. This co-sequencing ensures that both mRNA and protein data are linked to their respective cells via barcodes.\nMapping, alignment, and data integration: RNA reads are mapped to a reference genome or transcriptome as in traditional single-cell RNA-seq. Antibody-derived oligonucleotide reads, however, are mapped to predefined sequences representing the antibody barcodes. The two data modalities are then integrated, enabling joint analyses of mRNA and protein expression.\n\n\n\n\n\n\n\n\nFigure 5.14: From https://cite-seq.com/.\n\n\n\n\n\nRemark 5.1. Why surface markers matter\nSurface proteins often provide critical insights into a cell’s phenotype, function, and state that cannot always be inferred from mRNA expression alone. By combining transcriptomics with surface proteomics, CITE-seq enables deeper biological interpretation, such as identifying cell types or states that are transcriptionally similar but phenotypically distinct.\nCITE-seq is often performed on immune cells because of their abundance of surface markers, which play a critical role in their biological functions. Immune cells, such as T cells, B cells, natural killer cells, and monocytes, are defined by their expression of distinct surface proteins. These markers are essential for their roles in immune response, cell-cell communication, and recognition of pathogens or damaged cells.\n\n\n5.4.0.1 A brief note on other technologies\nSee (gong2025benchmark?) for a comparison between different technologies that measure single-cell transcriptomics and proteomics. See (lischetti2023dynamic?) for some notes about how CITE-seq has been used, or (as examples) its applications in leukemia (granja2019single?) or CAR T therapy (good2022post?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#a-primer-on-vaes",
    "href": "chapter4_protein.html#a-primer-on-vaes",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.5 A Primer on VAEs",
    "text": "5.5 A Primer on VAEs\nVariational autoencoders (VAE) is a type of deep-learning architecture that is exceptionally common for studying single-cell sequencing data. We’ll build up to this and its application of CITE-seq data slowly, piece-by-piece.\n\n5.5.1 What is an autoencoder?\n\nInput/Output. The input to an autoencoder is the same as any dimension reduction method (like PCA). The input is a matrix \\(X \\in \\mathbb{R}^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features. The output is \\(Z \\in \\mathbb{R}^{n \\times k}\\), where \\(k \\ll p\\) (where there are \\(k\\) latent dimensions). We call \\(Z\\) the embedding. Autoencoders also return the “denoised” matrix \\(X' \\in \\mathbb{R}^{n\\times p}\\).\n\nFigure 5.15 and Figure 5.16 illustrate the general structure of an autoencoder. It consists of two key components: an encoder, which maps the input data \\(x\\) into a latent space representation \\(z\\), and a decoder, which reconstructs the input \\(x\\) from the latent variables \\(z\\).\n\n\n\n\n\n\n\nFigure 5.15: (Left): From https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.16: (Right): From DCA (eraslan2019single?). (Note, DCA (eraslan2019single?) is not a variational autoencoder, since it is trained by simply minimizing the log-likelihood.)\n\n\n\n\nSome notable aspects: - Linear and non-linear functions: Typically, most of these neural network architectures for AEs are in the following form: for example, let the input for one layer be \\(X \\in \\mathbb{R}^{p}\\), and the AE learns a weight matrix \\(W \\in \\mathbb{R}^{p \\times K}\\). Then the output of that layer (which will become the input for the next layer) is \\(\\text{ReLU}(XW)\\), where ReLU is the Rectified Linear Unit.\n\nMinimizing a “deterministic” loss: As illustrated in Figure 5.15 and Figure 5.16, most AEs typically try to minimize the reconstruction loss (i.e., the difference between the original input and the final output). We’ll see how this changes when we discuss VAEs.\nCompression of information: As opposed to most NN architectures, in AEs, the number of units in each layer is smaller than the original dimension (typically called “compression” or “bottleneck”), exemplified in Figure 5.15 and Figure 5.16. In most other NN architectures (e.g., GANs, Convolution NN, Recurrent NN’s), the number of units typically grows. The latent representation for a cell with expression \\(X_i \\in \\mathbb{R}^{p}\\) will be called \\(Z_i \\in \\mathbb{R}^{K}\\) where \\(K \\ll p\\).\nUnsupervised setting: As opposed to most NN architectures, AEs are meant for the unsupervised setting. Unsurprisingly, since most of the statistical theory for NNs is for the supervised setting, our understanding of AEs is much more limited compared to that for other NN architectures.\nRelation to PCA: AEs are most similar to PCAs by understanding the reconstruction perspective of PCA ?eq-pca-minimize_reconstruction. PCA uses: 1) linear functions to encode and decode the data, and 2) sets the decoder to be exactly the “inverse” of the encoder. In AEs, we can: 1) use non-linear functions (such as ReLU) for more complex transformations of the data, and 2) allow the decoder to be different from the encoder. (The reason why the second point might seem useful will become clear when we explain VAEs below.)\n\n\n\n5.5.2 What is a variational autoencoder?\n\nInput/Output. The input and output of a VAE is the same as those of an autoencoder.\n\nHere are some motivating questions:\n\nWhy do we even need to specialize the autoencoder?\nConceptually, we could hope that an AE can serve as a means to generate new data — simply mix up the learned latent representation \\(Z_i\\)’s among all the \\(n\\) cells and use the decoder network to simulate new data. However, due to the highly non-linear nature of NNs, this might result in unexpectedly non-sensible values5. See Figure 5.17 and Figure 5.18.\n\n\n\n\n\n\n\n\nFigure 5.17: (Top): Ideal way to generate data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.18: (Bottom): Cartoonized illustration to demonstrate why this idealized strategy might not work. Both figures from https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73.\n\n\n\n\n\nHow does this impact our conceptual understanding of VAEs?\nHence, this is where we would like to adjust the NN architecture slightly — we want to insert randomness right in the middle of the AE to slightly perturb \\(Z\\), and we would like the cell’s original expression be similar (“on average”) to its reconstructed expression when its latent representation is slightly perturbed right before it’s decoded. The result is a framework illustrated in Figure 5.19.\n\n\n\n\n\n\n\n\nFigure 5.19: From https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73.\n\n\n\n\nNote: The formal way to describe how to set up this framework involves priors, posteriors, etc. We’ll avoid all of this for simplicity, but you can see (kingma2019introduction?) for a formal description.\n\nWhat does the architecture for a VAE look like?\nA brief quote of what a prototypical VAE is, from (battey2021visualizing?):\n\n\n\n“VAEs consist of a pair of deep neural networks in which the first network (the encoder) encodes input data as a probability distribution in a latent space and the second (the decoder) seeks to recreate the input given a set of latent coordinates (Kingma and Welling, 2013). Thus a VAE has as its target the input data itself.”\n\n\nFigure 5.20 and Figure 5.21 illustrate the typical VAE. We see there are now more components — the encoder network “splits” into two types of latent representations, one for the mean of the latent distribution and one for the variance. We then (during the training process) artificially inject Gaussian noise, which “perturbs” our latent representation \\(Z\\), which is then decoded to get our “denoised” cell’s expression.\n\n\n\n\n\n\n\nFigure 5.20: (Left): From (battey2021visualizing?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.21: (Right): From https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73.\n\n\n\n\nAt first, it might seem odd that we’re artificially injecting noise into our training process, as we might think this is harmful to our resulting denoising process6. There are two ways to think about this:\n\nMaking the training “more robust”: Despite the decoding network being so highly non-linear, we are trying to ensure that the latent representation still gives a good reconstruction despite minor perturbations7.\nGiving us additional control over the latent representation: We’re not getting into the details about the prior or posterior, but essentially, we’re going to regularize the latent distribution to “look like” a standard multivariate Gaussian (i.e., the prior). This ensures our latent representation: 1) doesn’t go wild and look extremely odd (i.e., the typical “overfitting” concern) and 2) it only wouldn’t still look like a standard multivariate Gaussian afterwards (i.e., the posterior) if there’s “enough evidence to deviate from the prior.”\nWhat does the objective function look like?\nAs with any NN architecture, the objective function is immensely important since this is how we “guide” the optimization procedure to prioritize “searching in certain spaces.”\n\nThe “variational” part of VAEs comes from the fact that the objective function is derived via a variational strategy8. This allows us to approximate the posterior distribution \\(p(Z|X)\\). After some complicated math of deriving what parameters “maximize the posterior distribution,” we can derive that the objective function amounts to maximizing something that loosely looks like the following (fixing the prior to typically be a standard multivariate Gaussian)9, for some tuning parameter \\(\\beta \\geq 0\\):\n\\[\n\\begin{aligned}\n\\max_{\\text{encoder}, \\text{decoder}} \\mathcal{L}(\\text{encoder}, \\text{decoder}; X_{1:n})\n&= \\sum_{i=1}^{n}\n   \\underbrace{\\mathbb{E}_{q_{\\text{encoder}}(Z_i \\mid X_i)}\n   \\Big[\\log p_{\\text{decoder}}(X_i \\mid Z_i)\\Big]}_{\\text{reconstruction error}} \\\\\n&\\quad - \\underbrace{\\beta \\cdot \\text{KL}\\big(q_{\\text{encoder}}(Z_i \\mid X_i)\n   \\,\\|\\, p_{\\text{prior}}(Z_i)\\big)}_{\\text{Deviation from prior}}\n\\end{aligned}\n\\tag{5.1}\\]\nor equivalently as a minimization (which is more commonly written, since we often call this the loss function),\n\\[\n\\begin{aligned}\n\\min_{\\text{encoder}, \\text{decoder}} \\mathcal{L}(\\text{encoder}, \\text{decoder}; X_{1:n})\n&= \\sum_{i=1}^{n} -\\mathbb{E}_{q_{\\text{encoder}}(Z_i|X_i)}\n    \\Big[\\log p_{\\text{decoder}}(X_i \\mid Z_i)\\Big] \\\\\n&\\quad + \\beta \\cdot \\text{KL}\\big(q_{\\text{encoder}}(Z_i \\mid X_i) \\,\\|\\, p_{\\text{prior}}(Z_i)\\big)\n\\end{aligned}\n\\]\nA brief quote from (battey2021visualizing?):\n\n\n“The loss function for a VAE is the sum of reconstruction error (how different the generated data is from the input) and Kullback-Leibler (KL) divergence between a sample’s distribution in latent space and a reference distribution which acts as a prior on the latent space (here we use a standard multivariate normal, but see (Davidson et al., 2018) for an alternative design with a hyperspherical latent space). The KL term of the loss function incentivizes the encoder to generate latent distributions with meaningful distances among samples, while the reconstruction error term helps to achieve good local clustering and data generation.”\n\n\nAdditional notes: - What are these terms?\nHere, “encoder” and “decoder” refer to the parameters we need to learn for the encoding or decoding NN. Then:\n1. \\(q_{\\text{encoder}}(Z_i|X_i)\\) refers to the variational distribution that maps \\(X_i\\) to \\(Z_i\\) which approximates the posterior \\(p(Z_i|X_i)\\). Thinking back to Figure 5.20 and Figure 5.21, we can think of this as, “based on our encoder (which maps \\(X_i\\) to a multivariate Gaussian’s mean and covariance matrix), what is the probability of observing the latent representation \\(Z_i\\)?”\n2. \\(p_{\\text{prior}}(Z_i)\\) is the prior that is typically set to a multivariate Gaussian, i.e., \\(Z_i \\sim N(0, I_{K})\\), which is typically not altered.\n3. \\(p_{\\text{decoder}}(X_i|Z_i)\\) refers to the conditional likelihood, which we can think of as, “based on the latent representation \\(Z_i\\), what is the probability of observing the expression vector \\(X_i\\)?”\n\nKL term: This term acts as a regularizer. It measures the KL divergence between the variational distribution (i.e., the approximation of the posterior) and the prior. As mentioned in (kingma2013auto?), we can typically compute this term in closed form if \\(q_{\\text{encoder}}(Z_i|X_i)\\) and \\(p_{\\text{prior}}(Z_i)\\) are both Gaussians.\nReconstruction error term: This is the typical reconstruction error. If we set up our NN architecture so that \\(p_{\\text{decoder}}(X_i|Z_i)\\) is a Gaussian, then this is proportional to some Euclidean distance. As mentioned in Appendix C.2 of (kingma2013auto?), this decoder typically splits into two (similar to the encoder illustrated in Figure 5.20 and Figure 5.21) since we need to map \\(Z_i\\) into a mean and variance of a \\(p\\)-dimensional Gaussian distribution.\nThis expectation is taken with respect to the variational distribution \\(q_{\\text{encoder}}(Z_i|X_i)\\), so we typically need to sample \\(Z_i\\)’s in order to approximate this expectation in practice.\nTuning hyperparameter: The \\(\\beta \\geq 0\\) hyperparameter was an addition to yield the \\(\\beta\\)-VAE (higgins2017beta?). This extra tuning parameter is commonly used to allow the user to determine how “much” to regularize the objective function. The larger the \\(\\beta\\), the more emphasis there is on learning a simple encoder (striving for less overfitting, at the cost of worse reconstruction).\nGive me a concrete example!\nUnfortunately, Equation 5.1 is written in generic math notation. In other words, it doesn’t really tell you the explicit formula on how to compute the objective in practice. If you want to see the full derivation of the objective for specifically: 1) a standard multivariate Gaussian prior for \\(p_{\\text{prior}}(Z_i)\\), and 2) a Gaussian data generation mechanism for \\(p_{\\text{decoder}}(X_i|Z_i)\\), then see (odaibo2019tutorial?). (Setting the prior and data-generation mechanism will determine the form of the log-likelihood in \\(\\log p_{\\text{decoder}}(X_i|Z_i)\\) and the form of the posterior distribution \\(q_{\\text{encoder}}(Z_i|X_i)\\).)\n\nHere are some extra terminology typically used in deep-learning: - Batches: Small subsets of cells used to calculate gradients during training, allowing for scalable optimization. One of the biggest innovations from deep-learning is that it turns out that 1) stochastic gradient descent is pretty good at solving difficult non-convex problems, and 2) stochastic gradient descent out-of-the-box can do this pretty well (with the help of automated differentiation techniques) so that you, the data analyst, don’t need to derive anything too complicated outside of the deep-learning loss and architecture. See Adam, the foundation of modern stochastic gradient descent (kingma2014adam?). The “batch” refers to the random subset of cells used to update the gradient in each step.\n\nEpochs: Full passes through the entire dataset during training.\n\nThe modular nature of VAEs allows for modifications, such as changing the structure of the encoder/decoder or altering the prior distribution, making them highly adaptable to different data types and tasks, including the integration of multimodal single-cell data like CITE-seq, as we will see soon in Section 5.6.1.\nscVI (single-cell Variational Inference) is a specialized implementation of a Variational Autoencoder (VAE) tailored to single-cell RNA-seq data. It models the observed count data as being drawn from a negative binomial (NB) distribution, allowing it to account for the overdispersion inherent in single-cell data. The framework is designed to simultaneously capture the low-dimensional latent structure of the data and model the technical variability associated with sequencing, such as differences in library sizes.\nLike a generic VAE, scVI consists of an encoder and a decoder. The encoder maps the observed gene expression counts ( X ) into a latent space ( Z ), capturing the underlying biological variation. The decoder reconstructs the counts ( X ) from ( Z ), but in scVI, this reconstruction explicitly models count data using the negative binomial distribution: [ X_{ij} ({ij}, r_j), ] where ( {ij} ) is the mean expression for gene ( j ) in cell ( i ), and ( r_j ) is the dispersion parameter for cell ( i ).\nThe latent space ( Z ) represents a compressed, biologically meaningful representation of each cell, and it is regularized to follow a standard Gaussian prior ( p(Z) = (0, I) ). The overall loss function to be minimized is the evidence lower bound (ELBO): \\[\n\\begin{align*}\n\\mathcal{L}(\\text{encoder}, \\text{decoder}; X_{1:n}) &=\n\\sum_{i=1}^{n}\\bigg[-\\mathbb{E}_{q_{\\text{encoders}}(Z_i, \\ell_i \\mid X_i)} \\Big[ \\log p_{\\text{decoder}}(X_i \\mid Z_i, \\ell_i) \\Big] \\\\\n&\\quad + \\text{KL}\\Big(q_{\\text{encoder1}}(Z_i \\mid X_i) \\, \\| \\, p_{\\text{prior1}}(Z_i)\\Big) \\\\\n&\\quad + \\text{KL}\\Big(q_{\\text{encoder2}}(\\ell_i \\mid X_i) \\, \\| \\, p_{\\text{prior2}}(\\ell_i)\\Big)\\bigg],\n\\end{align*}\n\\]\nwhere ( _i ) represents the library size (total counts per cell).\n\n\n5.5.3 Architecture for library size and overdispersion\nThe architecture consists of:\n\nEncoders: The input is the raw count \\(X_i\\). There are technically two encoders — one encodes the latent space \\(Z_i\\) (which represents the biological variation in cells) and the other, the library size \\(\\ell_i\\). The first encoder, \\(q_{\\text{encoder1}}(Z_i \\mid X_i)\\), learns a multivariate Gaussian with a diagonal covariance matrix, and the second encoder, \\(q_{\\text{encoder2}}(\\ell_i \\mid X_i)\\), is a log-normal distribution.\nDecoder: The inputs are the latent variables \\(Z_i\\) and library size \\(\\ell_i\\). The decoder predicts the mean \\(\\mu_{ij}\\) for each gene \\(j\\) based on \\(Z_i\\), and \\(\\mu_{ij}\\) alongside \\(\\ell_i\\) are used to compute the objective function. (The dispersion parameter \\(r_j\\) is learned via variational Bayesian inference — see the paper.)\n\nThe so-called “plate diagram” in Figure 5.22 gives a good schematic summary of this architecture (but I’ve personally found that these diagrams are more of a “useful reminder” — they’re a bit hard to learn from directly).\n\n\n\n\n\n\n\nFigure 5.22: A concise caption for the plate diagram.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#multi-modal-integration-applicable-beyond-cite-seq",
    "href": "chapter4_protein.html#multi-modal-integration-applicable-beyond-cite-seq",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.6 Multi-modal integration (applicable beyond CITE-seq)",
    "text": "5.6 Multi-modal integration (applicable beyond CITE-seq)\nComing back to CITE-seq data Section 5.4, suppose we have the following goal — we want to create a unified low-dimensional embedding (i.e., a “map” or “atlas” of sorts) that integrates the information from both the transcriptome and proteome. That is, there are some cell-type variations present only in the RNA or only in the ADT (Antibody-Derived Tags)10, and we want to create one unified embedding that combines all these information together. See Figure 5.23 and Figure 5.24 as an example.\n\n\n\n\n\n\n\nFigure 5.23: (Top) The UMAP of either just the genes (left) or just the proteins (right). From https://satijalab.org/seurat/articles/weighted_nearest_neighbor_analysis for a method called WNN (which isn’t TotalVI, but this figure is good at illustrating the point).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.24: (Bottom) The UMAP after a method (WNN in this case) integrates the RNA and ADT together. From https://satijalab.org/seurat/articles/weighted_nearest_neighbor_analysis.\n\n\n\n\nYour first thought might be to simply perform PCA on the entire single-cell sequencing (i.e., one big PCA on all the 2000+ highly variable genes, and 100-ish proteins). However, this might not work as you might hope, since this wouldn’t account for the different scales and sparsities of RNA and ADT signals. That is, the RNA-dominated gene expression data could overwhelm the smaller protein feature set. This addresses the core challenge in simply combining modalities — differences in dimensionality, normalization, and noise — so that a single embedding captures both transcriptomic and proteomic variation while preserving each modality’s unique biological signal.\nWe will mention this concept of integration between two omics more in ?sec-atac_integration when we talk about 10x multiome (RNA & ATAC). For now though, we will focus on TotalVI, which is one of the most common ways to achieve this goal for CITE-seq data.\n\n5.6.1 TotalVI (and other methods for the “union” of information)\n\nInput/Output. The input to TotalVI is a multi-modal sequencing matrix, where 1) the count matrix (for gene expression) \\(X \\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) genes, and 2) the count matrix (for protein abundance) \\(Y \\in \\mathbb{Z}_+^{n\\times q}\\) for \\(n\\) cells and \\(q\\) proteins. The output is: 1) \\(Z \\in \\mathbb{R}^{n \\times k}\\), where \\(k \\ll \\min\\{p,q\\}\\) (where there are \\(k\\) latent dimensions), and also 2) the normalized gene expression matrix \\(X' \\in \\mathbb{R}^{n\\times p}\\), and 3) the normalized protein abundance matrix \\(Y' \\in \\mathbb{R}^{n\\times q}\\).\n\nTotalVI (gayoso2021joint?) is a deep generative model specifically designed for the joint analysis of single-cell RNA and protein data obtained from techniques like CITE-seq. Unlike traditional single-cell RNA-seq methods that analyze transcriptomic data in isolation, TotalVI simultaneously incorporates transcriptomic and proteomic information to create a unified representation of cell states.\nTotalVI builds upon the VAE framework of scVI, adapting it to the multimodal nature of CITE-seq data. The RNA counts are modeled with a negative binomial distribution, as in scVI, to capture overdispersion in single-cell data. For proteins, TotalVI introduces a mixture model that separates background and foreground components. This enables accurate modeling of protein measurements, even in the presence of high background noise. TotalVI’s decoder uses neural networks to predict the parameters of both the RNA and protein likelihoods, while the encoder maps cells into a shared low-dimensional latent space that reflects their joint transcriptomic and proteomic profiles. See Figure 5.25 for an overview.\n\n\n\n\n\n\n\nFigure 5.25: From (gayoso2021joint?).\n\n\n\n\n\n5.6.1.1 The TotalVI model and key equations\nTotalVI models RNA and protein counts \\(x_{ng}\\) and \\(y_{nt}\\) for gene \\(g\\) and protein \\(t\\) in cell \\(n\\) as follows:\n\\[\nx_{ij} \\sim \\text{NB}(\\ell_i \\rho_{ij}, \\theta_g), \\quad \\text{and}\\quad y_{it} \\sim \\text{NB-Mixture}(\\text{Foreground} + \\text{Background}).\n\\]\nFirst, the RNA side: - \\(\\ell_n\\): RNA library size for cell \\(n\\). - \\(\\rho_{ng}\\): Normalized RNA expression frequency, predicted by the decoder network from the latent variables. - \\(\\theta_g\\): Gene-specific overdispersion parameter for gene \\(g\\) (estimated for each gene during training, much like in scVI).\nThe values in \\(\\rho_{ng}\\) are dictated by the learned latent embedding \\(z_n\\) using an encoder. A separate encoder will learn \\(\\ell_n\\).\nNext, the Protein side. The protein counts \\(y_{it}\\) are modeled with a negative binomial mixture:\n\\[\ny_{nt} \\sim v_{nt} \\,\\text{NB}(\\beta_{nt}, \\phi_t) + (1 - v_{nt}) \\,\\text{NB}(\\alpha_{nt} \\beta_{nt}, \\phi_t),\n\\]\nwhere: - \\(v_{nt}\\): Bernoulli variable indicating whether the count is from the background or foreground. - \\(\\beta_{nt}\\): Protein background mean, learned during training. - \\(\\alpha_{nt}\\): Scaling factor for foreground counts. - \\(\\phi_t\\): Protein-specific overdispersion parameter.\nThe values in \\(v_{nt}\\) and \\(\\alpha_{nt}\\) (but not \\(\\beta_{nt}\\)) are dictated by the latent embedding \\(z_n\\)11.\nThe latent embedding12 \\(z_n \\in \\mathbb{R}^k\\) dictates: 1. \\(\\rho_{ng}\\) (normalized gene expression), 2. \\(\\alpha_{nt}\\) (protein foreground counts), and 3. \\(\\phi_t\\) (probability of the protein displaying a foreground count).\n\n\n\n\n\n\n\nFigure 5.26: From (gayoso2021joint?).\n\n\n\n\nNow we understand the generative model (decoder). The encoder is composed of three separate encoders: 1. Encoding observed gene expression \\(x_n\\) and protein expression \\(y_n\\) to yield \\(z_n\\), 2. Encoding just \\(x_n\\) to yield \\(\\ell_n\\), and 3. Encoding just \\(y_n\\) to yield \\(\\beta_n\\).\nThe objective function is the evidence lower bound (ELBO) to be minimized:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\text{encoders}, \\text{decoders}; x_{\\text{all cells}}, y_{\\text{all cells}})\n&=\n\\sum_{n} \\bigg[-\\mathbb{E}_{q_{\\text{encoders}}(z_n, \\ell_n, \\beta_n | x_n, y_n)} \\log p_{\\text{decoders}}(x_n, y_n | z_n, \\ell_n, \\beta_n) \\\\\n&\\quad + \\text{KL}\\big(q_{\\text{encoder1}}(z_n | x_n, y_n) \\, \\| \\, p_{\\text{prior1}}(z_n)\\big) \\\\\n&\\quad + \\text{KL}\\big(q_{\\text{encoder2}}(\\ell_n | x_n) \\, \\| \\, p_{\\text{prior2}}(\\ell_n)\\big) \\\\\n&\\quad + \\text{KL}\\big(q_{\\text{encoder3}}(\\beta_n | y_n) \\, \\| \\, p_{\\text{prior3}}(\\beta_n)\\big) \\bigg].\n\\end{aligned}\n\\]\n\nRemark 5.2. Why did I call these “union” methods?\nFrom the generative model\n\\[\np_{\\text{decoders}}(x_n, y_n | z_n, \\ell_n, \\beta_n)\n\\]\nthe latent embedding \\(z_n\\) is used to generate both the RNA \\(x_n\\) and protein \\(y_n\\). Intuitively, \\(z_n\\) likely contains more information than either \\(x_n\\) or \\(y_n\\) alone. The goal of TotalVI is to create one big atlas that provides a bird’s-eye view of all the sources of variation in the data — some originating from RNA, others from protein.\n\nA brief note on other approaches.\nA popular non-deep-learning approach is Weighted Nearest Neighbors (WNN) (hao2020integrated?) (shown in Figure 5.23 and Figure 5.24). More generally, methods for joint embeddings of multimodal data are discussed in (wang2024progress?), (brombacher2022performance?), and (makrodimitris2024depth?). Deep learning dominates this field, with differences in how architectures and losses manage information flow. See Figure 5.27.\n\n\n\n\n\n\n\nFigure 5.27: From (makrodimitris2024depth?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  }
]