[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "scOmicNotes",
    "section": "",
    "text": "This is a Quarto book made from the content of BIOST 545 (Biostatistical Methods for Big Omics Data), taught by Dr. Kevin Lin at the University of Washington, Winter 2025. The lecture notes in BIOST 545 are meant to ease you into reading these notes, and the notes are meant to be a gateway to many references where the course’s goal is for you 1) to determine for yourself what areas you find interesting and are relevant to your research goals, and 2) use the references as a portal to find many ideas to bolster your own research. This Quarto book is assembled with the help of many student volunteers at the University of Washington: Turbo Du [TBD].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why study cell biology in public health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ahlmann-Eltze, Constantin, and Wolfgang Huber. 2023. “Comparison\nof Transformations for Single-Cell RNA-Seq Data.” Nature\nMethods, 1–8.\n\n\nArora, Sanjeev, Wei Hu, and Pravesh K Kothari. 2018. “An Analysis\nof the t-Sne Algorithm for Data Visualization.” In Conference\non Learning Theory, 1455–62. PMLR.\n\n\nCai, T Tony, and Rong Ma. 2022. “Theoretical Foundations of t-Sne\nfor Visualizing High-Dimensional Clustered Data.” Journal of\nMachine Learning Research 23 (301): 1–54.\n\n\nCarbonetto, Peter, Abhishek Sarkar, Zihao Wang, and Matthew Stephens.\n2021. “Non-Negative Matrix Factorization Algorithms Greatly\nImprove Topic Model Fits.” arXiv Preprint\narXiv:2105.13440.\n\n\nChari, Tara, and Lior Pachter. 2023. “The Specious Art of\nSingle-Cell Genomics.” PLOS Computational Biology 19\n(8): e1011288.\n\n\nChen, Wenan, Yan Li, John Easton, David Finkelstein, Gang Wu, and Xiang\nChen. 2018. “UMI-Count Modeling and Differential\nExpression Analysis for Single-Cell RNA Sequencing.”\nGenome Biology 19 (1): 70.\n\n\nClaringbould, Annique, and Judith B Zaugg. 2021. “Enhancers in\nDisease: Molecular Basis and Emerging Treatment Strategies.”\nTrends in Molecular Medicine 27 (11): 1060–73.\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes\nMethods and False Discovery Rates for Microarrays.” Genetic\nEpidemiology 23 (1): 70–86.\n\n\nGhojogh, Benyamin, Ali Ghodsi, Fakhri Karray, and Mark Crowley. 2021.\n“Factor Analysis, Probabilistic Principal Component Analysis,\nVariational Inference, and Variational Autoencoder: Tutorial and\nSurvey.” arXiv Preprint arXiv:2101.00734.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and\nKenneth J Livak. 2021. “Applying High-Dimensional Single-Cell\nTechnologies to the Analysis of Cancer Immunotherapy.” Nature\nReviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle\nGaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular\nClassification of Cancer: Class Discovery and Class Prediction by Gene\nExpression Monitoring.” Science 286 (5439): 531–37.\n\n\nHafemeister, Christoph, and Rahul Satija. 2019. “Normalization and\nVariance Stabilization of Single-Cell RNA-seq Data Using Regularized Negative Binomial\nRegression.” Genome Biology 20 (1): 1–15.\n\n\nHaghverdi, Laleh, Florian Buettner, and Fabian J Theis. 2015.\n“Diffusion Maps for High-Dimensional Single-Cell Analysis of\nDifferentiation Data.” Bioinformatics 31 (18): 2989–98.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023.\n“Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and\nTrials.” Nature Aging 3 (5): 506–19.\n\n\nHeumos, Lukas, Anna C Schaar, Christopher Lance, Anastasia Litinetskaya,\nFelix Drost, Luke Zappia, Malte D Lücken, et al. 2023. “Best\nPractices for Single-Cell Analysis Across Modalities.” Nature\nReviews Genetics 24 (8): 550–72.\n\n\nHou, Wenpin, Zhicheng Ji, Hongkai Ji, and Stephanie C Hicks. 2020.\n“A Systematic Evaluation of Single-Cell\nRNA-Sequencing Imputation Methods.” Genome\nBiology 21 (1): 1–30.\n\n\nHuang, Haiyang, Yingfan Wang, Cynthia Rudin, and Edward P Browne. 2022.\n“Towards a Comprehensive Evaluation of Dimension Reduction Methods\nfor Transcriptomic Data Visualization.” Communications\nBiology 5 (1): 719.\n\n\nIto, Shinsuke, Nando Dulal Das, Takashi Umehara, and Haruhiko Koseki.\n2022. “Factors and Mechanisms That Influence Chromatin-Mediated\nEnhancer–Promoter Interactions and Transcriptional Regulation.”\nCancers 14 (21): 5404.\n\n\nJohnson, Eric M, William Kath, and Madhav Mani. 2022.\n“EMBEDR: Distinguishing Signal from Noise in\nSingle-Cell Omics Data.” Patterns 3 (3).\n\n\nKent, JT, John Bibby, and KV Mardia. 1979. Multivariate\nAnalysis. Academic Press Amsterdam.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N\nShokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse\nProteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease\nBrain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nKharchenko, Peter V, Lev Silberstein, and David T Scadden. 2014.\n“Bayesian Approach to Single-Cell Differential Expression\nAnalysis.” Nature Methods 11 (7): 740.\n\n\nKorsunsky, Ilya, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang,\nKevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya\nRaychaudhuri. 2019. “Fast, Sensitive and Accurate Integration of\nSingle-Cell Data with Harmony.” Nature Methods, 1–8.\n\n\nLause, Jan, Philipp Berens, and Dmitry Kobak. 2021. “Analytic\nPearson Residuals for Normalization of Single-Cell RNA-Seq UMI\nData.” Genome Biology 22: 1–20.\n\n\n———. 2024. “The Art of Seeing the Elephant in the Room: 2D\nEmbeddings of Single-Cell Data Do Make Sense.” bioRxiv.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun\nYu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in\nCancer Research: Progress and Perspectives.” Journal of\nHematology & Oncology 14 (1): 91.\n\n\nLi, Lei, Yumei Li, Xudong Zou, Fuduan Peng, Ya Cui, Eric J Wagner, and\nWei Li. 2022. “Population-Scale Genetic Control of Alternative\nPolyadenylation and Its Association with Human Diseases.”\nQuantitative Biology 10 (1): 44–54.\n\n\nLin, Kevin Z, Yixuan Qiu, and Kathryn Roeder. 2022. “eSVD: Cohort-Level Differential Expression in\nMulti-Individual Single-Cell RNA-Seq Data Using\nExponential-Family Embeddings.” (In Preparation).\n\n\n———. 2024. “eSVD-DE: Cohort-Wide\nDifferential Expression in Single-Cell RNA-Seq Data Using\nExponential-Family Embeddings.” BMC Bioinformatics 25\n(1): 113.\n\n\nLopez, Romain, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir\nYosef. 2018. “Deep Generative Modeling for Single-Cell\nTranscriptomics.” Nature Methods 15 (12): 1053.\n\n\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014.\n“Moderated Estimation of Fold Change and Dispersion for\nRNA-Seq Data with DESeq2.” Genome\nBiology 15 (12): 550.\n\n\nLuecken, Malte D, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese,\nMarta Interlandi, Michaela F Müller, Daniel C Strobl, et al. 2022.\n“Benchmarking Atlas-Level Data Integration in Single-Cell\nGenomics.” Nature Methods 19 (1): 41–50.\n\n\nLuecken, Malte D, and Fabian J Theis. 2019. “Current Best\nPractices in Single-Cell RNA-Seq Analysis: A\nTutorial.” Molecular Systems Biology 15 (6): e8746.\n\n\nMa, Rong, Eric D Sun, David Donoho, and James Zou. 2024.\n“Principled and Interpretable Alignability Testing and Integration\nof Single-Cell Data.” Proceedings of the National Academy of\nSciences 121 (10): e2313719121.\n\n\nMaden, Sean K, Sang Ho Kwon, Louise A Huuki-Myers, Leonardo\nCollado-Torres, Stephanie C Hicks, and Kristen R Maynard. 2023.\n“Challenges and Opportunities to Computationally Deconvolve\nHeterogeneous Tissue with Varying Cell Sizes Using Single-Cell\nRNA-Sequencing Datasets.” Genome Biology 24 (1): 288.\n\n\nMcGinnis, Christopher S, Lyndsay M Murrow, and Zev J Gartner. 2019.\n“DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing\nData Using Artificial Nearest Neighbors.” Cell Systems 8\n(4): 329–37.\n\n\nPrater, Katherine E, and Kevin Z Lin. 2024. “All the Single Cells:\nSingle-Cell Transcriptomics/Epigenomics Experimental Design and Analysis\nConsiderations for Glial Biologists.” Glia.\n\n\nQian, Kun, Shiwei Fu, Hongwei Li, and Wei Vivian Li. 2022.\n“scINSIGHT for Interpreting Single-Cell Gene Expression from\nBiologically Heterogeneous Data.” Genome Biology 23 (1):\n1–23.\n\n\nRisso, Davide, John Ngai, Terence P Speed, and Sandrine Dudoit. 2014.\n“Normalization of RNA-Seq Data Using Factor Analysis\nof Control Genes or Samples.” Nature Biotechnology 32\n(9): 896–902.\n\n\nSalzberg, Steven L. 2018. “Open Questions: How Many Genes Do We\nHave?” BMC Biology 16 (1): 94.\n\n\nSarkar, Abhishek, and Matthew Stephens. 2021. “Separating\nMeasurement and Expression Models Clarifies Confusion in Single Cell\nRNA-Seq Analysis.” Nature Genetics 53 (6):\n770–77.\n\n\nSaunders, Lauren M, Sanjay R Srivatsan, Madeleine Duran, Michael W\nDorrity, Brent Ewing, Tor H Linbo, Jay Shendure, et al. 2023.\n“Embryo-Scale Reverse Genetics at Single-Cell Resolution.”\nNature 623 (7988): 782–91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde,\nand Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable\nResponses to Sequential and Prolonged Treatment.” Cell\nSystems 15 (3): 213–26.\n\n\nSetty, Manu, Vaidotas Kiseliovas, Jacob Levine, Adam Gayoso, Linas\nMazutis, and Dana Pe’er. 2019. “Characterization of Cell Fate\nProbabilities in Single-Cell Data with Palantir.”\nNature Biotechnology 37 (4): 451–60.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to\nTailor Treatments.” Science Translational Medicine 9\n(408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the\nMolecular and Cellular Mechanisms of Aging.” Journal of\nInvestigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year\n2019: Single-Cell Multimodal Omics.” Nature Methods 17\n(1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith\nKnight. 2005. “Sparsity and Smoothness via the Fused\nLasso.” Journal of the Royal Statistical Society Series B:\nStatistical Methodology 67 (1): 91–108.\n\n\nTipping, Michael E, and Christopher M Bishop. 1999. “Probabilistic\nPrincipal Component Analysis.” Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) 61 (3):\n611–22.\n\n\nTownes, F William, Stephanie C Hicks, Martin J Aryee, and Rafael A\nIrizarry. 2019. “Feature Selection and Dimension Reduction for\nSingle-Cell RNA-seq Based on a Multinomial\nModel.” Genome Biology 20 (1): 1–16.\n\n\nTran, Hoa Thi Nhu, Kok Siong Ang, Marion Chevrier, Xiaomeng Zhang,\nNicole Yee Shin Lee, Michelle Goh, and Jinmiao Chen. 2020. “A\nBenchmark of Batch-Effect Correction Methods for Single-Cell\nRNA Sequencing Data.” Genome Biology 21\n(1): 1–32.\n\n\nVan Dijk, David, Roshan Sharma, Juozas Nainys, Kristina Yim, Pooja\nKathail, Ambrose J Carr, Cassandra Burdziak, et al. 2018.\n“Recovering Gene Interactions from Single-Cell Data Using Data\nDiffusion.” Cell 174 (3): 716–29.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME\nBowdish. 2015. “An Introduction to Automated Flow Cytometry Gating\nTools and Their Implementation.” Frontiers in Immunology\n6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n.\n2015. “Microglial Cell Dysregulation in Brain Aging and\nNeurodegeneration.” Frontiers in Aging Neuroscience 7:\n124.\n\n\nWei, Lai, Derek Lee, Cheuk-Ting Law, Misty Shuo Zhang, Jialing Shen, Don\nWai-Ching Chin, Allen Zhang, et al. 2019. “Genome-Wide CRISPR/Cas9\nLibrary Screening Identified PHGDH as a Critical Driver for Sorafenib\nResistance in HCC.” Nature Communications 10 (1): 4681.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao\nZhu, et al. 2022. “Single-Cell Technologies: From Research to\nApplication.” The Innovation 3 (6).\n\n\nXi, Nan Miles, and Jingyi Jessica Li. 2021. “Benchmarking\nComputational Doublet-Detection Methods for Single-Cell RNA Sequencing\nData.” Cell Systems 12 (2): 176–94.\n\n\nXia, Lucy, Christy Lee, and Jingyi Jessica Li. 2024. “Statistical\nMethod scDEED for Detecting Dubious 2D Single-Cell Embeddings and\nOptimizing t-SNE and UMAP Hyperparameters.” Nature\nCommunications 15 (1): 1753.\n\n\nYazar, Seyhan, Jose Alquicira-Hernandez, Kristof Wing, Anne Senabouth, M\nGrace Gordon, Stacey Andersen, Qinyi Lu, et al. 2022. “Single-Cell\neQTL Mapping Identifies Cell Type-Specific\nGenetic Control of Autoimmune Disease.” Science 376\n(6589): eabf3041.\n\n\nYoung, Matthew D, and Sam Behjati. 2020. “SoupX Removes Ambient\nRNA Contamination from Droplet-Based Single-Cell RNA Sequencing\nData.” Gigascience 9 (12): giaa151.\n\n\nZhang, Yuqing, Giovanni Parmigiani, and W Evan Johnson. 2020.\n“ComBat-Seq: Batch Effect Adjustment for RNA-Seq Count\nData.” NAR Genomics and Bioinformatics 2 (3): lqaa078.\n\n\nZhao, Ruzhang, Jiuyao Lu, Weiqiang Zhou, Ni Zhao, and Hongkai Ji. 2024.\n“A Systematic Evaluation of Highly Variable Gene Selection Methods\nfor Single-Cell RNA-Sequencing.” bioRxiv, 2024–08.\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human\nBrain Development.” Nature Reviews Genetics 25 (1):\n26–45.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "intro.html#what-are-omics",
    "href": "intro.html#what-are-omics",
    "title": "2  Introduction",
    "section": "3.1 What are “omics”?",
    "text": "3.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n3.1.1 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 3.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 3.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 3.3.\n\n\n\n\n\n\n\n\nFigure 3.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#how-omics-comes-into-the-picture",
    "href": "intro.html#how-omics-comes-into-the-picture",
    "title": "2  Introduction",
    "section": "3.2 How “omics” comes into the picture",
    "text": "3.2 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 3.4 and Figure 3.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 3.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 3.7.\n\n\n\n\n\n\n\nFigure 3.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised—mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an ``average/typical’’ scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.\n\n\n\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "href": "intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "title": "2  Introduction",
    "section": "2.2 What do we hope to learn from single-cell data?",
    "text": "2.2 What do we hope to learn from single-cell data?\n\n2.2.1 Basic biology\nSingle-cell data offer a transformative lens to study the fundamental processes of life at unparalleled resolution. Unlike bulk data, which averages signals across populations of cells, single-cell technologies allow researchers to examine the diversity and complexity of individual cells within a tissue or organism. This level of detail provides insights into key biological phenomena, such as cellular differentiation during development, the plasticity of cell states in response to environmental cues, and the organization of complex tissues. For instance, single-cell RNA-sequencing (scRNA-seq) has uncovered new cell types in the brain and immune system, challenging traditional classifications and offering a more nuanced understanding of cellular identities and functions. These insights are essential for constructing more accurate models of how life operates at a cellular level.\nMany of the examples shown in Figs. Figure 2.1–Figure 2.2 are basic biology questions. Two additional examples are shown in Figure 2.8 and Figure 2.9.\n\n\n\n\n\n\n\nFigure 2.8: How do cancer cells survive successive therapies? (Schaff et al. 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: What deteriorates in a cell as it ages (Silva and Schumacher 2021)\n\n\n\n\n\n\n2.2.2 Benchside-to-bedside applications\nSingle-cell data have enormous potential to revolutionize clinical practice by bridging molecular biology and medicine. After learning basic biology, the next stage is to use our newfound understanding to advance treatments. (We typically call this translation research, to denote translating our basic biology knowledge to therapeutic improvements.) This is also called benchside (for the wet-bench, i.e. laboratory setting) to bedside (for the hospital setting).\nBy mapping cellular heterogeneity in diseased and healthy tissues, researchers can identify specific cell populations driving disease progression and therapeutic resistance. For example, in cancer, single-cell analyses have uncovered rare tumor subclones that evade treatment, providing critical targets for drug development. Similarly, in autoimmune diseases like rheumatoid arthritis, single-cell profiling of synovial tissues has identified inflammatory cell states that correlate with disease severity and treatment response. Beyond diagnostics, this technology enables precision medicine by tailoring treatments to the molecular profiles of individual patients. As single-cell approaches continue to evolve, they are poised to refine drug-discovery pipelines, improve vaccine design, and ultimately transform how diseases are diagnosed and treated.\nThere are a few ways this typically happens. Figure 2.10 shows one example, where single-cell research helps identify the specific cell types and specific edits needed to improve cellular function. Figure 2.11 shows another example, where understanding the cellular functions, we can improve how conventional methods can be used to measure more accurate biomarkers. Cancer research has been (by far) the biggest beneficiary of single-cell research, and Figure 2.12 illustrates how single-cell improves cancer therapies.\n\n\n\n\n\n\n\nFigure 2.10: There are current single-cell therapies that involve extracting blood from a donor, altering the cells outside the body, and then infusing the altered blood back into the donor. (Source: https://www.cancer.gov/news-events/cancer-currents-blog/2020/crispr-cancer-research-treatment)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: After learning the causal mechanisms for specific cell types for Alzheimer’s disease that can be detected from blood draws, we can refine existing biomarkers to monitor Alzheimer’s. (Hansson et al. 2023)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Example of how insights from single-cell studies transform cancer therapies. (Gohil et al. 2021)\n\n\n\n\n\n\n\n2.2.3 What existed prior to single-cell data?\nWestern blots and flow cytometry.\nBefore the advent of single-cell technologies, biological research relied heavily on methods like western blots and flow cytometry to study cells and molecules. Western blotting (shown in Figure 2.13 and Figure 2.14), developed in the late 1970 s, enabled researchers to detect and quantify specific proteins in a sample, providing insights into cellular pathways and protein-expression levels. However, this technique required lysing entire tissues or cell populations, averaging the signals from thousands or millions of cells. Flow cytometry (shown in Figure 2.15 and Figure 2.16), emerging in the 1980s, represented a major step forward by allowing researchers to analyse individual cells’ physical and chemical characteristics in suspension. While flow cytometry offered single-cell resolution, it was limited to analysing predefined markers and could not capture the full complexity of cellular states or gene expression.\n\n\n\n\n\n\n\nFigure 2.13: Illustration of the Western-blot technique. (Source: https://www.bio-rad.com/en-us/applications-technologies/western-blotting-electrophoresis-techniques)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Example Western-blot data. (Kepchia et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Illustration of flow-cytometry technique. (Source: https://www.streck.com/blog/principles-of-flow-cytometry/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: Example flow-cytometry data. (Verschoor et al. 2015)\n\n\n\n\nMicroarray technology.\nThe 1990s marked the rise of microarray technology (shown in Figure 2.17 and Figure 2.18), which allowed scientists to measure the expression levels of thousands of genes simultaneously. Microarrays revolutionised transcriptomics by enabling high-throughput studies of gene activity in various conditions, tissues, and diseases. Despite its transformative impact, microarray analysis was fundamentally a bulk method, averaging signals across all cells in a sample. It is also based on light intensity, which can be tricky to extract consistently. These limitations obscured cellular heterogeneity, especially in complex tissues where distinct cell types or states contribute uniquely to biological processes or disease mechanisms.\n\n\n\n\n\n\n\nFigure 2.17: Illustration of microarray technique. (Source: https://www.onlinebiologynotes.com/dna-microarray-principle-types-and-steps-involved-in-cdna-microarrays/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.18: Example microarray image prior to quantification. (Source: https://online.stat.psu.edu/stat555/node/28/)\n\n\n\n\n\nRemark (Personal opinion: the close history between microarrays and high-dimensional statistics).\nHistorically, the rise of microarray data spurred advances in high-dimensional statistics — e.g. the Lasso (Tibshirani et al. 2005), gene-expression classification for leukaemia (Golub et al. 1999), and empirical-Bayes multiple testing (Efron and Tibshirani 2002).\n\nBulk sequencing technologies.\nIn the 2000s, bulk sequencing technologies for DNA and RNA emerged, further advancing the study of genomes and transcriptomes. RNA-sequencing (RNA-seq) became a powerful tool for capturing the entire transcriptome with greater accuracy and dynamic range than microarrays. Similarly, DNA sequencing enabled comprehensive studies of genetic variation, from point mutations to structural alterations. However, like microarrays, bulk sequencing aggregated signals across many cells, masking rare cell populations and the heterogeneity critical to understanding dynamic processes such as tumour evolution or immune responses. These bulk techniques laid the groundwork for single-cell methods by driving innovations in high-throughput sequencing and data analysis, which would later be adapted for single-cell resolution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "href": "intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "title": "2  Introduction",
    "section": "2.3 When did single-cell data become popular, and how has the technology advanced?",
    "text": "2.3 When did single-cell data become popular, and how has the technology advanced?\n\n2.3.1 The rise of single-cell data\nSingle-cell data began gaining prominence in the early 2010s, fuelled by advances in microfluidics and next-generation sequencing. Single-cell RNA-sequencing (scRNA-seq), pioneered around 2009 – 2011, was among the first methods to achieve widespread adoption. It enabled measurement of gene expression in individual cells, uncovering heterogeneity that bulk analyses masked. Early applications revealed new cell types and states, reshaped our understanding of development, and identified rare populations in cancers and neurodegenerative disorders. Popularity grew as throughput increased, costs fell, and workflows became standardised.\nFigure 2.19 and Figure 2.20 contrasts bulk and single-cell sequencing. Figure 2.21 shows how single-cell data tease apart different sources of heterogeneity.\n\n\n\n\n\n\n\nFigure 2.19: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.21: Single-cell insights disentangle underlying biological mechanisms. (Lei et al. 2021)\n\n\n\n\n\n\n2.3.2 Expansion into other omics and spatial technologies\nBuilding on single-cell transcriptomics, the field rapidly expanded into other omics. Single-cell proteomics allows detailed analysis of protein expression and signalling pathways. Single-cell ATAC-seq profiles chromatin accessibility; Hi-C and related methods reveal 3D genome architecture. Spatial transcriptomics connects gene expression with tissue context. CRISPR-based single-cell screens enable high-throughput perturbations, and lineage-tracing barcodes add a temporal dimension, charting cell ancestry in development and disease. Together, these advances transformed single-cell biology into a multi-dimensional, integrative discipline.\nMany of these technologies appear in Figure 2.22.\n\n\n\n\n\n\n\nFigure 2.22: Illustration (circa 2020) of technologies that pair various omics at single-cell resolution. (Teichmann and Efremova 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "href": "intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "title": "2  Introduction",
    "section": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?",
    "text": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?\nCell biology is vast – especially for students trained primarily in statistics or biostatistics. Many disciplines intersect with cell biology. For example:\n\nStatistics / Biostatistics – statistical models for complex biological processes; translation between maths and biology\nComputational biology – scalable computation, leveraging public data\nBioinformatics / Genetics – tools that draw on large‐consortium resources\nEpidemiology – population-level data and policy recommendations\nBioengineering – new laboratory technologies for cheaper/faster measurement or imaging\nBiology – mechanistic studies in model organisms\nBiochemistry / Molecular biology – structure, function, interactions of specific molecules\nWet-bench medicine – disease mechanisms via tissues, models, cell lines\nClinical-facing medicine – patient treatment and real-world sample collection\nPharmacology – integrating evidence to design new drugs and therapies\n\nGiven so many players, what does a biostatistician contribute?\n\n2.4.1 How a biostatistician perceives the world\nGive me a concrete (ideally cleaned) dataset: the larger the better—and I will analyse it from many angles.\nCausality: A mathematically stricter notion than correlation, usually via (1) counterfactual reasoning, or (2) a directed-acyclic-graph picture of how variables relate.\n\nRemark (Personal opinion: statistical causality for cell biology is extremely difficult). Obstacles: (i) tracking the same cell over time is impossible because sequencing lyses it; (ii) longitudinal human tissue samples are rare. Strong modelling assumptions can help but must withstand biological scrutiny. The bottleneck is often data, not maths—ambitious statisticians who learn enough biology still have a fighting chance.\n\n\nRemark (Single-cell methods are largely an “associative” world): Most single-cell analyses discover mechanisms that are statistically correlational; the causal proof comes from experiments and biology.\n\nThe research inquiry starts and ends with a method: (How to integrate modalities? learn a gene-regulatory network? perform valid post-clustering tests?) Start with a statistical model and a parameter of interest \\(\\theta^*\\), then typically:\n\nDevelop a novel estimator of \\(\\theta^*\\), explaining why current methods fail (e.g. lack robustness, accuracy, power, or are heuristic). Focus on statistical logic: A clear, simple mathematical intuition should show the gap and how the new method fills it.\nProve theorems showing the estimate \\(\\hat\\theta\\) converges to \\(\\theta^*\\) under stated assumptions. Focus on consistency & convergence: More data should provably yield more accurate results (often the highlight of a statistics paper).\nSimulations demonstrating that when the true \\(\\theta^*\\) is known, \\(\\hat\\theta\\) beats competing estimators across many settings. Illustration via benchmarking: Empirically recover the correct answer more often than existing methods.\nReal-data demonstration showing results align with known biology or provide biologically sensible new insights. Focus on practicality: The method must work in real scenarios mirroring its target audience.\n\nMindset: deliver a reliable tool that others can trust as-is. Human validations are often impractical; guard-rails and diagnostics are vital.\nWhy biostatisticians need wet-lab biologists / clinicians: We rarely generate data ourselves, so collaborators supply (i) exciting data with novel questions, (ii) biological context for sensible assumptions, and (iii) experimental validation of statistical findings.\n\n\n2.4.2 How a wet-lab biologist / clinician perceives the world\nExperiments, experiments, experiments: Carefully controlled—even if small—to make downstream analysis straightforward.\nCausality comes from a chain of experiments. Suppose we study a gene’s role in disease:\n\nTemporal evidence, such as change in gene expression preceding a change in cell phenotype. A causal mechanism should occur before the phenotype.\nBiological logic providing explanation of the underlying mechanism (binding factors, protein function, evolutionary rationale, etc.). For example, there must be a coherent pathway from gene → protein → phenotype.\nUniversality of how the described association persists across cell lines or organisms. A causal mechanism should be discoverable in other systems (extent depends on how general the logic is). This is offten the highlight of a biology paper.\nValidation, such as knocking out the gene alters the outcome, whereas similar genes do not. For example, perturbing the specific gene (not its close counterparts) changes the outcome.\n\nInquiry starts and ends with a biological hypothesis: Large intellectual effort goes into proposing explanations and designing experiments to rule them in or out.\nMindset: Assemble overwhelming evidence for a mechanism, combining careful experiments and biological logic.\nWhy wet-lab scientists need biostatisticians: Data are now complex and plentiful; exhaustive experiments for every hypothesis are infeasible. Statistical methods can (i) account for data & biological complexity and (ii) prioritise hypotheses worth experimental investment.\n\nSo how does a biostatistician develop computational / statistical methods for cell biology?\n\n\nBiological context – What is the biological system and the “north-star” question? Which premises are accepted, which ones are to be tested, and why is it important to understand this mechanism better?\nTechnology, experiment, data – How are data collected and why did you choose this particular {technology, experiment, data} trio? What technical artefacts arise?\nBoundaries of current tools – Simple analyses first: what “breaks” in existing workflows? Is there preliminary evidence a new computational method would do better?\nStatistical model – What is the insight that a different computational method could interrogate the biology better? (Here the statistics training begins.)\nDevelop a method & show robustness – Robustness can be defined numerically (i.e., noise tolerance) or biologically (i.e., applicable across contexts/environments). Often, the biological question you study lacks a ground truth, so validity arguments lean on biological logic.\nUncover new / refined biology – Does the method advance our biological understanding? How confident are we that findings generalise beyond the original {technology, experiment, data} trio? (This is usually the crown-jewel of your computational biology paper – it’s not necessary your specific analyses, but the potential that your tool can be used on other biological studies beyond what you’ve demonstrated your method on.)\n\n\n\n\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes Methods and False Discovery Rates for Microarrays.” Genetic Epidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and Kenneth J Livak. 2021. “Applying High-Dimensional Single-Cell Technologies to the Analysis of Cancer Immunotherapy.” Nature Reviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023. “Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and Trials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N Shokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse Proteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease Brain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun Yu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in Cancer Research: Progress and Perspectives.” Journal of Hematology & Oncology 14 (1): 91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde, and Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable Responses to Sequential and Prolonged Treatment.” Cell Systems 15 (3): 213–26.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to Tailor Treatments.” Science Translational Medicine 9 (408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the Molecular and Cellular Mechanisms of Aging.” Journal of Investigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year 2019: Single-Cell Multimodal Omics.” Nature Methods 17 (1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. “Sparsity and Smoothness via the Fused Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME Bowdish. 2015. “An Introduction to Automated Flow Cytometry Gating Tools and Their Implementation.” Frontiers in Immunology 6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-study-cell-biology-in-public-health",
    "href": "intro.html#why-study-cell-biology-in-public-health",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n\n2.1.2 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 2.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 2.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)\n\n\n\n\n\n\n2.1.3 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 2.4 and Figure 2.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 2.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised – its instead mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an “average/typical” scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html",
    "href": "chapter1_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why study cell biology in public health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#why-study-cell-biology-in-public-health",
    "href": "chapter1_intro.html#why-study-cell-biology-in-public-health",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 What are “omics”?\nThe term “omics” refers to a broad field of biology aimed at the comprehensive characterisation and quantification of biological molecules that translate into the structure, function, and dynamics of an organism. At its core, “omics” encapsulates the idea of studying biological systems at a global scale rather than focusing on individual components. This includes genomics (DNA), transcriptomics (RNA), proteomics (proteins), epigenomics, metabolomics (metabolites), and more. Each of these fields leverages high-throughput technologies to generate massive datasets that capture complex interactions within cells, tissues, or organisms.\nThe rise of “omics” has revolutionised biology by enabling researchers to ask holistic questions such as how different genes, proteins, or metabolites interact in health and disease. It emphasises understanding systems as interconnected networks rather than isolated elements. This systems-level approach is particularly powerful in identifying biomarkers, understanding disease mechanisms, and tailoring precision-medicine strategies. In public health, “omics” provides tools to bridge molecular discoveries with population-level outcomes, offering new opportunities to tackle complex health challenges.\n\n\n2.1.2 Some examples of a “cell-biology” question\n\nHow the brain develops (i.e. the longitudinal sequence of events between birth and maturation), shown in Figure 2.1.\n\nHow microglia in the brain gain or lose certain functions during ageing, shown in Figure 2.2.\n\nWhy certain cells (i.e. cancer cells) divide uncontrollably, see Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.1: What is the sequence of cell types that emerge (and where) during human brain development? (Zhou, Song, and Ming 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Microglia (immune cells of the brain) radically change shape during ageing.\nWhat are the functional consequences of this? (Von Bernhardi, Eugenı́n-von Bernhardi, and Eugenı́n 2015)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Cancer is typically characterised by uncontrollable cell division. What drives this process, how can we detect it early, and how can we stop it? (Source: https://sites.duke.edu/seektobacco/2-the-role-of-tobacco-in-the-development-of-cancer/the-content/)\n\n\n\n\n\n\n2.1.3 How “omics” comes into the picture\nTo answer cell-biology questions we leverage different omics to learn clues about\n1. the cellular functions of a biological system, and\n2. how those functions change during disease, ageing, etc.\nAll these omics are related:\n\nThe central dogma of biology (DNA → RNA → protein) is illustrated in Figure 2.4 and Figure 2.5, linking the three most fundamental omics.\n\nThe epigenome, shown in Figure 2.6, comprises chemical modifications to DNA and histone proteins that regulate gene expression yet are not part of the DNA itself. The figure highlights three commonly studied features: DNA accessibility, DNA methylation, and histone modifications.\n\nThese layers are summarised in Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.4: Central dogma. (Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Central dogma. (Source: https://researchfeatures.com/adding-structure-biologys-central-dogma-cancer-treatments-thyroid-cancer/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Epigenetics. (Source: https://en.wikipedia.org/wiki/Epigenetics)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: A non-exhaustive list of omics. (Wen et al. 2022)\n\n\n\n\n\nRemark (Personal opinion: Biology is constantly revising the details).\nStatistical knowledge is rarely revised – its instead mostly refined. We know properties that generalize to much broader settings (i.e., less statistical assumptions) and much more refined statistical rates for the settings studied two decades ago. (1) There’s little revisions, since once someone proves a statistical theorem, it is very unlikely for it to get disproven in the future. (2) Statistics generally focuses on what happens in an “average/typical” scenario.\nBiology, in comparison, has many revisions and refinements. There’s a couple reasons for this: (1) Biology research is driven by technology. Hence, as we can image/sequence/profile new aspects of a cell, design new model organisms, or collect more data, we might revise a lot of understanding of how cells work. (2) While there are broad biological mechanisms that generally hold true, many diseases occur when the general biological principle no longer holds true. For this reasons, a lot of cell biology research is about these exceptions, which cause us to question how universally true a biological mechanism is. (As a simple example – we’re taught humans have 46 chromosomes. However, many conditions such as Down Syndrome, originate from having an abnormal number of chromosomes.)\n\n\nRemark (All the omics we will study in this course are matrices).\nOne of major missions of this course is to answer the following question: Every omic is represented as a matrix (generally, where the rows are cells, and columns are certain features, depending on the omic). In that case, how come some statistical methods designed for one omic isn’t applicable for another omic?\nWhile there are certain statistical answers to this question, most of the answers are based on biology. Certain methods rely on a specific biological premise, and that premise becomes hard to justify as you switch from one omic to another.\nThis is not too dissimilar from a causal analysis. In a causal analysis, the reason certain features get labeled as a confounder, treatment, outcome, instrumental variable, mediator, etc. relies on the context.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "href": "chapter1_intro.html#what-do-we-hope-to-learn-from-single-cell-data",
    "title": "2  Introduction",
    "section": "2.2 What do we hope to learn from single-cell data?",
    "text": "2.2 What do we hope to learn from single-cell data?\n\n2.2.1 Basic biology\nSingle-cell data offer a transformative lens to study the fundamental processes of life at unparalleled resolution. Unlike bulk data, which averages signals across populations of cells, single-cell technologies allow researchers to examine the diversity and complexity of individual cells within a tissue or organism. This level of detail provides insights into key biological phenomena, such as cellular differentiation during development, the plasticity of cell states in response to environmental cues, and the organization of complex tissues. For instance, single-cell RNA-sequencing (scRNA-seq) has uncovered new cell types in the brain and immune system, challenging traditional classifications and offering a more nuanced understanding of cellular identities and functions. These insights are essential for constructing more accurate models of how life operates at a cellular level.\nMany of the examples shown in Figs. Figure 2.1–Figure 2.2 are basic biology questions. Two additional examples are shown in Figure 2.8 and Figure 2.9.\n\n\n\n\n\n\n\nFigure 2.8: How do cancer cells survive successive therapies? (Schaff et al. 2024)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: What deteriorates in a cell as it ages (Silva and Schumacher 2021)\n\n\n\n\n\n\n2.2.2 Benchside-to-bedside applications\nSingle-cell data have enormous potential to revolutionize clinical practice by bridging molecular biology and medicine. After learning basic biology, the next stage is to use our newfound understanding to advance treatments. (We typically call this translation research, to denote translating our basic biology knowledge to therapeutic improvements.) This is also called benchside (for the wet-bench, i.e. laboratory setting) to bedside (for the hospital setting).\nBy mapping cellular heterogeneity in diseased and healthy tissues, researchers can identify specific cell populations driving disease progression and therapeutic resistance. For example, in cancer, single-cell analyses have uncovered rare tumor subclones that evade treatment, providing critical targets for drug development. Similarly, in autoimmune diseases like rheumatoid arthritis, single-cell profiling of synovial tissues has identified inflammatory cell states that correlate with disease severity and treatment response. Beyond diagnostics, this technology enables precision medicine by tailoring treatments to the molecular profiles of individual patients. As single-cell approaches continue to evolve, they are poised to refine drug-discovery pipelines, improve vaccine design, and ultimately transform how diseases are diagnosed and treated.\nThere are a few ways this typically happens. Figure 2.10 shows one example, where single-cell research helps identify the specific cell types and specific edits needed to improve cellular function. Figure 2.11 shows another example, where understanding the cellular functions, we can improve how conventional methods can be used to measure more accurate biomarkers. Cancer research has been (by far) the biggest beneficiary of single-cell research, and Figure 2.12 illustrates how single-cell improves cancer therapies.\n\n\n\n\n\n\n\nFigure 2.10: There are current single-cell therapies that involve extracting blood from a donor, altering the cells outside the body, and then infusing the altered blood back into the donor. (Source: https://www.cancer.gov/news-events/cancer-currents-blog/2020/crispr-cancer-research-treatment)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: After learning the causal mechanisms for specific cell types for Alzheimer’s disease that can be detected from blood draws, we can refine existing biomarkers to monitor Alzheimer’s. (Hansson et al. 2023)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Example of how insights from single-cell studies transform cancer therapies. (Gohil et al. 2021)\n\n\n\n\n\n\n\n2.2.3 What existed prior to single-cell data?\nWestern blots and flow cytometry.\nBefore the advent of single-cell technologies, biological research relied heavily on methods like western blots and flow cytometry to study cells and molecules. Western blotting (shown in Figure 2.13 and Figure 2.14), developed in the late 1970 s, enabled researchers to detect and quantify specific proteins in a sample, providing insights into cellular pathways and protein-expression levels. However, this technique required lysing entire tissues or cell populations, averaging the signals from thousands or millions of cells. Flow cytometry (shown in Figure 2.15 and Figure 2.16), emerging in the 1980s, represented a major step forward by allowing researchers to analyse individual cells’ physical and chemical characteristics in suspension. While flow cytometry offered single-cell resolution, it was limited to analysing predefined markers and could not capture the full complexity of cellular states or gene expression.\n\n\n\n\n\n\n\nFigure 2.13: Illustration of the Western-blot technique. (Source: https://www.bio-rad.com/en-us/applications-technologies/western-blotting-electrophoresis-techniques)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Example Western-blot data. (Kepchia et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Illustration of flow-cytometry technique. (Source: https://www.streck.com/blog/principles-of-flow-cytometry/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: Example flow-cytometry data. (Verschoor et al. 2015)\n\n\n\n\nMicroarray technology.\nThe 1990s marked the rise of microarray technology (shown in Figure 2.17 and Figure 2.18), which allowed scientists to measure the expression levels of thousands of genes simultaneously. Microarrays revolutionised transcriptomics by enabling high-throughput studies of gene activity in various conditions, tissues, and diseases. Despite its transformative impact, microarray analysis was fundamentally a bulk method, averaging signals across all cells in a sample. It is also based on light intensity, which can be tricky to extract consistently. These limitations obscured cellular heterogeneity, especially in complex tissues where distinct cell types or states contribute uniquely to biological processes or disease mechanisms.\n\n\n\n\n\n\n\nFigure 2.17: Illustration of microarray technique. (Source: https://www.onlinebiologynotes.com/dna-microarray-principle-types-and-steps-involved-in-cdna-microarrays/)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.18: Example microarray image prior to quantification. (Source: https://online.stat.psu.edu/stat555/node/28/)\n\n\n\n\n\nRemark (Personal opinion: the close history between microarrays and high-dimensional statistics).\nHistorically, the rise of microarray data spurred advances in high-dimensional statistics — e.g. the Lasso (Tibshirani et al. 2005), gene-expression classification for leukaemia (Golub et al. 1999), and empirical-Bayes multiple testing (Efron and Tibshirani 2002).\n\nBulk sequencing technologies.\nIn the 2000s, bulk sequencing technologies for DNA and RNA emerged, further advancing the study of genomes and transcriptomes. RNA-sequencing (RNA-seq) became a powerful tool for capturing the entire transcriptome with greater accuracy and dynamic range than microarrays. Similarly, DNA sequencing enabled comprehensive studies of genetic variation, from point mutations to structural alterations. However, like microarrays, bulk sequencing aggregated signals across many cells, masking rare cell populations and the heterogeneity critical to understanding dynamic processes such as tumour evolution or immune responses. These bulk techniques laid the groundwork for single-cell methods by driving innovations in high-throughput sequencing and data analysis, which would later be adapted for single-cell resolution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "href": "chapter1_intro.html#when-did-single-cell-data-become-popular-and-how-has-the-technology-advanced",
    "title": "2  Introduction",
    "section": "2.3 When did single-cell data become popular, and how has the technology advanced?",
    "text": "2.3 When did single-cell data become popular, and how has the technology advanced?\n\n2.3.1 The rise of single-cell data\nSingle-cell data began gaining prominence in the early 2010s, fuelled by advances in microfluidics and next-generation sequencing. Single-cell RNA-sequencing (scRNA-seq), pioneered around 2009 – 2011, was among the first methods to achieve widespread adoption. It enabled measurement of gene expression in individual cells, uncovering heterogeneity that bulk analyses masked. Early applications revealed new cell types and states, reshaped our understanding of development, and identified rare populations in cancers and neurodegenerative disorders. Popularity grew as throughput increased, costs fell, and workflows became standardised.\nFigure 2.19 and Figure 2.20 contrasts bulk and single-cell sequencing. Figure 2.21 shows how single-cell data tease apart different sources of heterogeneity.\n\n\n\n\n\n\n\nFigure 2.19: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Illustrations of how single-cell sequencing improves resolution for studying cancer resistance. (Shalek and Benson 2017)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.21: Single-cell insights disentangle underlying biological mechanisms. (Lei et al. 2021)\n\n\n\n\n\n\n2.3.2 Expansion into other omics and spatial technologies\nBuilding on single-cell transcriptomics, the field rapidly expanded into other omics. Single-cell proteomics allows detailed analysis of protein expression and signalling pathways. Single-cell ATAC-seq profiles chromatin accessibility; Hi-C and related methods reveal 3D genome architecture. Spatial transcriptomics connects gene expression with tissue context. CRISPR-based single-cell screens enable high-throughput perturbations, and lineage-tracing barcodes add a temporal dimension, charting cell ancestry in development and disease. Together, these advances transformed single-cell biology into a multi-dimensional, integrative discipline.\nMany of these technologies appear in Figure 2.22.\n\n\n\n\n\n\n\nFigure 2.22: Illustration (circa 2020) of technologies that pair various omics at single-cell resolution. (Teichmann and Efremova 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1_intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "href": "chapter1_intro.html#what-is-the-role-of-a-biostatistician-in-a-wet-lab-clinical-world",
    "title": "2  Introduction",
    "section": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?",
    "text": "2.4 What is the role of a biostatistician in a wet-lab / clinical world?\nCell biology is vast – especially for students trained primarily in statistics or biostatistics. Many disciplines intersect with cell biology. For example:\n\nStatistics / Biostatistics – statistical models for complex biological processes; translation between maths and biology\nComputational biology – scalable computation, leveraging public data\nBioinformatics / Genetics – tools that draw on large‐consortium resources\nEpidemiology – population-level data and policy recommendations\nBioengineering – new laboratory technologies for cheaper/faster measurement or imaging\nBiology – mechanistic studies in model organisms\nBiochemistry / Molecular biology – structure, function, interactions of specific molecules\nWet-bench medicine – disease mechanisms via tissues, models, cell lines\nClinical-facing medicine – patient treatment and real-world sample collection\nPharmacology – integrating evidence to design new drugs and therapies\n\nGiven so many players, what does a biostatistician contribute?\n\n2.4.1 How a biostatistician perceives the world\nGive me a concrete (ideally cleaned) dataset: the larger the better—and I will analyse it from many angles.\nCausality: A mathematically stricter notion than correlation, usually via (1) counterfactual reasoning, or (2) a directed-acyclic-graph picture of how variables relate.\n\nRemark (Personal opinion: statistical causality for cell biology is extremely difficult). Obstacles: (i) tracking the same cell over time is impossible because sequencing lyses it; (ii) longitudinal human tissue samples are rare. Strong modelling assumptions can help but must withstand biological scrutiny. The bottleneck is often data, not maths—ambitious statisticians who learn enough biology still have a fighting chance.\n\n\nRemark (Single-cell methods are largely an “associative” world): Most single-cell analyses discover mechanisms that are statistically correlational; the causal proof comes from experiments and biology.\n\nThe research inquiry starts and ends with a method: (How to integrate modalities? learn a gene-regulatory network? perform valid post-clustering tests?) Start with a statistical model and a parameter of interest \\(\\theta^*\\), then typically:\n\nDevelop a novel estimator of \\(\\theta^*\\), explaining why current methods fail (e.g. lack robustness, accuracy, power, or are heuristic). Focus on statistical logic: A clear, simple mathematical intuition should show the gap and how the new method fills it.\nProve theorems showing the estimate \\(\\hat\\theta\\) converges to \\(\\theta^*\\) under stated assumptions. Focus on consistency & convergence: More data should provably yield more accurate results (often the highlight of a statistics paper).\nSimulations demonstrating that when the true \\(\\theta^*\\) is known, \\(\\hat\\theta\\) beats competing estimators across many settings. Illustration via benchmarking: Empirically recover the correct answer more often than existing methods.\nReal-data demonstration showing results align with known biology or provide biologically sensible new insights. Focus on practicality: The method must work in real scenarios mirroring its target audience.\n\nMindset: deliver a reliable tool that others can trust as-is. Human validations are often impractical; guard-rails and diagnostics are vital.\nWhy biostatisticians need wet-lab biologists / clinicians: We rarely generate data ourselves, so collaborators supply (i) exciting data with novel questions, (ii) biological context for sensible assumptions, and (iii) experimental validation of statistical findings.\n\n\n2.4.2 How a wet-lab biologist / clinician perceives the world\nExperiments, experiments, experiments: Carefully controlled—even if small—to make downstream analysis straightforward.\nCausality comes from a chain of experiments. Suppose we study a gene’s role in disease:\n\nTemporal evidence, such as change in gene expression preceding a change in cell phenotype. A causal mechanism should occur before the phenotype.\nBiological logic providing explanation of the underlying mechanism (binding factors, protein function, evolutionary rationale, etc.). For example, there must be a coherent pathway from gene → protein → phenotype.\nUniversality of how the described association persists across cell lines or organisms. A causal mechanism should be discoverable in other systems (extent depends on how general the logic is). This is offten the highlight of a biology paper.\nValidation, such as knocking out the gene alters the outcome, whereas similar genes do not. For example, perturbing the specific gene (not its close counterparts) changes the outcome.\n\nInquiry starts and ends with a biological hypothesis: Large intellectual effort goes into proposing explanations and designing experiments to rule them in or out.\nMindset: Assemble overwhelming evidence for a mechanism, combining careful experiments and biological logic.\nWhy wet-lab scientists need biostatisticians: Data are now complex and plentiful; exhaustive experiments for every hypothesis are infeasible. Statistical methods can (i) account for data & biological complexity and (ii) prioritise hypotheses worth experimental investment.\n\nSo how does a biostatistician develop computational / statistical methods for cell biology?\n\n\nBiological context – What is the biological system and the “north-star” question? Which premises are accepted, which ones are to be tested, and why is it important to understand this mechanism better?\nTechnology, experiment, data – How are data collected and why did you choose this particular {technology, experiment, data} trio? What technical artefacts arise?\nBoundaries of current tools – Simple analyses first: what “breaks” in existing workflows? Is there preliminary evidence a new computational method would do better?\nStatistical model – What is the insight that a different computational method could interrogate the biology better? (Here the statistics training begins.)\nDevelop a method & show robustness – Robustness can be defined numerically (i.e., noise tolerance) or biologically (i.e., applicable across contexts/environments). Often, the biological question you study lacks a ground truth, so validity arguments lean on biological logic.\nUncover new / refined biology – Does the method advance our biological understanding? How confident are we that findings generalise beyond the original {technology, experiment, data} trio? (This is usually the crown-jewel of your computational biology paper – it’s not necessary your specific analyses, but the potential that your tool can be used on other biological studies beyond what you’ve demonstrated your method on.)\n\n\n\n\n\n\nEfron, Bradley, and Robert Tibshirani. 2002. “Empirical Bayes Methods and False Discovery Rates for Microarrays.” Genetic Epidemiology 23 (1): 70–86.\n\n\nGohil, Satyen H, J Bryan Iorgulescu, David A Braun, Derin B Keskin, and Kenneth J Livak. 2021. “Applying High-Dimensional Single-Cell Technologies to the Analysis of Cancer Immunotherapy.” Nature Reviews Clinical Oncology 18 (4): 244–56.\n\n\nGolub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science 286 (5439): 531–37.\n\n\nHansson, Oskar, Kaj Blennow, Henrik Zetterberg, and Jeffrey Dage. 2023. “Blood Biomarkers for Alzheimer’s Disease in Clinical Practice and Trials.” Nature Aging 3 (5): 506–19.\n\n\nKepchia, Devin, Ling Huang, Richard Dargusch, Robert A Rissman, Maxim N Shokhirev, Wolfgang Fischer, and David Schubert. 2020. “Diverse Proteins Aggregate in Mild Cognitive Impairment and Alzheimer’s Disease Brain.” Alzheimer’s Research & Therapy 12: 1–20.\n\n\nLei, Yalan, Rong Tang, Jin Xu, Wei Wang, Bo Zhang, Jiang Liu, Xianjun Yu, and Si Shi. 2021. “Applications of Single-Cell Sequencing in Cancer Research: Progress and Perspectives.” Journal of Hematology & Oncology 14 (1): 91.\n\n\nSchaff, Dylan L, Aria J Fasse, Phoebe E White, Robert J Vander Velde, and Sydney M Shaffer. 2024. “Clonal Differences Underlie Variable Responses to Sequential and Prolonged Treatment.” Cell Systems 15 (3): 213–26.\n\n\nShalek, Alex K, and Mikael Benson. 2017. “Single-Cell Analyses to Tailor Treatments.” Science Translational Medicine 9 (408): eaan4730.\n\n\nSilva, Paulo FL da, and Björn Schumacher. 2021. “Principles of the Molecular and Cellular Mechanisms of Aging.” Journal of Investigative Dermatology 141 (4): 951–60.\n\n\nTeichmann, Sarah, and Mirjana Efremova. 2020. “Method of the Year 2019: Single-Cell Multimodal Omics.” Nature Methods 17 (1): 2020.\n\n\nTibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. “Sparsity and Smoothness via the Fused Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (1): 91–108.\n\n\nVerschoor, Chris P, Alina Lelic, Jonathan L Bramson, and Dawn ME Bowdish. 2015. “An Introduction to Automated Flow Cytometry Gating Tools and Their Implementation.” Frontiers in Immunology 6: 380.\n\n\nVon Bernhardi, Rommy, Laura Eugenı́n-von Bernhardi, and Jaime Eugenı́n. 2015. “Microglial Cell Dysregulation in Brain Aging and Neurodegeneration.” Frontiers in Aging Neuroscience 7: 124.\n\n\nWen, Lu, Guoqiang Li, Tao Huang, Wei Geng, Hao Pei, Jialiang Yang, Miao Zhu, et al. 2022. “Single-Cell Technologies: From Research to Application.” The Innovation 3 (6).\n\n\nZhou, Yi, Hongjun Song, and Guo-li Ming. 2024. “Genetics of Human Brain Development.” Nature Reviews Genetics 25 (1): 26–45.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html",
    "href": "chapter2_sequencing.html",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "3.1 The sparse count matrix and the negative binomial\nAt the heart of single-cell sequencing data lies the sparse count matrix, where rows represent genes, columns represent cells, and entries capture the number of sequencing reads (or molecules) detected for each gene in each cell. This matrix is typically sparse because most genes are not expressed in any given cell, leading to a prevalence of zeros. The data’s sparsity reflects both the biological reality of selective gene expression and technical limitations such as sequencing depth. To model the variability in these counts, the negative binomial distribution is often employed. This distribution accommodates overdispersion, where the observed variability in gene expression counts exceeds what would be expected under simpler models like the Poisson distribution. By accounting for both biological heterogeneity and technical noise, the negative binomial provides a robust framework for analyzing sparse single-cell data.\nLet \\(X \\in \\mathbb{R}^{n\\times p}\\) denote the single-cell data matrix with \\(n\\) cells (rows) and \\(p\\) features (columns). In a single-cell RNA-seq data matrix, the \\(p\\) features represent the \\(p\\) genes.\nHere are some basic statistics about these matrices (based mainly from my experience):\nBased on these observations, the negative binomial distribution is very commonly used.\nThe negative binomial (NB) distribution is a widely used model in single-cell RNA-seq analysis due to its ability to handle overdispersion, which is common in gene expression data. Overdispersion occurs when the variance of the data exceeds the mean, a phenomenon that cannot be captured by the Poisson distribution. The NB distribution introduces an additional parameter to model this extra variability, making it more flexible for single-cell data.\nLet us focus on the count for cell \\(i\\) and gene \\(j\\), i.e., the value \\(X_{ij}\\). The probability mass function (pmf) of the NB distribution for a random variable ( X_{ij} ) is given by:\n\\[\nP(X_{ij} = k) = \\binom{k + r_j - 1}{k} p_{ij}^r (1 - p_{ij})^k, \\quad k = 0, 1, 2, \\ldots\n\\tag{3.1}\\]\nwhere \\(r_j \\&gt; 0\\) is the dispersion (or “overdispersion”) parameter and \\(p\\_{ij} \\in (0, 1)\\) is the probability of success. This is the “standard” parameterization, mentioned in https://en.wikipedia.org/wiki/Negative_binomial_distribution. This specific parameterization is actually not very commonly used.\nAlternatively, the NB distribution is most commonly parameterized in terms of the mean \\(\\mu_{ij}\\) and the dispersion parameter \\(r_j\\), which is often preferred in single-cell analysis:\n\\[\n\\text{Mean: } \\mu_{ij}, \\quad \\text{Variance: } \\mu_{ij} + \\frac{\\mu_{ij}^2}{r_j} = \\mu_{ij}\\left(1 + \\frac{\\mu_{ij}}{r_j}\\right).\n\\tag{3.2}\\]\n(Compare these relations to the Poisson distribution, where both the mean and variance would be \\(\\mu_{ij}\\).)\nTo relate equation Equation 3.1 to Equation 3.2, observe that we can derive,\n\\[\n\\mu_{ij} = \\frac{r_j (1 - p_{ij})}{p_{ij}}.\n\\]\nWe typically use a different overdispersion parameter \\(r_j\\) for each gene \\(j \\in \\{1,\\ldots,p\\}\\). Here, \\(r_j\\) controls the degree of overdispersion. When \\(r_j \\to \\infty\\), the variance approaches the mean, and the NB distribution converges to the Poisson distribution. For single-cell RNA-seq data, the introduction of \\(r_j\\) allows the model to capture both the biological variability between cells and technical noise, making it robust to the sparse and overdispersed nature of gene expression data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-sparse-count-matrix-and-the-negative-binomial",
    "href": "chapter2_sequencing.html#the-sparse-count-matrix-and-the-negative-binomial",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "For \\(n\\), we usually have 10,000 to 500,000 cells. These cells originate from a specific set of samples/donors/organisms1. See Figure 3.2 and Figure 3.3 for larger examples of single-cell datasets.\n\nFor \\(p\\), we usually have about 30,000 genes (but as you’ll see, we usually limit the analysis to 2000 to 5000 genes chosen from this full set).2\n\nIn a typical scRNA-seq matrix, more than 70% of the elements are exactly 0. (And among the remaining 30%, typically half are exactly 1. The maximum count can be in the hundreds, i.e., the distribution is extremely right-skewed.) This is illustrated in ?fig-count1 and ?fig-count2. We’ll see later in ?sec-scrnaseq_tech where these “counts” come from.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIllustration of data from https://satijalab.org/seurat/articles/pbmc3k_tutorial.html showing a snippet of the count matrix (left) or percentage of cells with non-zero counts for each gene (right). (satijalab_tutorial?)\n\n\n\n\n\n\n\n\n\n\n\n\nWhy would genes even have different overdispersions? We’re assuming in most statistical models that: 1) genes are modeled as a negative binomial random variable, and 2) genes have different overdispersion parameters \\(r_j\\). Why is this reasonable? Is there a biomolecular reason for this?\nWe draw upon Sarkar and Stephens (2021), explaining how the NB distribution is justified (both theoretically and empirically) for scRNA-seq data. After reading this paper, you’ll see that the overdispersion originates from the “biological noise” (i.e., the Gamma distribution). This differs from gene to gene because gene expression is subject to many aspects: transcriptomic bursting, the proximity between the gene and the promoter, the mRNA stability and degradation, copy number variation, the cell cycle stage, etc.\n\n\n3.1.0.1 How do you estimate the overdispersion parameter in practice?\nThe easiest way is to use MASS::glm.nb in R. However, this is very noisy for single-cell data (see Remark 3.1). Since we are estimate one dispersion parameter per gene, many methods use an Empirical Bayes framework to “smooth” the overdispersion parameters (Love, Huber, and Anders 2014). More sophisticated methods (such as SCTransform (Hafemeister and Satija 2019)) use the Bioconductor R package called glmGamPoi. Deep-learning methods simply incorporate estimating the dispersion parameter into the architecture and objective function, see scVI (Lopez et al. 2018).\nAlso, note that which we are assuming here that the overdispersion parameter \\(r_j\\) is shared across all the cells for gene \\(j\\), there are methods that also use different overdispersion parameters for different cell populations. See Chen et al. (2018) or the dispersion parameter in scVI (see https://docs.scvi-tools.org/en/stable/api/reference/scvi.model.SCVI.html#scvi.model.SCVI). In general, using different overdispersion parameters for the same gene across different cell populations is not common, so my suggestion is to: 1) have a diagnostic in mind on how would you know if need to use different overdispersion parameters for the same gene, and then 2) try analyzing the genes where each gene only has one overdispersion parameter and see if you have enough concrete evidence that such a model was too simplistic.\n\nPersonal opinion: My preferred parameterization The formulation Equation 3.2 by far is the most common way people write the NB distribution. We typically call \\(r_j\\) the “overdispersion” parameter, since the inclusion of \\(r_j\\) in our modeling is typically to denote that there is more variance than a Poisson. I personally don’t like it because I find it confusing that a larger \\(r_j\\) denotes a distribution with smaller variance. Hence, I usually write the variance as \\(\\mu_{ij}(1+\\alpha_j \\mu_{ij})\\) (where \\(\\alpha_j = 1/r_j\\)). This way, \\(\\alpha_j = 0\\) is a Poisson distribution. However, even though this makes more sense to me, it’s not commonly used. (It’s because this parameterization makes it confusing to work with the exponential-family distributions mathematically.)\nThe main takeaway is to always pay close attention to each paper’s NB parameterization. You’re looking for the mean-variance relation written somewhere in the paper to ensure you understand the author’s notation.\n\n\nRemark 3.1. What makes the negative binomial tricky Let me use an analogy that is based on the more familiar coin toss. Suppose 30 people each are given a weighted coin where the probability of heads is \\(10^{-9}\\) (basically, it’s impossible to get a heads). Everyone coerce and agree to a secret number \\(N\\), and everyone independently flips their own coin \\(N\\) times and records the number of heads. I (the moderator) do not know \\(N\\), but I get to see how many heads each person got during this experiment. My goal is to estimate \\(N\\). What makes this problem very difficult?\nIf \\(N\\) were 10, or 100, or maybe even 10,000, almost everyone would get 0 heads. That is, we cannot reliably estimate \\(N\\) since the log-likelihood function is very flat. The issue is because count data has a finite resolution — 0 is always the smallest possible value, and 1 is always the second smallest value. Hence, unlike continuous values (which have “infinite resolution” in some sense), non-negative integers “lose” information the smaller the range is.\n\nReturning the negative binomials, consider a simple coding example:\n\nset.seed(10)\nmu &lt;- 1\ntrue_overdispersion &lt;- 1000\ntrue_size &lt;- mu / (true_overdispersion - 1)\ndata &lt;- stats::rnbinom(1e5, size = true_size, mu = mu)\noverdispersion_vec &lt;- exp(seq(log(1), log(1e7), length.out = 100))\nlog_likelihood &lt;- sapply(overdispersion_vec, function(overdispersion) {\n  size &lt;- mu / (overdispersion - 1)\n  sum(dnbinom(data, size = size, mu = mu, log = T))\n})\nhist(data,\n     main = paste(\"% 0:\", round(length(which(data == 0)) / length(data) * 100, 2),\n                  \"\\nEmpirical mean:\", round(mean(data), 2),\n                  \"\\nEmpirical variance:\", round(var(data), 2))\n)\nhist(data[data != 0])\nplot(log10(overdispersion_vec), log_likelihood)\nplot(log10(overdispersion_vec[-c(1:5)]), log_likelihood[-c(1:5)])\n\n\n\n\n\n\n\n\nFigure 3.1: (Left) Histogram of a dataset where the true mean is 1 and the overdispersion (i.e., \\(\\alpha = 1/r\\)) is 1000, where almost all the values are 0. Almost 100% of the values are 0 among all 10,000 samples. (Right) The log-likelihood of the overdispersion parameter on the \\(\\log_{10}\\) scale.\n\n\n\n\n\n\n3.1.1 Remembering that these cells come from donors/tissues!\nThe \\(n\\) cells in our single-cell dataset originate from \\(m\\) tissues/samples/donors/etc. That is, the cells are stratified (or hierarchically organized). Based on your scientific question of interest, this is a consideration you might want your analysis to take into account. In general, each of the \\(n\\) cells in your single-cell dataset will have “metadata” (such as which sample the cell came from). See Figure 3.2 and Figure 3.3 as examples. We will see in ?sec-de_cohort on how some methods explicitly take into account this “hierarchical” structure of the data. However, most methods that are intended for cell lines or genetically-controlled scenarios do not focus on this hierarchical structure (since all the cell-lines or organisms are nearly identical).\n\n\n\n\n\n\n\nFigure 3.2: 982 individuals, from which roughly 1200 cells were sequenced from each (from the blood) to give rise to roughly \\(n=1.27\\) million cells. (Yazar et al. 2022)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: 1,223 zebrafish embryos, from which roughly 500 cells were sequenced from each to give rise to roughly \\(n=1.25\\) million cells. (Saunders et al. 2023)\n\n\n\n\nTypically, you can expect that the cells in a single-cell analysis come from one of three categories:\n\nCell-lines (In vitro): Cells grown in controlled laboratory conditions, such as HeLa or HEK293, typically on a petri dish, are commonly used for experiments requiring consistency and ease of manipulation. These cells are ideal for studying specific pathways or drug responses in a simplified system.\nModel organisms (In vivo): Cells derived from model organisms (such as zebrafish, genetically engineered mice, yeast, specific plants, etc.) allow researchers to study conserved biological processes in a controlled, organismal context. These systems often provide genetic and developmental insights that are directly relevant to human biology.\nHumans (In vivo): Cells from human tissues or blood samples provide direct insights into human biology, disease mechanisms, and patient-specific variability. They are often used in translational research to bridge findings from cell lines and model organisms to clinical applications.\n\nOther categories, such as xenografts, organoids, and co-culture systems, also play critical roles in capturing specific aspects of cellular behavior and interactions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#footnotes",
    "href": "chapter2_sequencing.html#footnotes",
    "title": "3  Single-cell sequencing",
    "section": "",
    "text": "You can have a simple hierarchical model in your head – each human donor contributes one (or more) tissue sample. Each tissue sample contains many cells.↩︎\nHow many genes are there in the human body? This is not a well-defined questions. If you want to only ask about protein-coding genes, there’s probably about 24,000 of them (Salzberg 2018). However, most gene callers (such as CellRanger https://www.10xgenomics.com/support/software/cell-ranger/latest, which is what we usually use for 10x single-cell data) also label genes that don’t translate. We’ll talk about this in ?sec-beyond_scrna.↩︎\nTypically, the feature selection step doesn’t use the normalized matrix even though normalization happens before feature selection. See https://github.com/satijalab/seurat/blob/ece572a/R/preprocessing.R#L3995. This is because it’s preferable to normalize using all the reads, even from genes you don’t actually include in your analysis.↩︎\nSee https://en.wikipedia.org/wiki/Low-rank_approximation.↩︎\nTypically, the feature selection step doesn’t use the normalized matrix even though normalization happens before feature selection. See https://github.com/satijalab/seurat/blob/ece572a/R/preprocessing.R#L3995. This is because it’s preferable to normalize using all the reads, even from genes you don’t actually include in your analysis.↩︎\nSee https://en.wikipedia.org/wiki/Low-rank_approximation.↩︎\nSometimes, people use the word “overcrowding” to refer to the fact that embedding methods might collapse nearby points too aggressively.↩︎\nSee Seurat’s recommendation in https://satijalab.org/seurat/articles/seurat5_integration.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html",
    "href": "chapter3_rna.html",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "4.1 Some important nouns and verbs",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#footnotes",
    "href": "chapter3_rna.html#footnotes",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "Technically, genomics (as a field) is a broader categorization and encapsulates genetics. But usually when people say they work on “genomics,” they are colloquially implying they work on biology that is not genetics (otherwise, they usually say they’re a geneticist).↩︎\nThis term is not very often used anymore. It was an umbrella label popularized in the mid-2000s. We instead typically refer to a more specific category of technology. For example, most of the data in this scRNA-seq chapter are referred to as “short-read 3′ single-cell RNA-sequencing.” (You could throw in “droplet-based” and “UMI” to be even more precise, but that’s usually not needed in most casual contexts.)↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter3_rna.html#some-important-nouns-and-verbs",
    "href": "chapter3_rna.html#some-important-nouns-and-verbs",
    "title": "4  Single-cell RNA-sequencing",
    "section": "",
    "text": "Sequencing (verb): The process of determining the order of nucleotides (A, T, C, G) in a DNA or RNA molecule, providing the primary structure of these biomolecules. Sequencing technologies have evolved to be high-throughput, enabling the analysis of entire genomes or transcriptomes.\nSequencing depth, read depth, library size (noun; all synonyms): These terms refer to the number of times a specific nucleotide or region of the genome is sequenced in an experiment. Greater depth provides more accurate detection of rare variants or lowly expressed genes but requires increased computational and financial resources.\nChromosome, DNA, RNA, protein (noun):\n\nChromosome: A large, organized structure of DNA and associated proteins that contains many genes and regulatory elements.\n\nDNA: The molecule that encodes genetic information in a double-helical structure.\n\nRNA: A single-stranded molecule transcribed from DNA that can act as a messenger (mRNA), a structural component (rRNA), or a regulator (e.g., miRNA). When we talk about scRNA-seq, we are usually referring to exclusively measuring mRNA.\n\nProtein: The functional biomolecule synthesized from RNA via translation, performing structural, enzymatic, and regulatory roles in cells.\n\nGenome vs. gene vs. intergenic region (noun):\n\nGenome: The complete set of DNA in an organism, encompassing all of its genetic material, including coding genes, non-coding regions, and regulatory elements. The genome is the blueprint that defines the biological potential of the organism.\n\nGene: A specific sequence within the genome that encodes a functional product, typically a protein or functional RNA. Genes include regions such as exons (coding sequences), introns (non-coding regions within a gene), and regulatory sequences (e.g., promoters and enhancers) that control gene expression. We will see more about the architecture of a gene in Section 7.1.\n\nIntergenic region: The stretches of DNA between genes that do not directly code for proteins or RNA. Intergenic regions were once considered “junk DNA,” but they often contain regulatory elements, such as enhancers and silencers, that influence the expression of nearby or distant genes. These regions also play roles in chromatin organization and genome stability.\n\nGenetics vs. genomics (noun): Genetics typically focuses on the role of the DNA among large populations (of people, of species, etc.), while genomics can encapsulate any omic, and does not necessarily imply studies across a large population1.\n“Next generation sequencing” (noun)2: A collection of high-throughput technologies that allow for the parallel sequencing of millions of DNA or RNA molecules. It has revolutionized biology by enabling large-scale studies of genomes, transcriptomes, and epigenomes.\nRead fragment (noun): A short sequence of DNA or RNA produced as an output from high-throughput sequencing. Fragments are typically between 50 bp and 300 bp long, depending on the sequencing technology, and they represent segments of the original molecule being sequenced.\nReference genome (noun): A curated, complete assembly of the genomic sequence for a species, used as a template to align and interpret sequencing reads. It serves as a baseline for identifying genetic variations, such as mutations or structural changes, and for annotating functional elements.\nCoding genes vs. non-coding genes (noun):\n\nCoding genes: Genes that contain instructions for producing proteins. They are transcribed into mRNA, which is then translated into functional proteins that perform structural, enzymatic, or regulatory roles in cells.\n\nNon-coding genes: Genes that do not produce proteins but instead generate functional RNA molecules, such as rRNA, tRNA, miRNA, or lncRNA, which regulate gene expression, maintain genomic stability, or perform other cellular functions. Non-coding genes highlight the complexity of gene regulation and cellular processes beyond protein synthesis.\n\nEpigenetics vs. epigenomics (noun): Epigenetics studies modifications to DNA and histones (e.g., methylation, acetylation) that regulate gene expression without altering the DNA sequence. Epigenomics examines these modifications across the entire genome.\nTranscriptome (noun): The complete set of RNA transcripts expressed in a cell or tissue at a given time, reflecting dynamic gene activity.\nProteome (noun): The full complement of proteins expressed in a cell, tissue, or organism, representing functional output.\nSingle-cell sequencing (noun): Sequencing technologies applied at the resolution of individual cells, allowing for the study of heterogeneity in gene expression, epigenetics, or genetic variation across cell populations.\nBulk sequencing (noun): Sequencing technologies that aggregate material (e.g., RNA, DNA) from many cells, providing an average profile of the population but masking individual cell variability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single-cell RNA-sequencing</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html",
    "href": "chapter4_protein.html",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "5.1 Review of the central dogma\nThe central dogma of molecular biology outlines the flow of genetic information within a cell: DNA is transcribed into RNA, and RNA is then translated into protein (see Figure 5.1). Specifically, coding genes in DNA are transcribed into messenger RNA (mRNA), which serves as a template for protein synthesis. During translation, mRNA sequences are read in sets of three nucleotides, known as codons, each corresponding to a specific amino acid. These amino acids are then linked together to form proteins, which carry out a vast array of functions within the cell. While this process provides a foundational framework, it is a dramatic over-simplification. As we’ll explore later in the course, the correlation between a gene and its corresponding protein levels is often surprisingly low, highlighting the complexity of gene expression regulation.\nUnderstanding proteins is crucial because they are the primary effectors of cellular function. Most cellular activities — whether structural, enzymatic, or signaling—are mediated by proteins. While RNA intermediates, such as mRNA, play important roles in carrying genetic information, the majority of RNA fragments never leave the cell, with a few exceptions like extracellular RNA in communication. Proteins, however, directly influence both intracellular processes and extracellular interactions. Given the weak correlation between genes and proteins and the central role proteins play in biological function, studying proteins arguably provides a more direct and meaningful insight into cellular and organismal behavior. This dual perspective on the central dogma will frame much of our exploration in this course.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#review-of-the-central-dogma",
    "href": "chapter4_protein.html#review-of-the-central-dogma",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "Figure 5.1: What I call the ``AP Biology textbook’’ figure. Screenshot taken from https://www.slideshare.net/slideshow/lecture-on-dna-to-proteins-the-central-dogma-of-molecular-biology/38811421.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: From (buccitelli2020mrnas?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: From (buccitelli2020mrnas?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#other-ways-to-study-proteins-that-were-not-going-to-discuss-here",
    "href": "chapter4_protein.html#other-ways-to-study-proteins-that-were-not-going-to-discuss-here",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "5.2 Other ways to study proteins that we’re not going to discuss here",
    "text": "5.2 Other ways to study proteins that we’re not going to discuss here\n\n5.2.1 So You Heard About AlphaFold…\nAlphaFold (see Figure 5.4) represents a revolutionary advancement in computational biology, designed to predict the three-dimensional structure of proteins from their amino acid sequences2. Historically, determining protein shapes required experimental techniques like X-ray crystallography, cryo-electron microscopy (EM) (see Figure 5.5), or nuclear magnetic resonance (NMR), which are resource-intensive and time-consuming. AlphaFold uses deep learning and structural biology insights to achieve high accuracy.\nHowever, significant challenges remain. There are still many open questions on how specific genetic modifications impact protein folding, how proteins dynamically change their conformation, or how they interact with other molecules such as DNA or other proteins. Additionally, ongoing developments in using large language models are showing promise in predicting not only shape but also potential functions directly from amino acid sequences.\n\n\n\n\n\n\n\nFigure 5.4: From (jumper2021highly?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: From https://myscope.training/CRYO_Introducing_Single_Particle_Analysis, as an example of what cryo-EM data “looks like,” just to give you a brief glimpse on how people study protein structure.\n\n\n\n\n\n\n5.2.2 Other Methods: Flow Cytometry, Spatial Proteomics, and FISH\nWhile AlphaFold focuses on protein structure, methods like flow cytometry and spatial proteomics explore proteins in their functional and cellular contexts. Flow cytometry, sometimes considered the “original” single-cell data method, measures the expression of surface and intracellular proteins across thousands of cells, providing rich insights into cellular heterogeneity. Spatial proteomics and techniques like fluorescence in situ hybridization (FISH) take this further by localizing proteins and RNA within tissue contexts, enabling researchers to map molecular interactions in their native environments. These approaches highlight the versatility of protein studies, from understanding their structure to dissecting their function and distribution in complex systems. While not the focus of this course, these methods are invaluable in expanding our understanding of proteins and their roles in biology.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter4_protein.html#footnotes",
    "href": "chapter4_protein.html#footnotes",
    "title": "5  ‘Single-cell’ proteomics",
    "section": "",
    "text": "Proteins typically degrade much slower than mRNA fragments. See https://book.bionumbers.org/how-fast-do-rnas-and-proteins-degrade. For this reason, you might hypothesize that “cellular memory” is stored via proteins, not mRNA.↩︎\nSee https://www.youtube.com/watch?v=P_fHJIYENdI for a fun YouTube video for more about this.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>'Single-cell' proteomics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html",
    "href": "chapter5_epigenetics.html",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "6.1 Primer on the genome, epigenetics, and enhancers\nThe genome is the complete set of DNA within an organism, encoding the instructions for life. While every cell in an organism typically contains the same genome, different cells exhibit distinct phenotypes and functions. This diversity arises not from changes in the underlying DNA sequence but from epigenetic regulation—heritable modifications that influence gene expression without altering the DNA itself. Epigenetics includes processes like DNA methylation, histone modification, and chromatin accessibility, all of which contribute to the dynamic regulation of gene activity in response to developmental cues and environmental signals.\nEnhancers play a critical role in this regulatory landscape. These are DNA sequences that, while not coding for proteins themselves, can dramatically increase the transcription of target genes. Enhancers act by binding specific transcription factors, proteins that recognize and attach to DNA sequences to regulate gene expression. Some transcription factors require assistance from chaperone proteins, which ensure their proper folding and functionality, or pioneer proteins, which can access and open tightly packed chromatin to allow other factors to bind. This interplay highlights the complexity of the regulatory machinery that governs cellular function. See Figure 6.1 and Figure 6.2 to appreciate how complex this machinery is.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#primer-on-the-genome-epigenetics-and-enhancers",
    "href": "chapter5_epigenetics.html#primer-on-the-genome-epigenetics-and-enhancers",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "Figure 6.1: From Ito et al. (2022).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: From Claringbould and Zaugg (2021).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#the-zoo-of-epigenetic-modalities",
    "href": "chapter5_epigenetics.html#the-zoo-of-epigenetic-modalities",
    "title": "6  Single-cell epigenetics",
    "section": "6.2 The zoo of epigenetic modalities",
    "text": "6.2 The zoo of epigenetic modalities\nEpigenetics encompasses a vast array of molecular mechanisms that regulate gene expression without altering the underlying DNA sequence. These mechanisms include modifications to DNA, RNA, chromatin, and the spatial organization of the genome, collectively forming a complex regulatory landscape. Below are some of the key modalities studied in epigenetics:\n\nDNA Accessibility: Techniques like ATAC-seq and DNase-seq measure how accessible DNA is to transcription factors and other regulatory proteins. Accessible regions often overlap with promoters and enhancers, providing critical insights into gene regulation. (This is what we’ll focus on this chapter.)\nDNA Methylation1: This modification, typically at cytosines in CpG dinucleotides, is a key epigenetic mark associated with gene silencing. Tools like bisulfite sequencing are used to map methylation patterns across the genome, revealing their roles in development and disease. See Figure 6.3.\n\n\n\n\n\n\n\n\nFigure 6.3: From (gauba2021immunomodulation?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: From (bruggeman2008using?).\n\n\n\n\n\nHi-C and Genome Organization2: Hi-C measures chromatin interactions to reveal the three-dimensional structure of the genome. It uncovers features like topologically associating domains (TADs) and enhancer-promoter loops, which are crucial for understanding how spatial organization influences gene regulation. See Figure 6.5.\n\n\n\n\n\n\n\n\nFigure 6.5: From (ea2015contribution?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: From (liu2021seeing?).\n\n\n\n\n\nHistone Modifications3: Post-translational modifications, such as acetylation, methylation, and phosphorylation, occur on histone proteins and regulate chromatin structure. Techniques like ChIP-seq and the newer Cut & Tag method are used to map these modifications and their role in gene expression at the bulk level. See Figure 6.7.\n\n\n\n\n\n\n\n\nFigure 6.7: From https://opened.cuny.edu/courseware/lesson/684/student/?section=2.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.8: From (steinbach2017pten?). (Note: This is an example of the “canonical” functions of common histone modifications. This is by no means exhaustive and guaranteed to hold true for all biological systems.)\n\n\n\n\nSome of you might be interested: Personally, I think one of the fascinating concepts based on histone modifications is bivalent chromatin (essentially, chromatin that is wrapped in such a way that is simultaneously activated and silenced), see Figure 6.9. It’s a particularly curious phenomenon that entire labs dedicate themselves to studying. See (blanco2020bivalent?) for an overview why this mechanism might be “beneficial.”\n\n\n\n\n\n\n\nFigure 6.9: From (macrae2023regulation?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.10: From (glancy2024bivalent?).\n\n\n\n\n\nRNA Modifications (m6A and Pseudouridine): Modifications like N6-methyladenosine (m6A) and pseudouridine occur on RNA molecules and are involved in processes like splicing, translation, and mRNA decay. See Figure 6.11 for what pseudouridine is, i.e., “a rotation of the uridine molecule.” These modifications add an epitranscriptomic layer to gene regulation.\n\n\n\n\n\n\n\n\nFigure 6.11: From (hamma2006pseudouridine?) about pseudouridine.\n\n\n\n\n\nUntranslated Regions (UTRs): Before talking about UTRs, it’s probably good to review what an mRNA fragment “looks like” at the different stages of transcription and translation, see Figure 6.12. The 5′ and 3′ UTRs contain regulatory elements that influence mRNA stability, localization, and translation efficiency. The 3′ UTR, in particular, serves as a binding platform for RNA-binding proteins and microRNAs, providing an additional layer of post-transcriptional gene regulation.\n\n\n\n\n\n\n\n\nFigure 6.12: From (niazi2023rna?). Notice the splicing (i.e., “removal” of the introns – see Alternative Splicing below) and that the protein-coding region is not the only part of the mRNA transcript – there are also the UTRs and poly-A tail. NOTE: The transcription start site (TSS) is not the same place as the promoter. The promoter is usually an un-transcribed region upstream of the TSS, while the TSS is where transcription actually starts.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.13: From (mignone2002untranslated?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.14: From https://www.cambio.co.uk/9/257/applications/methods/in-vitro-transcription/mrna-therapeutics/.\n\n\n\n\n\nAlternative Splicing: This post-transcriptional process generates multiple mRNA isoforms from the same gene, expanding the proteomic diversity, see Figure 6.15. This is a combinatorial explosion of isoforms. While not traditionally classified as epigenetic, splicing often intersects with chromatin modifications and RNA-binding proteins, blurring the lines between transcriptional and post-transcriptional regulation.\n\n\n\n\n\n\n\n\nFigure 6.15: From (chen2015alternative?).\n\n\n\n\n\nAlternative Polyadenylation (APA): APA generates transcript isoforms with different 3′ ends, affecting mRNA stability, localization, and translation, see Figure 6.16. By selecting distinct cleavage sites, APA alters the 3′ UTR length without affecting the poly-A tail itself, thereby modulating interactions with regulatory factors such as microRNAs and RNA-binding proteins.\n\n\n\n\n\n\n\n\nFigure 6.16: From Li et al. (2022).\n\n\n\n\nEfforts like the ENCODE project (https://pmc.ncbi.nlm.nih.gov/articles/PMC7061942/) have systematically mapped these modalities across cell types and tissues, creating a comprehensive resource for understanding genome function. ENCODE has provided invaluable datasets on DNA accessibility, histone modifications, and RNA-binding proteins, enabling researchers to uncover how epigenetic and transcriptomic layers work together to drive cellular processes.\nFinally, tools like MPRA (Massively Parallel Reporter Assays) are revolutionizing how we study enhancers and regulatory sequences. MPRA allows researchers to test thousands of DNA fragments for their regulatory activity, providing functional validation for epigenetic marks. This growing zoo of modalities continues to expand our understanding of how the genome is dynamically regulated in health and disease.\nSee Figure 6.17 for some courageous figures that try to display multiple types of epigenetic modifications all at once. See (lim2024advances?) for a broad overview about the different types of technologies to sequence different omics and layers of epigenetics, and how they are getting computationally put together.\n\n\n\n\n\n\n\nFigure 6.17: From https://www.sc-best-practices.org/chromatin_accessibility/introduction.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.18: From https://www.whatisepigenetics.com/type-2-diabetes-mellitus-and-epigenetics.\n\n\n\n\n\n\n\n\nClaringbould, Annique, and Judith B Zaugg. 2021. “Enhancers in Disease: Molecular Basis and Emerging Treatment Strategies.” Trends in Molecular Medicine 27 (11): 1060–73.\n\n\nIto, Shinsuke, Nando Dulal Das, Takashi Umehara, and Haruhiko Koseki. 2022. “Factors and Mechanisms That Influence Chromatin-Mediated Enhancer–Promoter Interactions and Transcriptional Regulation.” Cancers 14 (21): 5404.\n\n\nLi, Lei, Yumei Li, Xudong Zou, Fuduan Peng, Ya Cui, Eric J Wagner, and Wei Li. 2022. “Population-Scale Genetic Control of Alternative Polyadenylation and Its Association with Human Diseases.” Quantitative Biology 10 (1): 44–54.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter5_epigenetics.html#footnotes",
    "href": "chapter5_epigenetics.html#footnotes",
    "title": "6  Single-cell epigenetics",
    "section": "",
    "text": "The technology to measure this at single-cell resolution is still being developed (see snmC-seq2 (liu2021dna?)), and is particularly interesting due to methylation’s relation to cellular memory (kim2017dna?) and molecular clocks (hernando2019ageing?); (trapp2021profiling?); (gabbutt2022fluctuating?).↩︎\nSee Droplet-HiC (chang2024droplet?) for an example of single-cell Hi-C.↩︎\nSee Paired-tag (zhu2021joint?) for an example of single-cell histone modification.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Single-cell epigenetics</span>"
    ]
  },
  {
    "objectID": "chapter6_dna.html",
    "href": "chapter6_dna.html",
    "title": "7  ‘Single-cell’ DNA",
    "section": "",
    "text": "7.1 Genetics 101\nUnderstanding the fundamental concepts of genetics is essential for studying genomic variation, including copy-number variations (CNVs) and single nucleotide polymorphisms (SNPs). This section provides an overview of genetic architecture, SNPs and their detection, commonly sequenced tissues, and genome annotation resources such as the UCSC Genome Browser.\nSingle Nucleotide Polymorphisms (SNPs) and Their Detection.\nA single nucleotide polymorphism (SNP) is a variation at a single base pair position in the genome that is present in a significant fraction of the population. SNPs are the most common type of genetic variation and can have functional consequences depending on their location. When an SNP occurs within a coding region, it may alter the resulting protein sequence if it leads to an amino acid substitution (nonsynonymous SNP) or have no effect if the change is synonymous. SNPs in noncoding regions can impact gene regulation by affecting transcription factor binding sites, splicing efficiency, or untranslated regions (UTRs). Since you have “two copies” of each of your 23 chromosomes, this means SNP data is a data matrix of \\(n\\) people by \\(p\\) SNP regions (think of a couple million – more on this technicality later), where each value is \\(\\{0,1,2\\}\\), see Figure 7.1. Typically, the major allele is defined as “0”, and a “1” or “2” means if how many copies of the minor allele do you have.\nSNPs are detected using high-throughput sequencing technologies, primarily whole-genome sequencing (WGS) and whole-exome sequencing (WES). In these approaches, DNA is extracted from a biological sample, fragmented, and sequenced to generate short or long reads. The raw sequencing reads are then aligned to a reference genome, and variant calling algorithms such as those implemented in GATK (mckenna2010genome?), bcftools, and FreeBayes (garrison2012haplotype?) identify SNPs by comparing observed nucleotide differences to the reference sequence. (See (zverinova2022variant?) for a overview). The sequencing depth, or coverage, at a given genomic position determines the confidence in an SNP call, with higher coverage reducing the likelihood of sequencing errors.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>'Single-cell' DNA</span>"
    ]
  },
  {
    "objectID": "chapter6_dna.html#sec-genetics_basics",
    "href": "chapter6_dna.html#sec-genetics_basics",
    "title": "7  ‘Single-cell’ DNA",
    "section": "",
    "text": "Figure 7.1: From https://www.genome.gov/about-genomics/educational-resources/fact-sheets/human-genomic-variation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>'Single-cell' DNA</span>"
    ]
  },
  {
    "objectID": "chapter7_crispr.html",
    "href": "chapter7_crispr.html",
    "title": "8  Single-cell CRISPR editting",
    "section": "",
    "text": "8.1 Basics of how CRISPR works\nThe CRISPR-Cas system1 is a powerful genome-editing technology that allows for precise modifications of DNA in a wide range of biological systems. Originally derived from the bacterial adaptive immune system, CRISPR-Cas9 has been repurposed for genetic engineering by using a guide RNA (gRNA) to direct the Cas9 nuclease to a specific genomic locus for targeted DNA cleavage. This section discusses how CRISPR is performed in the wet lab, different functional applications of CRISPR, and how CRISPR-based perturbations are analyzed in single-cell gene expression studies.\nIn a typical CRISPR screen experiment (mainly a CRISPR knockout), a library of lentivirus-packaged guide RNAs is introduced into cells under conditions designed to infect each cell with only one or a few sgRNAs (single guide RNA), see Figure 8.1 and Figure 8.2. After selection to ensure stable integration, the cells are subjected to a particular stimulus such as drug treatment or other environmental challenge. Researchers then track the abundance of each sgRNA at the start and after the stimulus (for example, at day 0 and day 28) through next-generation sequencing. By comparing which sgRNAs become enriched or depleted, it is possible to discover genes essential for viability, pathways governing drug resistance, or other critical biological functions relevant to the phenotype under study, see Figure 8.3.\nTypical components of a CRISPR mechanism.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single-cell CRISPR editting</span>"
    ]
  },
  {
    "objectID": "chapter7_crispr.html#sec-chapter_7_basics",
    "href": "chapter7_crispr.html#sec-chapter_7_basics",
    "title": "8  Single-cell CRISPR editting",
    "section": "",
    "text": "Figure 8.1: Original from Wei et al. (2019), but this is directly from https://www.youtube.com/watch?v=JdCCl1uxCME.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: From https://www.idtdna.com/pages/education/decoded/article/overview-what-is-crispr-screening.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: From (esposito2019hacking?).\n\n\n\n\n\n\nCas9 protein: A DNA endonuclease that recognizes and cuts target DNA specified by the guide RNA; variations include nuclease-deactivated (dCas9) or nickase Cas9 for alternative applications. Other CRISPR-associated proteins like Cas12a (Cpf1), Cas13, or dCas9 (dead Cas9) can expand the range of target sequences, have different PAM requirements, or allow for gene regulation without cutting DNA.\nCas9 may bind sites with partial sequence complementarity, resulting in unintended cuts — this is called “off-target effects.” Various strategies (e.g., high-fidelity Cas9 variants, improved gRNA design) help minimize these effects.\nGuide RNA (gRNA): A customizable RNA sequence that directs Cas9 to the desired genomic locus; variations include single-guide RNA (sgRNA) and dual-RNA formats depending on the experimental design.\nTo streamline CRISPR-Cas9 applications in genetic engineering, scientists have designed a synthetic fusion of crRNA and tracrRNA into a single-guide RNA (sgRNA). This chimeric RNA retains both the target-specific recognition (crRNA component) and Cas9-binding function (tracrRNA component) but simplifies the system by reducing the number of necessary molecules. sgRNAs are commonly used in research and therapeutic applications due to their ease of design and efficiency in genome editing.\n\ncrRNA (CRISPR RNA): A short RNA sequence that is complementary to the target DNA and provides sequence specificity for Cas9 binding. This is typically 20 nucleotides long.\n\ntracrRNA (Trans-activating CRISPR RNA): A structural RNA that base-pairs with the crRNA and interacts with Cas9 to activate its nuclease function.\nIn this system, the crRNA and tracrRNA must form a duplex to guide Cas9 to the target sequence, which then leads to DNA cleavage.\nMultiple guide RNAs can be delivered simultaneously to target different loci at once, enabling complex genome-scale screens or combinatorial gene perturbations. Computational tools help optimize sgRNA sequences to maximize on-target efficiency while minimizing off-target effects. Chemically modified guide RNAs can improve stability and efficiency in vivo.\n\nPAM (Protospacer Adjacent Motif): A short DNA sequence adjacent to the target region that is essential for Cas9 to recognize and bind to DNA. The PAM sequence is typically “NGG” (where “N” represents any nucleotide, and “GG” is required). Without the correct PAM sequence, Cas9 cannot efficiently bind or cut the DNA, ensuring some level of specificity in genome editing. Different Cas9 variants have evolved or been engineered to recognize alternative PAM sequences, which offer broader targeting possibilities with reduced off-target effects. The cut typically happens 3 base pairs upstream of the PAM on the target strand (i.e., in the 5′ direction).\nLentivirus: Lentiviruses are widely used as viral vectors for delivering CRISPR components into cells (variations include different promoters, packaging systems, and envelope proteins to optimize transduction efficiency), particularly for experiments requiring stable and long-term expression of Cas9 and guide RNAs. Since many cell types are difficult to transfect using conventional methods, lentiviral transduction provides an efficient way to introduce CRISPR machinery into a broad range of cell types, including non-dividing and primary cells.\nWhat is delivered to cells? (See Figure 8.4–Figure 8.6 for the construction and architecture of the lentiviral vector.)\n\nCas9 expression construct: A lentiviral vector encoding the Cas9 nuclease under a suitable promoter. In some systems, inducible promoters (e.g., doxycycline-inducible) are used to control Cas9 activity.\n\nGuide RNA (gRNA) expression construct: A separate lentiviral vector encoding the guide RNA sequence under a promoter to ensure efficient transcription.\n\nSelection markers: Often, antibiotic resistance genes or fluorescent markers are included to facilitate selection of successfully transduced cells.\n\nLentiviral delivery is essential for CRISPR applications due to its ability to stably integrate into the host genome, ensuring long-term expression of Cas9 and guide RNAs. This stability is particularly advantageous for genome-wide CRISPR knockout screens and lineage tracing experiments. Additionally, many cell types, such as primary cells, stem cells, and immune cells, are notoriously difficult to transfect2; lentiviral transduction provides a more efficient and reliable alternative. See Figure 8.7–Figure 8.8 for how the gRNA can be injected into the cell, or if it’s integrated into the DNA, how it “borrows” the cell’s transcriptional machinery.\n\n\n\n\n\n\n\n\nFigure 8.4: Top: From (khan2022crispr?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: Bottom left: https://bpsbioscience.com/pd-1-crispr-cas9-lentivirus-integrating-78052.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6: Bottom right: From (mao2017heritability?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.7: Left: https://www.benchling.com/blog/how-to-synthesize-your-grnas-for-crispr.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.8: Right: https://bpsbioscience.com/lentiviruses?product_type_filter=5567.\n\n\n\n\n\n\n\n\nWei, Lai, Derek Lee, Cheuk-Ting Law, Misty Shuo Zhang, Jialing Shen, Don Wai-Ching Chin, Allen Zhang, et al. 2019. “Genome-Wide CRISPR/Cas9 Library Screening Identified PHGDH as a Critical Driver for Sorafenib Resistance in HCC.” Nature Communications 10 (1): 4681.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single-cell CRISPR editting</span>"
    ]
  },
  {
    "objectID": "chapter7_crispr.html#footnotes",
    "href": "chapter7_crispr.html#footnotes",
    "title": "8  Single-cell CRISPR editting",
    "section": "",
    "text": "CRISPR stands for “clustered regularly interspaced short palindromic repeats,” but it’s not too important to know why exactly it’s called this for the purposes of this chapter.↩︎\nTransfection is the process of introducing foreign nucleic acids (such as DNA or RNA) into eukaryotic cells to manipulate gene expression or enable genetic modifications. It is commonly used in plasmid-based CRISPR applications. In opposition, transduction refers to the introduction of genetic material using viral vectors.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single-cell CRISPR editting</span>"
    ]
  },
  {
    "objectID": "chapter8_lineage.html",
    "href": "chapter8_lineage.html",
    "title": "9  Single-cell lineage tracing",
    "section": "",
    "text": "9.1 Why are we interested in learning temporal dynamics?\nStudying how cells change over time provides critical insight into the sequence of events driving biological processes. By observing changes in cell populations across multiple time points, researchers can pinpoint when specific transitions or bifurcations occur. Such temporal information reveals which factors influence a cell’s fate and how quickly new traits emerge.\nMoreover, understanding temporal dynamics can help us develop better interventions. If we can identify the earliest signs of disease or undesirable changes in cells, then targeted therapies can be designed to prevent or slow progression. Such strategies are especially powerful for complex, multi-stage diseases where later intervention might be less effective.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Single-cell lineage tracing</span>"
    ]
  },
  {
    "objectID": "chapter8_lineage.html#why-are-we-interested-in-learning-temporal-dynamics",
    "href": "chapter8_lineage.html#why-are-we-interested-in-learning-temporal-dynamics",
    "title": "9  Single-cell lineage tracing",
    "section": "",
    "text": "Cancer subclone evolution: Tumors consist of various subclones that compete and evolve. Observing which subclones become dominant over time helps illuminate how certain cells acquire and propagate new mutations, sometimes conferring resistance to treatments. See Figure 9.1 and Figure 9.2.\n\n\n\n\n\n\n\n\nFigure 9.1: (Top) From (ashouri2023decoding?).\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: (Bottom) From (marine2020non?).\n\n\n\n\n\nCancer metastasis: Cells that detach from the primary tumor site and successfully colonize new tissues undergo significant genetic and phenotypic changes. Understanding these changes in temporal sequence highlights the adaptations needed for invasion and survival in distant environments. See Figure 9.3.\n\n\n\n\n\n\n\n\nFigure 9.3: From (fu2023emerging?).\n\n\n\n\n\nDisease progression: Many diseases advance in stages, even beyond cancer, with cells accumulating subtle changes that eventually manifest as severe pathologies. Timing the acquisition of these changes reveals how early molecular events cascade into full-blown disease. See Figure 9.4 for an example of this investigated in COVID.\n\n\n\n\n\n\n\n\nFigure 9.4: From (stephenson2021single?). You can see that at different stages of COVID, there is a slightly different proportion of cell types that constitute the immune system.\n\n\n\n\n\nEmbryonic/organ development and cell fate: During embryonic development, cells undergo a series of tightly regulated fate decisions that determine their final identity. These decisions are influenced by both intrinsic genetic programs and extrinsic signaling cues from the surrounding environment. Understanding the temporal dynamics of these transitions allows researchers to uncover the molecular mechanisms guiding differentiation and tissue formation. By tracking how cells commit to specific lineages, we gain insight into how organs form or how “cells make decisions”. See Figure 9.5.\n\n\n\n\n\n\n\n\nFigure 9.5: From (weinreb2020lineage?). This dataset is recorded longitudinally (i.e., cells were sequenced at Day 2, 4, and 6.) What’s shown here are cells known to be in different lineages, but certain lineages don’t differentiate by Day 6, differentiate into multiple cell types, or differentiate only into one cell type.\n\n\n\n\n\nStem cell research: Stem cells differentiate into specialized cell types following a tightly regulated timeline. Tracking these progressions uncovers the signals that guide each step and may inform regenerative medicine strategies for repairing damaged tissues. See Figure 9.6.\n\n\n\n\n\n\n\n\nFigure 9.6: From https://stemcellthailand.org/induced-pluripotent-stem-cells-ips-ipscs-hipscs/.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Single-cell lineage tracing</span>"
    ]
  },
  {
    "objectID": "chapter9_spatial.html",
    "href": "chapter9_spatial.html",
    "title": "10  ‘Single-cell’ spatial transcriptomics",
    "section": "",
    "text": "Single-cell spatial transcriptomics is an emerging technology that enables the measurement of gene expression while preserving the spatial organization of cells within a tissue. See Figure 10.1 for a useful schematic I like. Unlike traditional single-cell RNA sequencing, which dissociates cells and loses spatial context, spatial transcriptomics allows researchers to analyze gene expression in relation to cellular neighborhoods, tissue architecture, and microenvironments, see Figure 10.2.\n\n\n\n\n\n\n\nFigure 10.1: From https://www.mscience.com.au/supplier/10x-genomics/.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: From (kim2023single?).\n\n\n\n\nThe spatial arrangement of cells is fundamental to understanding cell biology, as cellular functions are often influenced by their local microenvironment:\n\nCancer: The tumor microenvironment plays a crucial role in disease progression, immune evasion, and therapeutic response. Understanding where specific cell populations reside within a tumor and how they interact with stromal and immune cells can reveal mechanisms of resistance and potential therapeutic targets.\nDevelopmental biology: Spatial positioning dictates lineage specification and organ formation, where disruptions in cell placement can lead to congenital defects. We can also overlay the gene expression at each spatial location with other imaging technologies or our current understanding of the structure of an organ, see Figure 10.3. This can help us learn how the cells in different regions of an organ have different functions, even if they are all the “same cell type.”\n\n\n\n\n\n\n\n\nFigure 10.3: From .\n\n\n\n\n\nDisease: In neurodegenerative diseases such as Alzheimer’s, the spatial distribution of microglia and astrocytes based on their (spatial) proximity to pathological hallmarks like amyloid plaques and tau tangles provides critical insights into disease mechanisms.\n\nAs one concrete example of how spatial transcriptomics advances cell biology beyond what other sequencing technologies offers: spatial transcriptomics is particularly valuable for studying cell-cell communication, as it enables the identification of ligand-receptor interactions that mediate signaling between neighboring cells. By integrating spatial data with single-cell transcriptomics, researchers can infer functional relationships between different cell types, uncovering regulatory networks that drive biological processes.\nNote: Spatial information in cell biology has been studied for decades, long before single-cell sequencing became commercially feasible. Techniques like fluorescence in situ hybridization (FISH) have been used to visualize the spatial localization of specific RNA molecules within cells, providing crucial insights into gene expression patterns. Microscopy-based methods, including confocal and super-resolution imaging, have also been instrumental in understanding cellular structures and interactions. The key novelty of modern spatial transcriptomics is its ability to scale these analyses – rather than measuring just a few genes at a time, we can now capture the spatial expression patterns of hundreds to thousands of genes simultaneously, enabling a much more comprehensive view of cellular organization and function.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>'Single-cell' spatial transcriptomics</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-role-of-normalization-to-adjust-for-sequencing-depth",
    "href": "chapter2_sequencing.html#the-role-of-normalization-to-adjust-for-sequencing-depth",
    "title": "3  Single-cell sequencing",
    "section": "3.2 The role of normalization to adjust for sequencing depth",
    "text": "3.2 The role of normalization to adjust for sequencing depth\nNormalization is a critical step in the analysis of single-cell sequencing data, addressing the technical variability introduced by differences in sequencing depth across cells. Sequencing depth refers to the total number of reads obtained for each cell, which can vary due to technical factors like library preparation or instrument sensitivity. Without normalization, cells with higher sequencing depth might appear to express more genes simply because of greater read coverage, not biological differences. Normalization methods aim to make gene expression counts comparable across cells by adjusting for these differences, ensuring that downstream analyses reflect true biological variation rather than technical artifacts. This step is foundational for accurate clustering, differential expression analysis, and other interpretative tasks in single-cell studies.\nIt has been known for a long time that when dealing with count data, proper normalization is foundational to properly doing meaningful inference. See (Risso et al. 2014). As we will see in Chapter 3, sequencing technologies cannot control how many reads (i.e., “counts”) are sequenced from each cell – this depends on the biochemistry efficency. Also, larger cells typically have more reads (which is itself a confounder) (Maden et al. 2023). We will describe normalization in more detail in Section 3.3.6.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  },
  {
    "objectID": "chapter2_sequencing.html#the-typical-workflow-for-any-sc-seq-data",
    "href": "chapter2_sequencing.html#the-typical-workflow-for-any-sc-seq-data",
    "title": "3  Single-cell sequencing",
    "section": "3.3 The typical workflow for any sc-seq data",
    "text": "3.3 The typical workflow for any sc-seq data\nWe will now describe the common steps at the start of a single-cell sequencing computational workflow (i.e., the steps that happen after the data has been collected and been aggregated into a matrix). More details about the downstream steps will be discussed in detail in Chapter 3. See (Luecken and Theis 2019; Heumos et al. 2023) for fantastic tutorials on best practices (among many). I’ll like to plug my own paper about best practices (Prater and Lin 2024).\n\n\n\n\n\n\n\nFigure 3.4: Abridged and typical workflow of a sc-seq dataset. (Luecken and Theis 2019)\n\n\n\n\nFor additional reference: See https://www.sc-best-practices.org/ and https://www.bigbioinformatics.org/intro-to-scrnaseq.\n\n3.3.1 (Optional) Ambient reads\n(This step will make more sense once you know a bit about how scRNA-seq data is generated, see Chapter 3.)\nAmbient reads and doublets (see Figure 3.5) are common technical artifacts in single-cell sequencing data that can distort biological interpretations if left unaddressed. Ambient reads arise from background RNA that is captured during sequencing but does not originate from the cell being analyzed. These reads can falsely inflate gene expression levels, particularly for highly abundant transcripts. Detecting and mitigating these artifacts is an optional but valuable step in data preprocessing, as it improves the overall quality and biological relevance of downstream analyses.\n\n\n\n\n\n\n\nFigure 3.5: Cartoon illustrating either ambient RNA or doublets. From https://www.10xgenomics.com/analysis-guides/introduction-to-ambient-rna-correction.\n\n\n\n\n\nInput/Output. The input to ambient detection is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes), and the output is \\(X'\\in \\mathbb{Z}_+^{n'\\times p}\\) where \\(X'_{ij} \\leq X_{ij}\\) for all cell \\(i\\) and feature \\(j\\).\n\n\n\n3.3.2 How to do this in practice\nThis is not always done in a standard analysis of 10x scRNA-seq data because the CellRanger pipeline does a pretty good job for you already by default. However, for certain finicky biological systems that deviate from the norm, you might purposely not use CellRanger’s default ambient detection and doublet detection, and instead choose to manually perform your own so that you have more control. In that case, the main ambient RNA detection method I’ve seen used is SoupX (Young and Behjati 2020).\n\n\n3.3.3 Cell filtering (or doublet detection)\nCell filtering is a crucial preprocessing step in single-cell sequencing analysis, aimed at removing low-quality or irrelevant cells to ensure reliable downstream analyses. During the sequencing process, some cells may produce insufficient data due to poor capture efficiency, leading to low gene counts or incomplete profiles. Additionally, some “cells” may actually be empty droplets or doublets. Doublets occur when two cells are captured together in the same droplet or well, leading to mixed gene expression profiles that do not represent any single cell. Filtering typically involves setting thresholds on metrics like the total number of detected genes, the fraction of mitochondrial gene reads (a marker of stressed or dying cells), and overall sequencing depth. By carefully selecting cells that meet quality standards, researchers can reduce noise, improve the robustness of analyses, and focus on biologically meaningful signals. This step helps ensure that the dataset represents a true and interpretable snapshot of cellular diversity.\n(Typically, ambient RNA means you’re trying to subtract “background” counts from your scRNA-seq matrix. The number of cells remains the same. In contrast, cell filtering is removing cells, typically geared to target cells deemed to be empty droplets or doublets.)\n\nInput/Output. The input to cell filtering is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) genes, and the output is \\(X'\\in \\mathbb{Z}_+^{n'\\times p}\\) where \\(n' \\leq n\\), whose rows are a strict subset of those in \\(X\\).\n\n\n\n3.3.4 How to do this in practice\nThis is commonly using the subset function in R, where we commonly filter based on nFeature_RNA (i.e., the number of genes that are expressed in a cell) and percent.mt (i.e., the fraction of counts that are originating from mitochondrial genes). The latter is usually computed using Seurat::PercentageFeatureSet3. See Figure 3.6 for an illustration. There is no commonly used threshold for these filters, but it typically depends on removing the extreme quantiles in your dataset. If this simple filter isn’t sophisticated enough to detect doublets (and you have good biological/technical reasons to suspect doublets, typically due to either the cell isolation or droplet formulation step), the doublet detection method I’ve seen used is DoubletFinder (McGinnis, Murrow, and Gartner 2019).\nNote: In Seurat, the cells are denoted as columns and features (such as genes) as rows. This is the transpose of the typical statistical notation.\n\n\n\n\n\n\n\nFigure 3.6: The filtering in the Seurat tutorial dataset goes from 2700 cells to 2638 cells. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\nWe usually filter based on nFeature_RNA because cells with too few genes are potentially deemed as empty droplets, and cells with too many genes are potentially deemed as doublets. High percentage of gene expression from mitochondrial genes is typically an indicator of poor sample quality4.\n\n\n3.3.5 A brief note on other approaches\nSee (Xi and Li 2021) for a benchmarking of doublet detection methods.\n\n\n3.3.6 Normalization\nNormalization is a key step in single-cell sequencing analysis, designed to adjust raw data so that gene expression measurements are comparable across cells. Variability in sequencing depth and technical artifacts can cause discrepancies in the total number of reads captured per cell, making raw counts unsuitable for direct comparison. Normalization methods address this by scaling or transforming the data to account for these differences, ensuring that observed expression levels more accurately reflect true biological variation. Approaches can range from simple scaling based on total counts to more sophisticated strategies that model the underlying distribution of the data. Normalization not only improves the accuracy of downstream analyses, such as clustering and differential expression, but also enhances the interpretability of results by reducing technical noise.\nTypically, the normalization has the following form:\n\\[\nX_{ij} \\leftarrow \\log\\Big(\\frac{10,000 \\cdot X_{ij}}{\\sum_{j'=1}^{p}X_{ij'}}+1\n\\Big).\n\\tag{3.3}\\]\nThe fraction \\(X_{ij}/\\sum_{j'=1}^{p}X_{ij'}\\) allows us to model the relative proportions (instead of absolute counts) of gene expression in each cell. The \\(\\log(\\cdot)\\) is to handle the right-skewed nature of the data (since an entry of \\(X_{ij}\\) is still 0 even after computing the fraction, and taking a fraction doesn’t adjust for the skewed nature by itself). The \\(+1\\) is handle the fact that we cannot take \\(\\log(0)\\). The only arbitrary thing in the factor of 10,000 – it’s simply for convenience to scale all the values (remember, \\(\\log(AB) = \\log(A) + \\log(B)\\), so essentially, this normalization is shifting all the fractions up by \\(\\log(10,000)\\)). The number of 10,000 (or sometimes 1 million) became commonplace because we can now interpret the number \\(10,000 \\cdot (X_{ij}/\\sum_{j'=1}^{p}X_{ij'})\\) as the hypothetical number of counts we would’ve gotten in cell \\(i\\) for gene \\(j\\) if the sequencing depth of cell \\(i\\) were 10,000.\n\n\n3.3.7 How to do this in practice\nSee the Seurat::LogNormalize function, illustrated in Figure 3.7.\n\n\n\n\n\n\n\nFigure 3.7: The normalization step in the Seurat tutorial. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\nInput/Output. The input to normalization is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes), and the output is \\(X'\\in \\mathbb{R}^{n\\times p}\\).\n\n\n\n3.3.8 A brief note on other approaches\nThere’s actually a lot of normalization methods since normalizing count data has existed ever since bulk-sequencing. However, three notable mentions are: SCTransform (Hafemeister and Satija 2019) which uses a NB GLM model to normalize data and GLM-PCA (Townes et al. 2019) which uses a GLM matrix factorization to adjust for sequencing depth, where both methods use adjust using the observed sequencing depth for each cell. Deep-learning methods like scVI (Lopez et al. 2018) include the library size as a latent variable to be estimated itself. See (Lause, Berens, and Kobak 2021; Ahlmann-Eltze and Huber 2023) for benchmarkings.\n\nRemark (Personal opinion: The log-transformation and lack of the negative binomial). It is well documented that this log-normalization is not best – see (Townes et al. 2019) and (Hafemeister and Satija 2019) for in-depth discussions. The issues stem from “discrete-ness” of the log transformation. However, surprisingly, as shown in papers such as (Ahlmann-Eltze and Huber 2023), the log-transformation is actually quite robust.\n\n\n\n3.3.9 Feature selection\nFeature selection is a critical step in single-cell sequencing analysis that focuses on identifying the most informative genes for downstream analyses. Single-cell datasets often include thousands of genes, many of which may be uninformative due to low variability or consistent expression across all cells. By narrowing the focus to a subset of highly variable genes (HVGs), feature selection reduces noise, enhances computational efficiency, and highlights the genes most likely to drive biological differences. These selected features are used in clustering, dimensionality reduction, and other tasks where capturing meaningful variation is essential. Effective feature selection ensures that the resulting analyses are both interpretable and biologically relevant.\n\nInput/Output. The input to feature selection is a count matrix \\(X\\in \\mathbb{Z}_+^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes)5, and the output is \\(X'\\in \\mathbb{R}^{n\\times p'}\\) where \\(p' \\leq p\\), whose columns are a strict subset of those in \\(X\\).\n\n\n\n3.3.10 The standard procedure: Variance Stabilizing Transformation\nThe vst (variance stabilizing transformation) procedure is a commonly used method in single-cell RNA-seq analysis for identifying highly variable genes. It is designed to account for the relationship between a gene’s mean expression and its variability, ensuring that variability is measured in a way that is independent of mean expression levels. The procedure involves the following steps:\n\nFitting the Mean-Variance Relationship:\n\nFirst, the relationship between the log-transformed variance and log-transformed mean of gene expression values is modeled using local polynomial regression, commonly referred to as loess. This step captures the expected variance for a given mean expression level. Let \\(\\mu_j\\) denote the mean expression of gene \\(j\\) and \\(\\sigma_j^2\\) its variance. The loess fit provides the expected variance, \\(\\hat{\\sigma}_j^2\\), as a smooth function of \\(\\log(\\mu_j)\\).\n\nStandardizing Gene Expression Values:\n\nThe observed expression values for each gene are then standardized to account for the expected variance: \\[\nZ_{ij} = \\frac{X_{ij} - \\mu_j}{\\sqrt{\\hat{\\sigma}_j^2}},\n\\] where \\(X_{ij}\\) is the observed expression value for gene \\(j\\) in cell \\(i\\), \\(\\mu_j\\) is the observed mean for gene \\(j\\), and \\(\\hat{\\sigma}_j^2\\) is the expected variance given by the fitted loess line. This standardization ensures that variability is measured relative to what is expected for genes with similar mean expression levels.\n\nClipping and Variance Calculation: To prevent outliers from dominating the analysis, the standardized values \\(Z_{ij}\\) are clipped to a maximum value, determined by a parameter such as clip.max. The variance of each gene is then calculated based on the clipped standardized values. Genes with the highest variance are selected as highly variable and prioritized for downstream analyses, such as clustering and trajectory inference.\n\nThis procedure addresses the inherent bias where genes with higher mean expression tend to exhibit greater variance, even if this variance is not biologically meaningful. By standardizing variability against the expected mean-variance relationship, the vst method provides a robust approach to identifying genes that truly capture biological heterogeneity across cells.\n\nRemark (Feature selection, agnostic of the rest of the analysis). For the statistical students, feature selection might seem a bit odd. After all, we’re selecting the genes to use in our analysis (again, typically about 2000 genes among 30,000 genes) before we do any other modeling. This is unlike the Lasso, where the feature selection is done while we’re doing a downstream task (i.e., regression in this case).\nIn a scRNA-seq analysis, we typically use the HVGs upfront, and use only these genes for all remaining downstream analyses. This is mainly for pragmatic reasons – most genes are 0’s in almost all the cells, so there’s no need to involve these genes in any of our downstream analysis.\n\n\n\n3.3.11 How to do this in practice\nSee the Seurat::FindVariableFeatures function, illustrated in Figure 3.8.\n\n\n\n\n\n\n\nFigure 3.8: The feature selection step in the Seurat tutorial, which selects the highly variable 2000 genes (from the 13,714 genes sequenced). See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n3.3.12 A brief note on other approaches\nSee (Zhao et al. 2024) for an overview of feature selection methods. This is a interesting statistical modeling question in the sense that the goal for these methods is often how to distinguish between “technical variance” (i.e., variation attributed to sequencing depth) from “biological variance” (i.e., if a gene has different expressions across cells in the tissue).\n\n\n3.3.13 (Optional) Imputation\nImputation refers to methods used to infer missing or undetected values in single-cell sequencing data, particularly addressing the zeros that dominate scRNA-seq datasets. Early in the development of single-cell technologies, these zeros were often attributed to “dropouts,” where lowly expressed genes failed to be captured due to technical limitations. Zero-inflated models were developed to distinguish between “true” biological zeros – where a gene is genuinely not expressed – and dropout zeros, filling in the latter to provide a more complete representation of gene expression. However, with advancements like unique molecular identifiers (UMIs), scRNA-seq data have become more reliable and less prone to dropout artifacts. As a result, the need for imputation has diminished, and modern workflows often bypass it entirely, favoring raw or minimally processed data that more accurately reflect true biological variation. Imputation remains an optional step, typically reserved for specific analyses where reconstructing missing data is critical. See (Hou et al. 2020) for a review of many methods in this category.\nSee (Kharchenko, Silberstein, and Scadden 2014) for the landmark paper that originally discussed dropouts. However, see ?rem-zero-inflation for why these “dropouts” are no longer common to worry about in your single-cell analysis.\n\n\n3.3.14 How to do this in practice\nI’ve see MAGIC (Markov Affinity-based Graph Imputation of Cells) (Van Dijk et al. 2018) used quite often, if there’s a data imputation being performed. This method imputes missing gene expression values by leveraging similarities among cells using a graph-based approach. MAGIC constructs a nearest-neighbor graph where cells are nodes, and edges represent biological similarity. Through data diffusion, information is shared across similar cells to denoise and recover underlying gene expression patterns. See Figure 3.9.\n\n\n\n\n\n\n\nFigure 3.9: Schematic of the MAGIC method (Van Dijk et al. 2018).\n\n\n\n\n\n\n3.3.15 Dimension reduction\nDimension reduction, particularly methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), serves as a cornerstone in single-cell sequencing analysis. The primary goal is twofold: first, to obtain a low-dimensional representation of the data, which is critical for visualizing complex datasets and enabling computationally efficient downstream analyses such as clustering and trajectory inference. By condensing thousands of genes into a smaller number of principal components, researchers can focus on the dominant patterns of variation that drive biological differences. Second, dimension reduction helps denoise the data by filtering out technical noise and minor variations that are less biologically relevant. This makes the resulting analysis more robust and interpretable, allowing researchers to capture meaningful cellular heterogeneity while mitigating the impact of sparsity and noise inherent to single-cell datasets.\n\nInput/Output. The input to dimension reduction is a matrix \\(X\\in \\mathbb{R}^{n\\times p}\\) for \\(n\\) cells and \\(p\\) features (i.e., genes). The output is \\(Z \\in \\mathbb{R}^{n\\times k}\\) where \\(k \\ll p\\) (where there are \\(k\\) latent dimensions, typically between 5 to 50). We call \\(Z\\) the score matrix. Most dimension reduction (typically only linear ones – i.e., ones based on a matrix factorization) also return: (1) gene loading matrix \\(W \\in \\mathbb{R}^{p \\times k}\\), and (2) “denoised” matrix \\(X'\\in \\mathbb{R}^{n\\times p}\\), which is some form of \\(X' = ZW^\\top\\).\n\n\n\n3.3.16 The standard procedure: PCA\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique widely used in single-cell RNA-seq analysis to simplify complex datasets while retaining the most important patterns of variation. By transforming high-dimensional gene expression data into a smaller set of principal components, PCA captures the dominant trends in the data, such as differences between cell types or states. This reduced representation helps to mitigate the noise and sparsity inherent in single-cell data, making downstream tasks like clustering, trajectory inference, and visualization more efficient and interpretable. PCA serves as a foundational step in many single-cell workflows, offering both computational efficiency and biological insight.\n\n\n\n\n\n\n\nFigure 3.10: Illustration of the PCA of a \\(p=2\\)-dimensional dataset reduced to one dimension (\\(K=1\\)). (Left) Showing the maximization or minimization perspective of PCA.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.11: Showing how to interpret PCA as a matrix approximation. Here, the “component” is \\(\\hat{W}\\), and the “loading” is \\(\\hat{Z}=X\\hat{W}\\).\n\n\n\n\nI’ll describe the PCA in a bit more detail here since it’s can be a bit confusing.\n\nTheorem (Equivalent formulations of PCA). Let \\(X \\in \\mathbb{R}^{n\\times p}\\) be a centered matrix ( i.e., \\(\\overline{X}_{\\cdot j} = 0\\) for all \\(j\\in\\{1,\\ldots,p\\}\\)) and \\(K\\) be a pre-chosen latent dimensionality (such that \\(k \\ll p\\)). Let \\(\\hat{\\Sigma} = X^\\top X/n\\) denote the empircial covariance matrix.\nThe following four estimated embeddings \\(\\hat{Z} \\in \\mathbb{R}^{n\\times k}\\) are “equivalent.”\n\nMaximization of information captured by the covariance (shown in Figure 3.10 (left)): Define \\(\\hat{Z} = X\\hat{W}\\) where \\[\n\\hat{W} = \\argmax_{W \\in \\mathbb{R}^{p\\times K}}\\tr\\Big(W^\\top \\hat{\\Sigma} W\\Big),\n\\quad \\text{such that} \\quad W^\\top W = I_K.\n\\tag{3.4}\\]\nMinimization of reconstruction error (via squared Euclidean loss) (shown in Figure 3.10 (left)): Define \\(\\hat{Z} = X \\hat{W}\\) where \\[\n\\hat{W} = \\argmin_{W \\in \\mathbb{R}^{p\\times K}}\\frac{1}{n}\\sum_{i=1}^{n}\\Big\\|X_{i\\cdot} - WW^\\top X_{i\\cdot}\\Big\\|^2_F\n\\quad \\text{such that} \\quad W^\\top W = I_K.\n\\tag{3.5}\\]\nLow-rank approximation of the interpoint distances (via squared Euclidean distances): Let \\(D_X \\in \\mathbb{R}^{n\\times n}_+\\) denote the matrix of squared Euclidean distances among all \\(n\\) samples in \\(X\\), i.e., \\([D_X]_{i_1,i_2} = \\|X_{i_1,\\cdot} - X_{i_2,\\cdot}\\|^2_F\\) for all \\(i_1,i_2 \\in \\{1,\\ldots,n\\}\\). (This is a function of \\(X\\).) Then, construct \\(\\hat{Z}=X\\hat{W}\\) where \\[\n\\hat{W} = \\argmin_{W \\in \\mathbb{R}^{p\\times K}} \\sum_{i_1,i_2 \\in \\{1,\\ldots,n\\}}  \\big[D_X\\big]_{i_1,i_2} - \\big[D_{XW}\\big]_{i_1,i_2} \\quad \\text{such that} \\quad W^\\top W = I_K,\n\\tag{3.6}\\] and \\(D_{XW}  \\in \\mathbb{R}^{n\\times n}_+\\) is the matrix of squared Euclidean distances constructed from \\(XW\\).\n\n\nNotice that Formulation #3 is simply the sum of differences (not squared) since projections are non-expansive, so the interpoint Euclidean distances necessarily decrease. We are not accurately describing the identifiability conditions above to formalize what “equivalence” technically means.\nSome remarks:\n\nRelation to eigen-decompositions: PCA is fundamentally tied to eigen-decompositions due to the Low-Rank Approximation theorem6 (more formally, the Eckart-Young-Mirsky theorem). Specifically, \\(\\hat{W}\\) is the eigenvalues of \\(\\hat{\\Sigma}\\), the covariance matrix. See Figure 3.11 (right).\n\n(If you are familiar with how to manipulate eigen-decompositions, you can convince yourself that Formulation 2 is most directly related to the Low-Rank Approximation.)\n\nInterpretation of Formulation #1: In Formulation 1, we want to project the data \\(X\\) onto a lower-dimensional subspace defined by \\(\\hat{W}\\) such the low-dimensional data \\(\\hat{Z}\\) has as much variance as possible. This is by far the most common explanation of PCA. Part of its appeal is that it’s easy to explain what the “population model’s target quantity” is through this interpretation, but 1) it requires you to intrinsically understand why preserving covariance is something useful when we’re more interested in about the cells themselves, and 2) does not give insight to the more popular extensions of PCA.\n\nAdjustable aspects to obtain other dimension-reduction methods: 1) Choosing a more suitable estimate of the population covariance \\(\\Sigma = \\mathbb{E}[X^\\top X]/n\\) (or its correlation counterpart), or 2) enforcing additional structure (such as sparsity) onto the columns of \\(\\hat{W}\\).\n\nInterpretation of Formulation #2: In Formulation 2, we want to project the data \\(X\\) onto a lower-dimensional subspace defined by \\(\\hat{W}\\) such that the ambient-dimensional data is as close to the original data as possible (i.e., the projection “disturbs” the data the least). This interpretation reveals that PCA is trying to find the subspace that “distorts” each cell’s expression the least. While this won’t be used to motivate more modern embedding methods, it 1) offers a formal relation between PCA and matrix denoising, and 2) is the motivation for autoencoders later on.\n\nAdjustable aspects to obtain other dimension-reduction methods: 1) How to measure the “error” between each cell’s expression and its reconstructed expression, 2) how this reconstruction is constructed given a cell’s low-dimensional representation?\n\nInterpretation of Formulation #3: In Formulation 3, we conceptually think about the cells’ interpoint distances (i.e., the distances between any two pairs of cells). Then, we want to project the data \\(X\\) onto a lower-dimensional subspace defined by \\(\\hat{W}\\) such the low-dimensional data \\(\\hat{Z}\\) preserves the distances between any two pairs of cells as much as possible. This perspective is perhaps the most intuitive, and also most revealing on the shortcomings of PCA. PCA is choosing to preserve the pairwise distances in a very specific fashion, via the squared Euclidean distances. There are a two aspects that reveal the deficiency of PCA:\n\n\nPCA is trying to preserve the squared Euclidean distance, so it’s going to put most of its attention on pairs of points that are very far away from one another.\nWe consider all pairs of points.\n\nThese two aspects combined yield the qualitative statement that “PCA favors a global embedding that preserve far-away points”.7 In some sense, this is the exact opposite of what we want – qualitatively, we probably care more about getting the relative distances of each points and its immediate/local neighbors (so we can accurately see the subtle shifts in cells, as in trajectory inference).\nAdjustable aspects to obtain other dimension-reduction methods: 1) How we’re measuring distance in \\(X\\) or in \\(Z\\) we’re using (perhaps using a distance that adapts to the geometry of the data), 2) how to measure the “distortion” in the pairwise distances, 3) which pairs of cells do we care about preserving their distances?\n\nReferences for the proofs: Relating one and two is mainly an exercise in matrix algebra. To show that Formulation #3 is also PCA, see Theorem 14.4.1 in (Kent, Bibby, and Mardia 1979).\n\n\nRemark (The words we use to call the columns of \\(\\hat{W}\\) after doing a linear-dimension reduction method like PCA). Note that \\(\\hat{W}\\) is a \\(p\\)-by-\\(K\\) matrix. That is, each column of \\(\\hat{W}\\) has one number for each feature (i.e., gene). We typically call each column one of a few things: “gene program” or “pathway” (both more biology-facing) or “eigen-gene” or “topic” (both more statistics-facing). Regardless, these words is to denote that each column of \\(\\hat{W}\\) denotes a set of genes that work together, based on the magnitude and sign that feature has in this column.\n\n\n\n3.3.17 How to do this in practice\nSee the Seurat::RunPCA function, illustrated in Figure 3.12. The resulting visualization is shown in Figure 3.13.\n\n\n\n\n\n\n\nFigure 3.12: The PCA step. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.13: Using the Seurat::DimPlot(pbmc, reduction = \"pca\") function to visualize the first two PCs. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n3.3.18 A brief note on other approaches\nThe distinction between “imputation” and “dimension reduction” gets murky because 1) computationally, many imputation methods rely on a dimension reduction, and 2) statistically, due to the low-rank approximation property of PCA, you can interpret many dimension reductions methods as an imputation method. When I say that “imputation is optional,” what I really mean is that modeling zero-inflation is typically optional.\nReviews of imputation methods such as (Hou et al. 2020) contain overview of dimension-reduction methods. eSVD (Lin, Qiu, and Roeder 2024) and GLM-PCA (Townes et al. 2019) are dimension reduction methods that leverage the fact that the NB is in the exponential family (among many). FastTopics (Carbonetto et al. 2021) is a popular non-negative matrix factorization method (among many) for modeling single-cell data. (We won’t have time to discuss non-negative matrix factorization in this course.) All these mentioned methods are able to perform normalization with dimension reduction simultaneously. Also, dimension-reductions are commonly folded into a method that aims to do multiple tasks simultaneously (for example, see scINSIGHT (Qian et al. 2022), among many examples). See (Ghojogh et al. 2021) for a massive review of common dimension-reduction frameworks.\nHowever, by far the most popular method nowadays is using VAEs, since it is easy to combine dimension reduction with other downstream tasks. We’ll discuss VAEs in more detail in ?sec-vae.\n\nRemark (Personal opinion: Considerations for picking an appropriate dimension-reduction method). The categorizations of “linear” vs. “non-linear” is a bit blurry. Personally, I would call methods like eSVD (Lin, Qiu, and Roeder 2024) and GLM-PCA (Townes et al. 2019) as “linear” (in addition to PCA and fastTopics (Carbonetto et al. 2021)) since you estimate a matrix factorization and there’s a fixed transformation that relates the observed count \\(X_{ij}\\) to the inner product between a cell’s latent vector \\(Z_{i,\\cdot}\\) and a gene’s latent vector \\(W_{j,\\cdot}\\).\nTypically, people refer to non-linear dimension reductions as methods that do not use a matrix factorization (think of diffusion maps (Haghverdi, Buettner, and Theis 2015) or deep-learning methods (Lopez et al. 2018)). Since non-linear methods can capture more complex relations than linear methods, what are some considerations you should make to pick a particular method?\n\nMean-variance relation: PCA is intimately connected to modeling the data with constant Gaussian noise (i.e., every entry of \\(X_{ij}\\) is observed with the same amount of Gaussian error (Tipping and Bishop 1999). However, you might want to use a method where you expect the amount of variability in the data increases as the mean increases (such as a Poisson distribution). See (Lin, Qiu, and Roeder 2022) for more details.\nGene programs: Methods that do not use a matrix factorization often do not give you a built-in way to interpret collections of genes. In contrast, after using a matrix factorization approach like PCA, you can interpret how genes are coordinating through the estimated matrix \\(\\hat{W}\\).\n“Distance” between cells: As you saw in Formulation #3 Equation 3.6, PCA cares about the Euclidean distance between any two cells. Sometimes, such as in a diffusion map (see Palantir for instance (Setty et al. 2019)), you have in mind a more appropriate way to measure how “different” two cells are.\n\n(Notice that by itself, the fact that “non-linear methods can capture non-linear structure” is not really a good enough reason to use a non-linear method. This is because this statement itself is somewhat vacuous. By the low-rank approximation theorem (https://en.wikipedia.org/wiki/Low-rank_approximation), as long as you use enough latent dimensions, any linear method can also capture non-linear structures. The only advantage of non-linear methods in this setting is that the non-linear method can represent this non-linear structure with “less numbers” (i.e., an information-compression perspective).)\nIn general, there is never a “strictly better” option. Every reasonable method has its own niche on what kind of {technology, experiment, data, biological-question} setting it outshines other competitors. Your job as a thoughtful researcher is to: 1) understand the conceptual differences between all these methods, and 2) have a concrete strategy on how you would make an informed decision on which method you want to apply.\n\n\n\n3.3.19 Batch correction/Covariate adjustment\nBatch correction is a vital step in single-cell sequencing analysis that addresses technical variability introduced by differences in experimental conditions, such as processing time, reagent batches, or sequencing runs. These batch effects can obscure true biological signals and create artificial differences between groups of cells. Batch correction methods aim to align data from different batches while preserving meaningful biological variation. Techniques range from simple scaling adjustments to advanced algorithms that model batch effects explicitly. As single-cell studies increasingly combine datasets from multiple experiments or institutions, effective batch correction ensures that comparisons and integrative analyses reflect biological reality rather than technical artifacts. See ?fig-batch1 for an illustration of this.\nWe will talk about this a bit more when we discuss integration methods for cell-type labeling ?sec-celltype_labeling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIllustration of what the goal of batch correction is. All these are UMAPs (see Section 3.3.23). The top row colors cells by their batch, and the bottom row colors cells by cell type. (Left) Original data showing batch effects. (Middle) Intermediate processing. (Right) The batch-corrected data. (Tran et al. 2020)\n\n\nInput/Output. The input to batch correction is (1) either the normalized matrix \\(X \\in \\mathbb{R}^{n\\times p}\\) or the score matrix \\(Z \\in \\mathbb{R}^{n\\times k}\\) and (2) an categorical vector \\(C \\in \\{1,\\ldots,B\\}^n\\) which denotes which cell belongs to which batch. (Certain batch correction methods might require more information.) In a literal sense, both \\(X\\) and \\(Z\\) are technically row-wise concatenations of many datasets.\n\nIf the input is a normalized matrix, the output is a batch-corrected normalized matrix \\(X'   \\in \\mathbb{R}^{n\\times p}\\). If the input is a score matrix \\(Z\\), the output is a batch-corrected score matrix \\(Z'   \\in \\mathbb{R}^{n\\times k}\\).\n\nRemark (What makes this task statistically possible?). There are many sources of “noise” (i.e., things unrelated to the biology we’re trying to study) in a single-cell dataset.\n\nSequencing noise: This comes from the fact that we “randomly” get counts from each cell, and we can’t control how effective each cell is at producing counts. We can think of this as “mean-zero, i.i.d. across all cells” in some sense.\nBatch noise: This is the “noise” that originates from a particular batch. It is probably more statistically appropriate to think of this as a “bias,” because it is not mean-zero. Instead, we conceptually think of this as a shift occurring to all the cells in a batch more-or-less uniformly (but could impact different genes differently).\nBiological confounding noise: Say you want to study how lung tissue changes during aging, but some of your tissue samples come from donors who smoke. The aging biological signature you’re hoping to find would likely be confounded by a smoking signature. This “noise” is also not mean-zero, and it could be different across different cell types. However, given a cell type, this biological confounding effect should be the same across batches. (The trouble is that you might not be aware of all the confounders during an analysis.)\n\nHence, it is possible to “remove” batch effects because: 1) it’s a constant “shift” for all the cells in the same batch, and 2) we always know the “batches,” since it is purely an artifact of how data is generated. Most batch-correction papers are statistically motivated, but do not have formal theoretical properties. This is primarily due to the fact that in practice, it’s very hard to know how to model a batch effect rigorously. However, there are a few papers that try to leverage this intuition formally in a statistical framework, see (Ma et al. 2024).\n\n\nRemark (The term “batch” is very overloaded). There are many steps during a single-cell analysis (see Chapter 3). These are some of the things a “batch” could refer to: 1) cells processed together during droplet/gem formation, 2) cells processed during during PCR amplification, 3) cells processed together during sequencing (this is most common usage of the word “batch”), 4) cells processed using different protocols, 5) cells processed by different labs, 6) cells of different donors/organisms, among many more. (The first three steps are similar in “resolution,” but the last three are more “coarse” in how a batch is defined.)\nTechnically, the statistical logic of most batch-correction methods isn’t suitable for trying to align cells from different donors/organisms since in this setting, the “batches” are based on biology, not a technical artifact. For example, the differences between cells from two donors could be much more complicated than simply cells in two different sequencing batches.\nIn general, be sure to discuss with the people who generated the data (who likely will have a lot more experience on what technical artifacts are worth worrying about) on how to think about batch effects.\n\n\n\n3.3.20 What’s the difference between “batch-correcting” and “adjusting for covariates”?\nTechnically, there are some batch-correction methods whose premise is to treat each batch as simply a covariate to be adjusted for. The primary example is COMBAT (Zhang, Parmigiani, and Johnson 2020). However, most batch-correction methods do not do this. There is no formal reason why. I personally feel it’s because batch effects is such a critical bottleneck for almost every single-cell analysis that it’s imperative to use a method that is as (sensibly) flexible as possible. See my remark later in Remark 3.2. See (Tran et al. 2020; Luecken et al. 2022) for benchmarking multiple batch correction methods.\n\n\n3.3.21 A typical choice: Harmony\nHarmony (Korsunsky et al. 2019) is one of the most commonly used methods to integrate single-cell datasets from different experiments, technologies, or biological conditions into a shared low-dimensional embedding. The method begins with a principal component analysis (PCA) embedding and iteratively adjusts it to remove batch effects while preserving biological variation. Harmony groups cells into clusters using soft clustering, which assigns cells probabilistically to multiple clusters. These clusters account for technical and biological differences while capturing shared cell types and states. Correction factors are then computed for each cluster and applied to individual cells, ensuring that the final embedding reflects intrinsic cellular phenotypes without being confounded by dataset-specific biases. See Figure 3.14.\n\n\n\n\n\n\n\nFigure 3.14: Schematic of the Harmony (Korsunsky et al. 2019).\n\n\n\n\n\n\n3.3.22 How to do this in practice\nThere is no standardize default for batch correction. The usual practice is to: 1) decide upfront (with your collaborators) on how you would deem what is a “successful” batch correction. This usually involves understanding the data generation protocol and the biology question of interest. This will involve a blend of biological logic (i.e., statements that should be true, given the biological context), qualitative evaluations (i.e., based on plots), and numerical metrics (i.e., based on some score where you decide upfront how the score relates to a desirable batch correction). Then, 2) you try many batch correction methods8.\n\nRemark 3.2. Remark (Personal opinion: Batch correction is single-handedly the most precarious step in almost every analysis). While a cautious statistician might be doubtful for “cherry-picking” a good batch-correction method, the reality is that there is no guaranteed procedure to always work. This is because there are so many types of “batches,” and the underlying data generation pipeline, technology, and biology differs from lab to lab. Also, doing a “bad” batch correction is extremely detrimental. Unlike many other steps in a single-cell analysis where most choices of how to perform a step are “reasonable” (but some methods might have slightly better power or slightly more robust), a bad batch correction can single-handedly invalidate almost all other aspects of a single-cell analysis regardless of how sophisticated those methods are.\nAlso: batch correction is limited by the experimental design. No “real” way to validate this if you’re only given datasets – the validations need to be decided during the planning of the experimental design. For example, if you are studying cancer, and you put all the cancer donors’ cells in one batch and all the healthy donors’ cells in another batch, then you have no ability to disentangle the batch effects from the biological effect.\nTo aid validation of batch correction, a common experimental design is to put one reference tissue (where it’s possible to obtain many samples from this one tissue) in every sequencing batch.\n\n\n\n3.3.23 Visualization\nVisualization is a key step in single-cell sequencing analysis, offering a way to explore and interpret complex datasets intuitively. One of the most popular visualization techniques is Uniform Manifold Approximation and Projection (UMAP), which creates a low-dimensional embedding designed to preserve local and global structure in the data. UMAP is particularly effective for capturing non-linear relationships and revealing clusters of similar cells, making it a powerful tool for visualizing cellular heterogeneity. However, UMAP coordinates are less reliable than those obtained through methods like PCA for quantitative analysis. Unlike PCA, which is grounded in linear transformations and retains a direct link to the original data, UMAP embeddings are highly dependent on parameter choices and random initializations, making them less reproducible. Despite these limitations, UMAP excels as a visualization tool, helping researchers intuitively explore relationships between cells and identify patterns that guide deeper analyses.\n\nInput/Output. The input to visualization is typically the score matrix \\(Z\\in \\mathbb{R}^{n\\times k}\\) for \\(n\\) cells and \\(k\\) latent dimensions, and the output is matrix with \\(n\\) cells and 2 (sometimes 3) columns, where the visualization is literally plotting the values in the first column against the second column as a scatterplot.\n\n\n\n3.3.24 The standard procedure: UMAP\nUniform Manifold Approximation and Projection (UMAP) (Becht et al. 2019) is a dimensionality reduction technique designed to capture both local and global structure in high-dimensional data. UMAP builds on principles of manifold learning, aiming to preserve the topology of the original data in a lower-dimensional space.\n\nConstructing the high-dimensional graph: UMAP begins by modeling the local neighborhood relationships in the high-dimensional space:\n\n\nFor each data point \\(i\\), a probability distribution \\(p_{ij}\\) is constructed over its neighbors \\(j\\) based on distances, typically using a kernel function.\nThe resulting graph captures the high-dimensional structure of the data.\n\n\nOptimizing the low-dimensional embedding: In the low-dimensional space, UMAP aims to construct a graph with similar relationships. It models pairwise probabilities \\(q_{ij}\\) for the low-dimensional embedding using a smooth, differentiable cost function: \\[\nC = \\sum_{i,j} \\left( - p_{ij} \\log q_{ij} - (1 - p_{ij}) \\log (1 - q_{ij}) \\right).\n\\] Here:\n\n\n\\(p_{ij}\\) and \\(q_{ij}\\) represent probabilities of connectivity in the high- and low-dimensional spaces, respectively.\nThe first term encourages similar points in the high-dimensional space to remain close, while the second term discourages unrelated points from being artificially clustered.\n\n\nGradient descent: The low-dimensional coordinates are iteratively optimized using gradient descent to minimize \\(C\\), aligning the local relationships in the low-dimensional embedding with those in the original space.\n\nUMAP excels at preserving local neighborhoods while maintaining some global structure, making it particularly effective for visualizing high-dimensional single-cell RNA-seq data. However, the resulting embeddings depend on hyperparameters (e.g., number of neighbors, minimum distance) and are not designed for quantitative analysis, as they do not preserve metric properties of the original data. See https://github.com/jlmelville/uwot for great in-depth discussions on UMAPs.\n\nRemark (Yes, UMAPs are also a “dimension reduction,” but they serve a very different purpose). You can think of UMAPs as a very specific dimension reduction where \\(k\\) is always 2 or 3. However, UMAPs serve a very different role – very few single-cell analyses relies on the UMAP coordinates, but many single-cell methods rely upon the low-dimensional vector \\(Z_{i,\\cdot}\\in\\mathbb{R}^k\\) from a dimension reduction method.\nThe reason is that UMAPs are usually solely for visualizations and nothing else. There is almost no concrete relation between the distance between any two cells based on their UMAP coordinates and their true dissimilarity based on their gene expression profiles. The most common analogy is to think of visualizing the Earth on a 2D map, see Figure 3.15. No matter how you try to draw the world on a sheet of paper, there’s always some unnatural distortion. If you interpret this map very literally, it seems like California is further away from Japan than Spain is, and Antartica is almost as large as all the other continents combined. The point is, every low-dimensional projection always sacrifices some aspects, and when you try projecting high-dimensional data to specifically only 2 dimensions, a lot of large-scale qualities “break.”\nSee (Prater and Lin 2024) for more discussions about how to think about UMAPs, Figure 3.16. There’s a big debate on what is “acceptable” or “not acceptable” to infer from UMAPS. See (Chari and Pachter 2023; Lause, Berens, and Kobak 2024), and a recent large controversy in genetics (https://www.science.org/content/article/huge-genome-study-confronted-concerns-over-race-analysis).\nMy take: UMAPs are a useful visualization tool (in the sense that it’ll be ridiculous to claim we should never visualize our data, and we should just accept every visualization tool will have its own drawbacks). However, you should have quantitative metrics planned out prior to visualization so you’re not solely relying on UMAPs for your analysis. UMAPs are useful for giving you a sense of the data quality and broad cellular relations before investing time to carefully quantify the biology of interest. Your biological evidence should not solely rely on UMAPs.\n\n\n\n\n\n\n\n\nFigure 3.15: Map of the world\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.16: (Top row): UMAP computed from 3 different random seeds, to illustrated the “arbitrary” nature of how large “continents” of cells relate to one another. (Bottom left): t-SNE, which often are a lot more “spread out” than UMAPs in terms of how separated the “continents” are. (Bottom right): PCA, which is reproducible (i.e., no unexplainable extrinsic randomness during the computation), statistically rigorous and transparent (i.e., we know how to mathematically interpret any pair of cells). However, unlike UMAP and t-SNE, we often need many PCs to appropriate represent the data, and we can only visualize 2 PCs at a time. (Prater and Lin 2024)\n\n\n\n\n\n\n3.3.25 Relation to PCA\nUMAPs are most related to the Formulation #3 of PCA Equation 3.6. Specifically, instead of “\\(D_X\\)” and “\\(D_{XW}\\)” (originally measuring the distance between two cells using Euclidean distance), UMAPs instead use an adaptive kernel on a cell-cell graph to measure the similarity between two cells, and the loss function isn’t simply a difference between the two similarities.\n\n\n3.3.26 How to do this in practice\nSee the Seurat::RunUMAP function, illustrated in Figure 3.17. The resulting visualization is shown in Figure 3.18.\n\n\n\n\n\n\n\nFigure 3.17: The UMAP visualization step. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.18: Using the Seurat::DimPlot(pbmc, reduction = \"umap\") function to visualize the first two PCs. See the tutorial in https://satijalab.org/seurat/articles/pbmc3k_tutorial.html.\n\n\n\n\n\n\n3.3.27 A brief note on other approaches\nBecause visualizing high-dimensional data will always be a need, and there’s never going to be an “optimal” way to do it, the visualization method of choice is going to change based on the community’s desires every 5-10 years. While currently UMAP is king (which dethroned t-SNE, who was the previous king), there’s definitely methods trying to dethrone UMAP. The method I’ve seen the most popularity to dethrone UMAP so far is PaCMAP, but as of 2024-25, UMAP is still by far the most common visualization method. See (Huang et al. 2022) for a broad benchmarking of such methods.\nThere are two lines of work I’ll mention that might be of interest for the statistical students. One is developing methods to assess how “distorted” the visualization is, see (Johnson, Kath, and Mani 2022; Xia, Lee, and Li 2024). Another is to trying to prove (using a formal statistical model) how well these visualization methods actually “cluster” the cells, see (Arora, Hu, and Kothari 2018; Cai and Ma 2022). ::::::::::::::::::::::::::::::::::::::::\n\n\n\n\nAhlmann-Eltze, Constantin, and Wolfgang Huber. 2023. “Comparison of Transformations for Single-Cell RNA-Seq Data.” Nature Methods, 1–8.\n\n\nArora, Sanjeev, Wei Hu, and Pravesh K Kothari. 2018. “An Analysis of the t-Sne Algorithm for Data Visualization.” In Conference on Learning Theory, 1455–62. PMLR.\n\n\nBecht, Etienne, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. 2019. “Dimensionality Reduction for Visualizing Single-Cell Data Using UMAP.” Nature Biotechnology 37 (1): 38.\n\n\nCai, T Tony, and Rong Ma. 2022. “Theoretical Foundations of t-Sne for Visualizing High-Dimensional Clustered Data.” Journal of Machine Learning Research 23 (301): 1–54.\n\n\nCarbonetto, Peter, Abhishek Sarkar, Zihao Wang, and Matthew Stephens. 2021. “Non-Negative Matrix Factorization Algorithms Greatly Improve Topic Model Fits.” arXiv Preprint arXiv:2105.13440.\n\n\nChari, Tara, and Lior Pachter. 2023. “The Specious Art of Single-Cell Genomics.” PLOS Computational Biology 19 (8): e1011288.\n\n\nChen, Wenan, Yan Li, John Easton, David Finkelstein, Gang Wu, and Xiang Chen. 2018. “UMI-Count Modeling and Differential Expression Analysis for Single-Cell RNA Sequencing.” Genome Biology 19 (1): 70.\n\n\nGhojogh, Benyamin, Ali Ghodsi, Fakhri Karray, and Mark Crowley. 2021. “Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey.” arXiv Preprint arXiv:2101.00734.\n\n\nHafemeister, Christoph, and Rahul Satija. 2019. “Normalization and Variance Stabilization of Single-Cell RNA-seq Data Using Regularized Negative Binomial Regression.” Genome Biology 20 (1): 1–15.\n\n\nHaghverdi, Laleh, Florian Buettner, and Fabian J Theis. 2015. “Diffusion Maps for High-Dimensional Single-Cell Analysis of Differentiation Data.” Bioinformatics 31 (18): 2989–98.\n\n\nHeumos, Lukas, Anna C Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D Lücken, et al. 2023. “Best Practices for Single-Cell Analysis Across Modalities.” Nature Reviews Genetics 24 (8): 550–72.\n\n\nHou, Wenpin, Zhicheng Ji, Hongkai Ji, and Stephanie C Hicks. 2020. “A Systematic Evaluation of Single-Cell RNA-Sequencing Imputation Methods.” Genome Biology 21 (1): 1–30.\n\n\nHuang, Haiyang, Yingfan Wang, Cynthia Rudin, and Edward P Browne. 2022. “Towards a Comprehensive Evaluation of Dimension Reduction Methods for Transcriptomic Data Visualization.” Communications Biology 5 (1): 719.\n\n\nJohnson, Eric M, William Kath, and Madhav Mani. 2022. “EMBEDR: Distinguishing Signal from Noise in Single-Cell Omics Data.” Patterns 3 (3).\n\n\nKent, JT, John Bibby, and KV Mardia. 1979. Multivariate Analysis. Academic Press Amsterdam.\n\n\nKharchenko, Peter V, Lev Silberstein, and David T Scadden. 2014. “Bayesian Approach to Single-Cell Differential Expression Analysis.” Nature Methods 11 (7): 740.\n\n\nKorsunsky, Ilya, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang, Kevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya Raychaudhuri. 2019. “Fast, Sensitive and Accurate Integration of Single-Cell Data with Harmony.” Nature Methods, 1–8.\n\n\nLause, Jan, Philipp Berens, and Dmitry Kobak. 2021. “Analytic Pearson Residuals for Normalization of Single-Cell RNA-Seq UMI Data.” Genome Biology 22: 1–20.\n\n\n———. 2024. “The Art of Seeing the Elephant in the Room: 2D Embeddings of Single-Cell Data Do Make Sense.” bioRxiv.\n\n\nLin, Kevin Z, Yixuan Qiu, and Kathryn Roeder. 2022. “eSVD: Cohort-Level Differential Expression in Multi-Individual Single-Cell RNA-Seq Data Using Exponential-Family Embeddings.” (In Preparation).\n\n\n———. 2024. “eSVD-DE: Cohort-Wide Differential Expression in Single-Cell RNA-Seq Data Using Exponential-Family Embeddings.” BMC Bioinformatics 25 (1): 113.\n\n\nLopez, Romain, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir Yosef. 2018. “Deep Generative Modeling for Single-Cell Transcriptomics.” Nature Methods 15 (12): 1053.\n\n\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.” Genome Biology 15 (12): 550.\n\n\nLuecken, Malte D, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese, Marta Interlandi, Michaela F Müller, Daniel C Strobl, et al. 2022. “Benchmarking Atlas-Level Data Integration in Single-Cell Genomics.” Nature Methods 19 (1): 41–50.\n\n\nLuecken, Malte D, and Fabian J Theis. 2019. “Current Best Practices in Single-Cell RNA-Seq Analysis: A Tutorial.” Molecular Systems Biology 15 (6): e8746.\n\n\nMa, Rong, Eric D Sun, David Donoho, and James Zou. 2024. “Principled and Interpretable Alignability Testing and Integration of Single-Cell Data.” Proceedings of the National Academy of Sciences 121 (10): e2313719121.\n\n\nMaden, Sean K, Sang Ho Kwon, Louise A Huuki-Myers, Leonardo Collado-Torres, Stephanie C Hicks, and Kristen R Maynard. 2023. “Challenges and Opportunities to Computationally Deconvolve Heterogeneous Tissue with Varying Cell Sizes Using Single-Cell RNA-Sequencing Datasets.” Genome Biology 24 (1): 288.\n\n\nMcGinnis, Christopher S, Lyndsay M Murrow, and Zev J Gartner. 2019. “DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing Data Using Artificial Nearest Neighbors.” Cell Systems 8 (4): 329–37.\n\n\nPrater, Katherine E, and Kevin Z Lin. 2024. “All the Single Cells: Single-Cell Transcriptomics/Epigenomics Experimental Design and Analysis Considerations for Glial Biologists.” Glia.\n\n\nQian, Kun, Shiwei Fu, Hongwei Li, and Wei Vivian Li. 2022. “scINSIGHT for Interpreting Single-Cell Gene Expression from Biologically Heterogeneous Data.” Genome Biology 23 (1): 1–23.\n\n\nRisso, Davide, John Ngai, Terence P Speed, and Sandrine Dudoit. 2014. “Normalization of RNA-Seq Data Using Factor Analysis of Control Genes or Samples.” Nature Biotechnology 32 (9): 896–902.\n\n\nSalzberg, Steven L. 2018. “Open Questions: How Many Genes Do We Have?” BMC Biology 16 (1): 94.\n\n\nSarkar, Abhishek, and Matthew Stephens. 2021. “Separating Measurement and Expression Models Clarifies Confusion in Single Cell RNA-Seq Analysis.” Nature Genetics 53 (6): 770–77.\n\n\nSaunders, Lauren M, Sanjay R Srivatsan, Madeleine Duran, Michael W Dorrity, Brent Ewing, Tor H Linbo, Jay Shendure, et al. 2023. “Embryo-Scale Reverse Genetics at Single-Cell Resolution.” Nature 623 (7988): 782–91.\n\n\nSetty, Manu, Vaidotas Kiseliovas, Jacob Levine, Adam Gayoso, Linas Mazutis, and Dana Pe’er. 2019. “Characterization of Cell Fate Probabilities in Single-Cell Data with Palantir.” Nature Biotechnology 37 (4): 451–60.\n\n\nTipping, Michael E, and Christopher M Bishop. 1999. “Probabilistic Principal Component Analysis.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61 (3): 611–22.\n\n\nTownes, F William, Stephanie C Hicks, Martin J Aryee, and Rafael A Irizarry. 2019. “Feature Selection and Dimension Reduction for Single-Cell RNA-seq Based on a Multinomial Model.” Genome Biology 20 (1): 1–16.\n\n\nTran, Hoa Thi Nhu, Kok Siong Ang, Marion Chevrier, Xiaomeng Zhang, Nicole Yee Shin Lee, Michelle Goh, and Jinmiao Chen. 2020. “A Benchmark of Batch-Effect Correction Methods for Single-Cell RNA Sequencing Data.” Genome Biology 21 (1): 1–32.\n\n\nVan Dijk, David, Roshan Sharma, Juozas Nainys, Kristina Yim, Pooja Kathail, Ambrose J Carr, Cassandra Burdziak, et al. 2018. “Recovering Gene Interactions from Single-Cell Data Using Data Diffusion.” Cell 174 (3): 716–29.\n\n\nXi, Nan Miles, and Jingyi Jessica Li. 2021. “Benchmarking Computational Doublet-Detection Methods for Single-Cell RNA Sequencing Data.” Cell Systems 12 (2): 176–94.\n\n\nXia, Lucy, Christy Lee, and Jingyi Jessica Li. 2024. “Statistical Method scDEED for Detecting Dubious 2D Single-Cell Embeddings and Optimizing t-SNE and UMAP Hyperparameters.” Nature Communications 15 (1): 1753.\n\n\nYazar, Seyhan, Jose Alquicira-Hernandez, Kristof Wing, Anne Senabouth, M Grace Gordon, Stacey Andersen, Qinyi Lu, et al. 2022. “Single-Cell eQTL Mapping Identifies Cell Type-Specific Genetic Control of Autoimmune Disease.” Science 376 (6589): eabf3041.\n\n\nYoung, Matthew D, and Sam Behjati. 2020. “SoupX Removes Ambient RNA Contamination from Droplet-Based Single-Cell RNA Sequencing Data.” Gigascience 9 (12): giaa151.\n\n\nZhang, Yuqing, Giovanni Parmigiani, and W Evan Johnson. 2020. “ComBat-Seq: Batch Effect Adjustment for RNA-Seq Count Data.” NAR Genomics and Bioinformatics 2 (3): lqaa078.\n\n\nZhao, Ruzhang, Jiuyao Lu, Weiqiang Zhou, Ni Zhao, and Hongkai Ji. 2024. “A Systematic Evaluation of Highly Variable Gene Selection Methods for Single-Cell RNA-Sequencing.” bioRxiv, 2024–08.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Single-cell sequencing</span>"
    ]
  }
]