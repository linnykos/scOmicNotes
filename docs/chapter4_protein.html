<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; ‘Single-cell’ proteomics – scOmicNotes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter5_epigenetics.html" rel="next">
<link href="./chapter3_rna.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-364982630eef5352dd1537128a8ed5cb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter4_protein.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">‘Single-cell’ proteomics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">scOmicNotes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2_sequencing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Single-cell sequencing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3_rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Single-cell RNA-sequencing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4_protein.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">‘Single-cell’ proteomics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5_epigenetics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Single-cell epigenetics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#review-of-the-central-dogma" id="toc-review-of-the-central-dogma" class="nav-link active" data-scroll-target="#review-of-the-central-dogma"><span class="header-section-number">5.1</span> Review of the central dogma</a></li>
  <li><a href="#other-ways-to-study-proteins-that-were-not-going-to-discuss-here" id="toc-other-ways-to-study-proteins-that-were-not-going-to-discuss-here" class="nav-link" data-scroll-target="#other-ways-to-study-proteins-that-were-not-going-to-discuss-here"><span class="header-section-number">5.2</span> Other ways to study proteins that we’re not going to discuss here</a>
  <ul>
  <li><a href="#so-you-heard-about-alphafold" id="toc-so-you-heard-about-alphafold" class="nav-link" data-scroll-target="#so-you-heard-about-alphafold"><span class="header-section-number">5.2.1</span> So You Heard About AlphaFold…</a></li>
  <li><a href="#other-methods-flow-cytometry-spatial-proteomics-and-fish" id="toc-other-methods-flow-cytometry-spatial-proteomics-and-fish" class="nav-link" data-scroll-target="#other-methods-flow-cytometry-spatial-proteomics-and-fish"><span class="header-section-number">5.2.2</span> Other Methods: Flow Cytometry, Spatial Proteomics, and FISH</a></li>
  </ul></li>
  <li><a href="#just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response" id="toc-just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response" class="nav-link" data-scroll-target="#just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response"><span class="header-section-number">5.3</span> Just a very brief blurb about the passive and adaptive immune response</a></li>
  <li><a href="#sec-cite-seq_tech" id="toc-sec-cite-seq_tech" class="nav-link" data-scroll-target="#sec-cite-seq_tech"><span class="header-section-number">5.4</span> CITE-seq: Sequencing mRNA alongside cell surface markers, simulteaneously</a>
  <ul>
  <li><a href="#a-brief-note-on-other-technologies" id="toc-a-brief-note-on-other-technologies" class="nav-link" data-scroll-target="#a-brief-note-on-other-technologies"><span class="header-section-number">5.4.0.1</span> A brief note on other technologies</a></li>
  </ul></li>
  <li><a href="#a-primer-on-vaes" id="toc-a-primer-on-vaes" class="nav-link" data-scroll-target="#a-primer-on-vaes"><span class="header-section-number">5.5</span> A Primer on VAEs</a>
  <ul>
  <li><a href="#what-is-an-autoencoder" id="toc-what-is-an-autoencoder" class="nav-link" data-scroll-target="#what-is-an-autoencoder"><span class="header-section-number">5.5.1</span> What is an autoencoder?</a></li>
  <li><a href="#sec-vae" id="toc-sec-vae" class="nav-link" data-scroll-target="#sec-vae"><span class="header-section-number">5.5.2</span> What is a variational autoencoder?</a></li>
  <li><a href="#architecture-for-library-size-and-overdispersion" id="toc-architecture-for-library-size-and-overdispersion" class="nav-link" data-scroll-target="#architecture-for-library-size-and-overdispersion"><span class="header-section-number">5.5.3</span> Architecture for library size and overdispersion</a></li>
  </ul></li>
  <li><a href="#multi-modal-integration-applicable-beyond-cite-seq" id="toc-multi-modal-integration-applicable-beyond-cite-seq" class="nav-link" data-scroll-target="#multi-modal-integration-applicable-beyond-cite-seq"><span class="header-section-number">5.6</span> Multi-modal integration (applicable beyond CITE-seq)</a>
  <ul>
  <li><a href="#sec-union_totalvi" id="toc-sec-union_totalvi" class="nav-link" data-scroll-target="#sec-union_totalvi"><span class="header-section-number">5.6.1</span> TotalVI (and other methods for the “union” of information)</a>
  <ul class="collapse">
  <li><a href="#the-totalvi-model-and-key-equations" id="toc-the-totalvi-model-and-key-equations" class="nav-link" data-scroll-target="#the-totalvi-model-and-key-equations"><span class="header-section-number">5.6.1.1</span> The TotalVI model and key equations</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">‘Single-cell’ proteomics</span></h1>
<p class="subtitle lead">Foray into deep learning and multi-omic integration</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="review-of-the-central-dogma" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="review-of-the-central-dogma"><span class="header-section-number">5.1</span> Review of the central dogma</h2>
<p>The central dogma of molecular biology outlines the flow of genetic information within a cell: DNA is transcribed into RNA, and RNA is then translated into protein (see <a href="#fig-ap-biology" class="quarto-xref">Figure&nbsp;<span>5.1</span></a>). Specifically, coding genes in DNA are transcribed into messenger RNA (mRNA), which serves as a template for protein synthesis. During translation, mRNA sequences are read in sets of three nucleotides, known as codons, each corresponding to a specific amino acid. These amino acids are then linked together to form proteins, which carry out a vast array of functions within the cell. While this process provides a foundational framework, it is a dramatic over-simplification. As we’ll explore later in the course, the correlation between a gene and its corresponding protein levels is often surprisingly low, highlighting the complexity of gene expression regulation.</p>
<div class="block">
<div id="fig-ap-biology" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ap-biology-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/ap-biology.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ap-biology-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: What I call the ``AP Biology textbook’’ figure. Screenshot taken from <a href="https://www.slideshare.net/slideshow/lecture-on-dna-to-proteins-the-central-dogma-of-molecular-biology/38811421" class="uri">https://www.slideshare.net/slideshow/lecture-on-dna-to-proteins-the-central-dogma-of-molecular-biology/38811421</a>.
</figcaption>
</figure>
</div>
</div>
<p>Understanding proteins is crucial because they are the primary effectors of cellular function. Most cellular activities — whether structural, enzymatic, or signaling—are mediated by proteins. While RNA intermediates, such as mRNA, play important roles in carrying genetic information, the majority of RNA fragments never leave the cell, with a few exceptions like extracellular RNA in communication. Proteins, however, directly influence both intracellular processes and extracellular interactions. Given the weak correlation between genes and proteins and the central role proteins play in biological function, studying proteins arguably provides a more direct and meaningful insight into cellular and organismal behavior. This dual perspective on the central dogma will frame much of our exploration in this course.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="block">
<div id="fig-gene-protein-correlation" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gene-protein-correlation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/gene-protein-correlation.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gene-protein-correlation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: From <span class="citation" data-cites="buccitelli2020mrnas">(<a href="references.html#ref-buccitelli2020mrnas" role="doc-biblioref"><strong>buccitelli2020mrnas?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-gene-expression-control" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gene-expression-control-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/gene-expression-control.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gene-expression-control-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: From <span class="citation" data-cites="buccitelli2020mrnas">(<a href="references.html#ref-buccitelli2020mrnas" role="doc-biblioref"><strong>buccitelli2020mrnas?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
</section>
<section id="other-ways-to-study-proteins-that-were-not-going-to-discuss-here" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="other-ways-to-study-proteins-that-were-not-going-to-discuss-here"><span class="header-section-number">5.2</span> Other ways to study proteins that we’re not going to discuss here</h2>
<section id="so-you-heard-about-alphafold" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="so-you-heard-about-alphafold"><span class="header-section-number">5.2.1</span> So You Heard About AlphaFold…</h3>
<p>AlphaFold (see <a href="#fig-alphafold" class="quarto-xref">Figure&nbsp;<span>5.4</span></a>) represents a revolutionary advancement in computational biology, designed to predict the three-dimensional structure of proteins from their amino acid sequences<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Historically, determining protein shapes required experimental techniques like X-ray crystallography, cryo-electron microscopy (EM) (see <a href="#fig-cryo-em" class="quarto-xref">Figure&nbsp;<span>5.5</span></a>), or nuclear magnetic resonance (NMR), which are resource-intensive and time-consuming. AlphaFold uses deep learning and structural biology insights to achieve high accuracy.<br>
However, significant challenges remain. There are still many open questions on how specific genetic modifications impact protein folding, how proteins dynamically change their conformation, or how they interact with other molecules such as DNA or other proteins. Additionally, ongoing developments in using large language models are showing promise in predicting not only shape but also potential functions directly from amino acid sequences.</p>
<div class="block">
<div id="fig-alphafold" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alphafold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/alphafold.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alphafold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: From <span class="citation" data-cites="jumper2021highly">(<a href="references.html#ref-jumper2021highly" role="doc-biblioref"><strong>jumper2021highly?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-cryo-em" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cryo-em-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/cryo-em.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cryo-em-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: From <a href="https://myscope.training/CRYO_Introducing_Single_Particle_Analysis" class="uri">https://myscope.training/CRYO_Introducing_Single_Particle_Analysis</a>, as an example of what cryo-EM data “looks like,” just to give you a brief glimpse on how people study protein structure.
</figcaption>
</figure>
</div>
</div>
</section>
<section id="other-methods-flow-cytometry-spatial-proteomics-and-fish" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="other-methods-flow-cytometry-spatial-proteomics-and-fish"><span class="header-section-number">5.2.2</span> Other Methods: Flow Cytometry, Spatial Proteomics, and FISH</h3>
<p>While AlphaFold focuses on protein structure, methods like flow cytometry and spatial proteomics explore proteins in their functional and cellular contexts. Flow cytometry, sometimes considered the “original” single-cell data method, measures the expression of surface and intracellular proteins across thousands of cells, providing rich insights into cellular heterogeneity. Spatial proteomics and techniques like fluorescence in situ hybridization (FISH) take this further by localizing proteins and RNA within tissue contexts, enabling researchers to map molecular interactions in their native environments. These approaches highlight the versatility of protein studies, from understanding their structure to dissecting their function and distribution in complex systems. While not the focus of this course, these methods are invaluable in expanding our understanding of proteins and their roles in biology.</p>
</section>
</section>
<section id="just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="just-a-very-brief-blurb-about-the-passive-and-adaptive-immune-response"><span class="header-section-number">5.3</span> Just a very brief blurb about the passive and adaptive immune response</h2>
<p>The immune system functions as a highly coordinated defense network, designed to detect foreign invaders such as pathogens and viruses, mount an appropriate response, and retain a memory of the invader for faster recognition in the future. This response involves two primary arms: the <strong>innate (passive) immune response</strong>, which provides a rapid but non-specific reaction, and the <strong>adaptive immune response</strong>, which is slower but highly specific and long-lasting (<a href="#fig-immune1" class="quarto-xref">Figure&nbsp;<span>5.6</span></a>). The innate immune system acts as the first line of defense, utilizing <strong>macrophages</strong> and <strong>dendritic cells</strong> to patrol tissues, engulf pathogens, and present antigens to activate the adaptive response. These antigen-presenting cells serve as surveillance officers, alerting the adaptive immune system to potential threats.</p>
<div class="block">
<div id="fig-immune1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-immune1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/immune1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-immune1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: From <span class="citation" data-cites="primorac2022adaptive">(<a href="references.html#ref-primorac2022adaptive" role="doc-biblioref"><strong>primorac2022adaptive?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<p>Once the adaptive immune system is activated, a complex cellular expansion process takes place. <strong>T-cells</strong> and <strong>B-cells</strong>, the key players of adaptive immunity, undergo rapid division to generate an army of pathogen-specific defenders (<a href="#fig-immune3-1" class="quarto-xref">Figure&nbsp;<span>5.7</span></a>; <a href="#fig-immune3-2" class="quarto-xref">Figure&nbsp;<span>5.8</span></a>). B-cells produce antibodies that neutralize the invader, while T-cells can either directly kill infected cells or help orchestrate the immune response. Once the threat is eliminated, the expanded immune cell population undergoes programmed cell death to prevent excessive immune activity, leaving behind a small number of <strong>memory cells</strong> that provide long-term immunity. This cycle of surveillance, expansion, and contraction resembles a law enforcement system that transitions from routine surveillance (innate immunity) to full military mobilization (adaptive immunity) when an invasion occurs, ensuring both immediate and long-term protection.</p>
<div class="block">
<div id="fig-immune3-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-immune3-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/immune3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-immune3-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: (Top): From <a href="https://www.youtube.com/watch?v=Kd-cTfVYwf8" class="uri">https://www.youtube.com/watch?v=Kd-cTfVYwf8</a>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-immune3-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-immune3-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/immune2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-immune3-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.8: (Bottom): From <span class="citation" data-cites="joseph2022trained">(<a href="references.html#ref-joseph2022trained" role="doc-biblioref"><strong>joseph2022trained?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
</section>
<section id="sec-cite-seq_tech" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-cite-seq_tech"><span class="header-section-number">5.4</span> CITE-seq: Sequencing mRNA alongside cell surface markers, simulteaneously</h2>
<p>CITE-seq (Cellular Indexing of Transcriptomes and Epitopes by sequencing) <span class="citation" data-cites="stoeckius2017simultaneous">(<a href="references.html#ref-stoeckius2017simultaneous" role="doc-biblioref"><strong>stoeckius2017simultaneous?</strong></a>)</span> extends the capabilities of single-cell RNA-seq by simultaneously measuring messenger RNA (mRNA) and surface protein expression in the same cell. This is achieved by combining traditional single-cell RNA-seq protocols with oligonucleotide-labeled antibodies that bind to specific surface proteins. These oligonucleotides are then sequenced alongside the mRNA, providing a comprehensive view of both transcriptional and protein-level information for each cell.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> See <a href="#fig-citeseq-validation" class="quarto-xref">Figure&nbsp;<span>5.9</span></a> for a comparison between the cell populations that are separable with flow cytometry verses the surface proteins in a CITE-seq dataset.</p>
<div class="block">
<div id="fig-citeseq-validation" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-citeseq-validation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/citeseq-validation.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-citeseq-validation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.9: From <span class="citation" data-cites="stoeckius2017simultaneous">(<a href="references.html#ref-stoeckius2017simultaneous" role="doc-biblioref"><strong>stoeckius2017simultaneous?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<p>First, some vocabulary: - <strong>Antibody and its epitope</strong>: An antibody is a Y-shaped protein produced by the immune system that binds specifically to an antigen, which is often a protein or peptide. The part of the antigen that the antibody recognizes and binds to is called the <em>epitope</em>. This interaction is highly specific, allowing antibodies to target unique molecular features of cells, pathogens, or other biological molecules. See (<a href="#fig-antibody-1" class="quarto-xref">Figure&nbsp;<span>5.10</span></a>; <a href="#fig-antibody-2" class="quarto-xref">Figure&nbsp;<span>5.11</span></a>; <a href="#fig-antibody-3" class="quarto-xref">Figure&nbsp;<span>5.12</span></a>).<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="block">
<div id="fig-antibody-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-antibody-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/antigen.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-antibody-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.10: (Top) From <a href="https://sciencenotes.org/antigen-definition-function-and-types" class="uri">https://sciencenotes.org/antigen-definition-function-and-types</a>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-antibody-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-antibody-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/antibody_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-antibody-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.11: (Bottom Left) From <a href="https://www.sigmaaldrich.com/US/en/technical-documents/technical-article/protein-biology/elisa/antigens-epitopes-antibodies" class="uri">https://www.sigmaaldrich.com/US/en/technical-documents/technical-article/protein-biology/elisa/antigens-epitopes-antibodies</a>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-antibody-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-antibody-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/antibody_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-antibody-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.12: (Bottom Right) From <span class="citation" data-cites="sanchez2017fundamentals">(<a href="references.html#ref-sanchez2017fundamentals" role="doc-biblioref"><strong>sanchez2017fundamentals?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<ul>
<li><strong>Surface markers (CDs)</strong>: Cell surface markers, often referred to as CD (cluster of differentiation) markers, are proteins expressed on the surface of cells that serve as distinguishing features of specific cell types or states. These markers are commonly used in immunology and single-cell analyses to classify and study immune cells. For example, CD4 and CD8 are well-known markers for helper T cells and cytotoxic T cells, respectively. The ability to target CD markers with labeled antibodies is central to techniques like flow cytometry and CITE-seq. See <a href="#fig-immune-cd" class="quarto-xref">Figure&nbsp;<span>5.13</span></a> for a table of common CDs used to identify immune cells.</li>
</ul>
<div class="block">
<div id="fig-immune-cd" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-immune-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/immune-cd.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-immune-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.13: From <a href="https://www.cellsignal.com/pathways/immune-cell-markers-human" class="uri">https://www.cellsignal.com/pathways/immune-cell-markers-human</a>.
</figcaption>
</figure>
</div>
</div>
<p>Here is a brief description of the CITE-seq steps (see <a href="#fig-citeseq-protocol" class="quarto-xref">Figure&nbsp;<span>5.14</span></a>):</p>
<ol type="1">
<li><p><strong>Tissue dissociation and cell isolation</strong>: This step remains identical to traditional single-cell RNA-seq. The tissue of interest is dissociated into a suspension of single cells, and cells are isolated via techniques like fluorescence-activated cell sorting (FACS) or microfluidics. The primary difference comes afterward, when specific oligonucleotide-conjugated antibodies are added to the cell suspension.</p></li>
<li><p><strong>Antibody labeling</strong>: In this unique step, oligonucleotide-labeled antibodies are used to bind to surface proteins on each cell. Each antibody is tagged with a DNA barcode unique to the specific protein it targets. These DNA barcodes are critical for linking sequencing data back to the corresponding surface proteins.</p></li>
<li><p><strong>Droplet formation</strong>: Similar to RNA-seq, cells are encapsulated into droplets along with barcoded beads. These beads capture both the mRNA and the antibody-bound oligonucleotides. This simultaneous capture is what makes CITE-seq distinct, as it preserves information about both RNA transcripts and protein markers.</p></li>
<li><p><strong>Cell lysis and RNA/antibody capture</strong>: Cells are lysed within droplets, releasing both RNA and antibody-conjugated oligonucleotides. The RNA hybridizes to barcoded beads as in traditional RNA-seq. Simultaneously, the antibody-derived oligonucleotides are captured using complementary sequences on the same beads.</p></li>
<li><p><strong>Reverse transcription, cDNA synthesis, and amplification</strong>: The RNA is reverse-transcribed into complementary DNA (cDNA), while the antibody-derived oligonucleotides are also amplified. This ensures sufficient material for sequencing both modalities.</p></li>
<li><p><strong>Library preparation and sequencing</strong>: Separate libraries are prepared for the RNA and antibody-derived oligonucleotides, but they are sequenced together on the same high-throughput platform. This co-sequencing ensures that both mRNA and protein data are linked to their respective cells via barcodes.</p></li>
<li><p><strong>Mapping, alignment, and data integration</strong>: RNA reads are mapped to a reference genome or transcriptome as in traditional single-cell RNA-seq. Antibody-derived oligonucleotide reads, however, are mapped to predefined sequences representing the antibody barcodes. The two data modalities are then integrated, enabling joint analyses of mRNA and protein expression.</p></li>
</ol>
<div class="block">
<div id="fig-citeseq-protocol" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-citeseq-protocol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/citeseq-protocol.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-citeseq-protocol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.14: From <a href="https://cite-seq.com/" class="uri">https://cite-seq.com/</a>.
</figcaption>
</figure>
</div>
</div>
<div id="rem-why_surface_markers" class="block2 proof remark">
<p><span class="proof-title"><em>Remark 5.1</em>. </span><strong>Why surface markers matter</strong><br>
Surface proteins often provide critical insights into a cell’s phenotype, function, and state that cannot always be inferred from mRNA expression alone. By combining transcriptomics with surface proteomics, CITE-seq enables deeper biological interpretation, such as identifying cell types or states that are transcriptionally similar but phenotypically distinct.</p>
<p>CITE-seq is often performed on immune cells because of their abundance of surface markers, which play a critical role in their biological functions. Immune cells, such as T cells, B cells, natural killer cells, and monocytes, are defined by their expression of distinct surface proteins. These markers are essential for their roles in immune response, cell-cell communication, and recognition of pathogens or damaged cells.</p>
</div>
<section id="a-brief-note-on-other-technologies" class="level4" data-number="5.4.0.1">
<h4 data-number="5.4.0.1" class="anchored" data-anchor-id="a-brief-note-on-other-technologies"><span class="header-section-number">5.4.0.1</span> A brief note on other technologies</h4>
<p>See <span class="citation" data-cites="gong2025benchmark">(<a href="references.html#ref-gong2025benchmark" role="doc-biblioref"><strong>gong2025benchmark?</strong></a>)</span> for a comparison between different technologies that measure single-cell transcriptomics and proteomics. See <span class="citation" data-cites="lischetti2023dynamic">(<a href="references.html#ref-lischetti2023dynamic" role="doc-biblioref"><strong>lischetti2023dynamic?</strong></a>)</span> for some notes about how CITE-seq has been used, or (as examples) its applications in leukemia <span class="citation" data-cites="granja2019single">(<a href="references.html#ref-granja2019single" role="doc-biblioref"><strong>granja2019single?</strong></a>)</span> or CAR T therapy <span class="citation" data-cites="good2022post">(<a href="references.html#ref-good2022post" role="doc-biblioref"><strong>good2022post?</strong></a>)</span>.</p>
</section>
</section>
<section id="a-primer-on-vaes" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="a-primer-on-vaes"><span class="header-section-number">5.5</span> A Primer on VAEs</h2>
<p>Variational autoencoders (VAE) is a type of deep-learning architecture that is exceptionally common for studying single-cell sequencing data. We’ll build up to this and its application of CITE-seq data slowly, piece-by-piece.</p>
<section id="what-is-an-autoencoder" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="what-is-an-autoencoder"><span class="header-section-number">5.5.1</span> What is an autoencoder?</h3>
<div class="block3">
<p><strong>Input/Output.</strong> The input to an autoencoder is the same as any dimension reduction method (like PCA). The input is a matrix <span class="math inline">\(X \in \mathbb{R}^{n\times p}\)</span> for <span class="math inline">\(n\)</span> cells and <span class="math inline">\(p\)</span> features. The output is <span class="math inline">\(Z \in \mathbb{R}^{n \times k}\)</span>, where <span class="math inline">\(k \ll p\)</span> (where there are <span class="math inline">\(k\)</span> latent dimensions). We call <span class="math inline">\(Z\)</span> the embedding. Autoencoders also return the “denoised” matrix <span class="math inline">\(X' \in \mathbb{R}^{n\times p}\)</span>.</p>
</div>
<p><a href="#fig-dca1-1" class="quarto-xref">Figure&nbsp;<span>5.15</span></a> and <a href="#fig-dca1-2" class="quarto-xref">Figure&nbsp;<span>5.16</span></a> illustrate the general structure of an <em>autoencoder</em>. It consists of two key components: an <strong>encoder</strong>, which maps the input data <span class="math inline">\(x\)</span> into a latent space representation <span class="math inline">\(z\)</span>, and a <strong>decoder</strong>, which reconstructs the input <span class="math inline">\(x\)</span> from the latent variables <span class="math inline">\(z\)</span>.</p>
<div class="block">
<div id="fig-dca1-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dca1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/vae_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dca1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.15: (Left): From <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-dca1-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dca1-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/dca.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dca1-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.16: (Right): From DCA <span class="citation" data-cites="eraslan2019single">(<a href="references.html#ref-eraslan2019single" role="doc-biblioref"><strong>eraslan2019single?</strong></a>)</span>. (Note, DCA <span class="citation" data-cites="eraslan2019single">(<a href="references.html#ref-eraslan2019single" role="doc-biblioref"><strong>eraslan2019single?</strong></a>)</span> is not a variational autoencoder, since it is trained by simply minimizing the log-likelihood.)
</figcaption>
</figure>
</div>
</div>
<p>Some notable aspects: - <strong>Linear and non-linear functions</strong>: Typically, most of these neural network architectures for AEs are in the following form: for example, let the input for one layer be <span class="math inline">\(X \in \mathbb{R}^{p}\)</span>, and the AE learns a weight matrix <span class="math inline">\(W \in \mathbb{R}^{p \times K}\)</span>. Then the output of that layer (which will become the input for the next layer) is <span class="math inline">\(\text{ReLU}(XW)\)</span>, where ReLU is the Rectified Linear Unit.</p>
<ul>
<li><p><strong>Minimizing a “deterministic” loss</strong>: As illustrated in <a href="#fig-dca1-1" class="quarto-xref">Figure&nbsp;<span>5.15</span></a> and <a href="#fig-dca1-2" class="quarto-xref">Figure&nbsp;<span>5.16</span></a>, most AEs typically try to minimize the reconstruction loss (i.e., the difference between the original input and the final output). We’ll see how this changes when we discuss VAEs.</p></li>
<li><p><strong>Compression of information</strong>: As opposed to most NN architectures, in AEs, the number of units in each layer is smaller than the original dimension (typically called “compression” or “bottleneck”), exemplified in <a href="#fig-dca1-1" class="quarto-xref">Figure&nbsp;<span>5.15</span></a> and <a href="#fig-dca1-2" class="quarto-xref">Figure&nbsp;<span>5.16</span></a>. In most other NN architectures (e.g., GANs, Convolution NN, Recurrent NN’s), the number of units typically grows. The latent representation for a cell with expression <span class="math inline">\(X_i \in \mathbb{R}^{p}\)</span> will be called <span class="math inline">\(Z_i \in \mathbb{R}^{K}\)</span> where <span class="math inline">\(K \ll p\)</span>.</p></li>
<li><p><strong>Unsupervised setting</strong>: As opposed to most NN architectures, AEs are meant for the unsupervised setting. Unsurprisingly, since most of the statistical theory for NNs is for the supervised setting, our understanding of AEs is much more limited compared to that for other NN architectures.</p></li>
<li><p><strong>Relation to PCA</strong>: AEs are most similar to PCAs by understanding the reconstruction perspective of PCA <span class="quarto-unresolved-ref">?eq-pca-minimize_reconstruction</span>. PCA uses: 1) linear functions to encode and decode the data, and 2) sets the decoder to be exactly the “inverse” of the encoder. In AEs, we can: 1) use non-linear functions (such as ReLU) for more complex transformations of the data, and 2) allow the decoder to be different from the encoder. (The reason why the second point might seem useful will become clear when we explain VAEs below.)</p></li>
</ul>
</section>
<section id="sec-vae" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="sec-vae"><span class="header-section-number">5.5.2</span> What is a variational autoencoder?</h3>
<div class="block3">
<p><strong>Input/Output.</strong> The input and output of a VAE is the same as those of an autoencoder.</p>
</div>
<p>Here are some motivating questions:</p>
<ul>
<li><strong>Why do we even need to specialize the autoencoder?</strong><br>
Conceptually, we could hope that an AE can serve as a means to generate new data — simply mix up the learned latent representation <span class="math inline">\(Z_i\)</span>’s among all the <span class="math inline">\(n\)</span> cells and use the decoder network to simulate new data. However, due to the highly non-linear nature of NNs, this might result in unexpectedly non-sensible values<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. See <a href="#fig-vae2-1" class="quarto-xref">Figure&nbsp;<span>5.17</span></a> and <a href="#fig-vae2-2" class="quarto-xref">Figure&nbsp;<span>5.18</span></a>.</li>
</ul>
<div class="block">
<div id="fig-vae2-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vae2-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/vae_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vae2-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.17: (Top): Ideal way to generate data.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-vae2-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vae2-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/vae_3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vae2-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.18: (Bottom): Cartoonized illustration to demonstrate why this idealized strategy might not work. Both figures from <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.
</figcaption>
</figure>
</div>
</div>
<ul>
<li><strong>How does this impact our conceptual understanding of VAEs?</strong><br>
Hence, this is where we would like to adjust the NN architecture slightly — we want to insert randomness right in the middle of the AE to slightly perturb <span class="math inline">\(Z\)</span>, and we would like the cell’s original expression be similar (“on average”) to its reconstructed expression when its latent representation is slightly perturbed right before it’s decoded. The result is a framework illustrated in <a href="#fig-vae4" class="quarto-xref">Figure&nbsp;<span>5.19</span></a>.</li>
</ul>
<div class="block">
<div id="fig-vae4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vae4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/vae_4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vae4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.19: From <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.
</figcaption>
</figure>
</div>
</div>
<p>Note: The formal way to describe how to set up this framework involves priors, posteriors, etc. We’ll avoid all of this for simplicity, but you can see <span class="citation" data-cites="kingma2019introduction">(<a href="references.html#ref-kingma2019introduction" role="doc-biblioref"><strong>kingma2019introduction?</strong></a>)</span> for a formal description.</p>
<ul>
<li><strong>What does the architecture for a VAE look like?</strong><br>
A brief quote of what a prototypical VAE is, from <span class="citation" data-cites="battey2021visualizing">(<a href="references.html#ref-battey2021visualizing" role="doc-biblioref"><strong>battey2021visualizing?</strong></a>)</span>:</li>
</ul>
<div class="block">
<blockquote class="blockquote">
<p>“VAEs consist of a pair of deep neural networks in which the first network (the encoder) encodes input data as a probability distribution in a latent space and the second (the decoder) seeks to recreate the input given a set of latent coordinates (Kingma and Welling, 2013). Thus a VAE has as its target the input data itself.”</p>
</blockquote>
</div>
<p><a href="#fig-vae1-1" class="quarto-xref">Figure&nbsp;<span>5.20</span></a> and <a href="#fig-vae1-2" class="quarto-xref">Figure&nbsp;<span>5.21</span></a> illustrate the typical VAE. We see there are now more components — the encoder network “splits” into two types of latent representations, one for the mean of the latent distribution and one for the variance. We then (during the training process) artificially inject Gaussian noise, which “perturbs” our latent representation <span class="math inline">\(Z\)</span>, which is then decoded to get our “denoised” cell’s expression.</p>
<div class="block">
<div id="fig-vae1-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vae1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/vae_5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vae1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.20: (Left): From <span class="citation" data-cites="battey2021visualizing">(<a href="references.html#ref-battey2021visualizing" role="doc-biblioref"><strong>battey2021visualizing?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-vae1-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vae1-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/vae_6.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vae1-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.21: (Right): From <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.
</figcaption>
</figure>
</div>
</div>
<p>At first, it might seem odd that we’re artificially injecting noise into our training process, as we might think this is harmful to our resulting denoising process<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. There are two ways to think about this:</p>
<ul>
<li><p><strong>Making the training “more robust”</strong>: Despite the decoding network being so highly non-linear, we are trying to ensure that the latent representation still gives a good reconstruction despite minor perturbations<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p></li>
<li><p><strong>Giving us additional control over the latent representation</strong>: We’re not getting into the details about the prior or posterior, but essentially, we’re going to regularize the latent distribution to “look like” a standard multivariate Gaussian (i.e., the prior). This ensures our latent representation: 1) doesn’t go wild and look extremely odd (i.e., the typical “overfitting” concern) and 2) it only wouldn’t still look like a standard multivariate Gaussian afterwards (i.e., the posterior) if there’s “enough evidence to deviate from the prior.”</p></li>
<li><p><strong>What does the objective function look like?</strong><br>
As with any NN architecture, the objective function is immensely important since this is how we “guide” the optimization procedure to prioritize “searching in certain spaces.”</p></li>
</ul>
<p>The “variational” part of VAEs comes from the fact that the objective function is derived via a variational strategy<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. This allows us to approximate the posterior distribution <span class="math inline">\(p(Z|X)\)</span>. After some complicated math of deriving what parameters “maximize the posterior distribution,” we can derive that the objective function amounts to maximizing something that loosely looks like the following (fixing the prior to typically be a standard multivariate Gaussian)<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, for some tuning parameter <span class="math inline">\(\beta \geq 0\)</span>:</p>
<p><span id="eq-vae_objective"><span class="math display">\[
\begin{aligned}
\max_{\text{encoder}, \text{decoder}} \mathcal{L}(\text{encoder}, \text{decoder}; X_{1:n})
&amp;= \sum_{i=1}^{n}
   \underbrace{\mathbb{E}_{q_{\text{encoder}}(Z_i \mid X_i)}
   \Big[\log p_{\text{decoder}}(X_i \mid Z_i)\Big]}_{\text{reconstruction error}} \\
&amp;\quad - \underbrace{\beta \cdot \text{KL}\big(q_{\text{encoder}}(Z_i \mid X_i)
   \,\|\, p_{\text{prior}}(Z_i)\big)}_{\text{Deviation from prior}}
\end{aligned}
\tag{5.1}\]</span></span></p>
<p>or equivalently as a minimization (which is more commonly written, since we often call this the <em>loss function</em>),</p>
<p><span class="math display">\[
\begin{aligned}
\min_{\text{encoder}, \text{decoder}} \mathcal{L}(\text{encoder}, \text{decoder}; X_{1:n})
&amp;= \sum_{i=1}^{n} -\mathbb{E}_{q_{\text{encoder}}(Z_i|X_i)}
    \Big[\log p_{\text{decoder}}(X_i \mid Z_i)\Big] \\
&amp;\quad + \beta \cdot \text{KL}\big(q_{\text{encoder}}(Z_i \mid X_i) \,\|\, p_{\text{prior}}(Z_i)\big)
\end{aligned}
\]</span></p>
<p>A brief quote from <span class="citation" data-cites="battey2021visualizing">(<a href="references.html#ref-battey2021visualizing" role="doc-biblioref"><strong>battey2021visualizing?</strong></a>)</span>:</p>
<div class="block">
<blockquote class="blockquote">
<p>“The loss function for a VAE is the sum of reconstruction error (how different the generated data is from the input) and Kullback-Leibler (KL) divergence between a sample’s distribution in latent space and a reference distribution which acts as a prior on the latent space (here we use a standard multivariate normal, but see (Davidson et al., 2018) for an alternative design with a hyperspherical latent space). The KL term of the loss function incentivizes the encoder to generate latent distributions with meaningful distances among samples, while the reconstruction error term helps to achieve good local clustering and data generation.”</p>
</blockquote>
</div>
<p>Additional notes: - <strong>What are these terms?</strong><br>
Here, “encoder” and “decoder” refer to the parameters we need to learn for the encoding or decoding NN. Then:<br>
1. <span class="math inline">\(q_{\text{encoder}}(Z_i|X_i)\)</span> refers to the variational distribution that maps <span class="math inline">\(X_i\)</span> to <span class="math inline">\(Z_i\)</span> which approximates the posterior <span class="math inline">\(p(Z_i|X_i)\)</span>. Thinking back to <a href="#fig-vae1-1" class="quarto-xref">Figure&nbsp;<span>5.20</span></a> and <a href="#fig-vae1-2" class="quarto-xref">Figure&nbsp;<span>5.21</span></a>, we can think of this as, “based on our encoder (which maps <span class="math inline">\(X_i\)</span> to a multivariate Gaussian’s mean and covariance matrix), what is the probability of observing the latent representation <span class="math inline">\(Z_i\)</span>?”<br>
2. <span class="math inline">\(p_{\text{prior}}(Z_i)\)</span> is the prior that is typically set to a multivariate Gaussian, i.e., <span class="math inline">\(Z_i \sim N(0, I_{K})\)</span>, which is typically not altered.<br>
3. <span class="math inline">\(p_{\text{decoder}}(X_i|Z_i)\)</span> refers to the conditional likelihood, which we can think of as, “based on the latent representation <span class="math inline">\(Z_i\)</span>, what is the probability of observing the expression vector <span class="math inline">\(X_i\)</span>?”</p>
<ul>
<li><p><strong>KL term</strong>: This term acts as a regularizer. It measures the KL divergence between the variational distribution (i.e., the approximation of the posterior) and the prior. As mentioned in <span class="citation" data-cites="kingma2013auto">(<a href="references.html#ref-kingma2013auto" role="doc-biblioref"><strong>kingma2013auto?</strong></a>)</span>, we can typically compute this term in closed form if <span class="math inline">\(q_{\text{encoder}}(Z_i|X_i)\)</span> and <span class="math inline">\(p_{\text{prior}}(Z_i)\)</span> are both Gaussians.</p></li>
<li><p><strong>Reconstruction error term</strong>: This is the typical reconstruction error. If we set up our NN architecture so that <span class="math inline">\(p_{\text{decoder}}(X_i|Z_i)\)</span> is a Gaussian, then this is proportional to some Euclidean distance. As mentioned in Appendix C.2 of <span class="citation" data-cites="kingma2013auto">(<a href="references.html#ref-kingma2013auto" role="doc-biblioref"><strong>kingma2013auto?</strong></a>)</span>, this decoder typically splits into two (similar to the encoder illustrated in <a href="#fig-vae1-1" class="quarto-xref">Figure&nbsp;<span>5.20</span></a> and <a href="#fig-vae1-2" class="quarto-xref">Figure&nbsp;<span>5.21</span></a>) since we need to map <span class="math inline">\(Z_i\)</span> into a mean and variance of a <span class="math inline">\(p\)</span>-dimensional Gaussian distribution.</p>
<p>This expectation is taken with respect to the variational distribution <span class="math inline">\(q_{\text{encoder}}(Z_i|X_i)\)</span>, so we typically need to sample <span class="math inline">\(Z_i\)</span>’s in order to approximate this expectation in practice.</p></li>
<li><p><strong>Tuning hyperparameter</strong>: The <span class="math inline">\(\beta \geq 0\)</span> hyperparameter was an addition to yield the <span class="math inline">\(\beta\)</span>-VAE <span class="citation" data-cites="higgins2017beta">(<a href="references.html#ref-higgins2017beta" role="doc-biblioref"><strong>higgins2017beta?</strong></a>)</span>. This extra tuning parameter is commonly used to allow the user to determine how “much” to regularize the objective function. The larger the <span class="math inline">\(\beta\)</span>, the more emphasis there is on learning a simple encoder (striving for less overfitting, at the cost of worse reconstruction).</p></li>
<li><p><strong>Give me a concrete example!</strong><br>
Unfortunately, <a href="#eq-vae_objective" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> is written in generic math notation. In other words, it doesn’t really tell you the explicit formula on how to compute the objective in practice. If you want to see the full derivation of the objective for specifically: 1) a standard multivariate Gaussian prior for <span class="math inline">\(p_{\text{prior}}(Z_i)\)</span>, and 2) a Gaussian data generation mechanism for <span class="math inline">\(p_{\text{decoder}}(X_i|Z_i)\)</span>, then see <span class="citation" data-cites="odaibo2019tutorial">(<a href="references.html#ref-odaibo2019tutorial" role="doc-biblioref"><strong>odaibo2019tutorial?</strong></a>)</span>. (Setting the prior and data-generation mechanism will determine the form of the log-likelihood in <span class="math inline">\(\log p_{\text{decoder}}(X_i|Z_i)\)</span> and the form of the posterior distribution <span class="math inline">\(q_{\text{encoder}}(Z_i|X_i)\)</span>.)</p></li>
</ul>
<p>Here are some extra terminology typically used in deep-learning: - <strong>Batches</strong>: Small subsets of cells used to calculate gradients during training, allowing for scalable optimization. One of the biggest innovations from deep-learning is that it turns out that 1) stochastic gradient descent is pretty good at solving difficult non-convex problems, and 2) stochastic gradient descent out-of-the-box can do this pretty well (with the help of automated differentiation techniques) so that you, the data analyst, don’t need to derive anything too complicated outside of the deep-learning loss and architecture. See Adam, the foundation of modern stochastic gradient descent <span class="citation" data-cites="kingma2014adam">(<a href="references.html#ref-kingma2014adam" role="doc-biblioref"><strong>kingma2014adam?</strong></a>)</span>. The “batch” refers to the random subset of cells used to update the gradient in each step.</p>
<ul>
<li><strong>Epochs</strong>: Full passes through the entire dataset during training.</li>
</ul>
<p>The modular nature of VAEs allows for modifications, such as changing the structure of the encoder/decoder or altering the prior distribution, making them highly adaptable to different data types and tasks, including the integration of multimodal single-cell data like CITE-seq, as we will see soon in <a href="#sec-union_totalvi" class="quarto-xref"><span>Section 5.6.1</span></a>.</p>
<p>scVI (single-cell Variational Inference) is a specialized implementation of a Variational Autoencoder (VAE) tailored to single-cell RNA-seq data. It models the observed count data as being drawn from a negative binomial (NB) distribution, allowing it to account for the overdispersion inherent in single-cell data. The framework is designed to simultaneously capture the low-dimensional latent structure of the data and model the technical variability associated with sequencing, such as differences in library sizes.</p>
<p>Like a generic VAE, scVI consists of an encoder and a decoder. The encoder maps the observed gene expression counts ( X ) into a latent space ( Z ), capturing the underlying biological variation. The decoder reconstructs the counts ( X ) from ( Z ), but in scVI, this reconstruction explicitly models count data using the negative binomial distribution: [ X_{ij} (<em>{ij}, r_j), ] where ( </em>{ij} ) is the mean expression for gene ( j ) in cell ( i ), and ( r_j ) is the dispersion parameter for cell ( i ).</p>
<p>The latent space ( Z ) represents a compressed, biologically meaningful representation of each cell, and it is regularized to follow a standard Gaussian prior ( p(Z) = (0, I) ). The overall loss function to be minimized is the evidence lower bound (ELBO): <span class="math display">\[
\begin{align*}
\mathcal{L}(\text{encoder}, \text{decoder}; X_{1:n}) &amp;=
\sum_{i=1}^{n}\bigg[-\mathbb{E}_{q_{\text{encoders}}(Z_i, \ell_i \mid X_i)} \Big[ \log p_{\text{decoder}}(X_i \mid Z_i, \ell_i) \Big] \\
&amp;\quad + \text{KL}\Big(q_{\text{encoder1}}(Z_i \mid X_i) \, \| \, p_{\text{prior1}}(Z_i)\Big) \\
&amp;\quad + \text{KL}\Big(q_{\text{encoder2}}(\ell_i \mid X_i) \, \| \, p_{\text{prior2}}(\ell_i)\Big)\bigg],
\end{align*}
\]</span></p>
<p>where ( _i ) represents the library size (total counts per cell).</p>
</section>
<section id="architecture-for-library-size-and-overdispersion" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="architecture-for-library-size-and-overdispersion"><span class="header-section-number">5.5.3</span> Architecture for library size and overdispersion</h3>
<p>The architecture consists of:</p>
<ul>
<li><p><strong>Encoders</strong>: The input is the raw count <span class="math inline">\(X_i\)</span>. There are technically two encoders — one encodes the latent space <span class="math inline">\(Z_i\)</span> (which represents the biological variation in cells) and the other, the library size <span class="math inline">\(\ell_i\)</span>. The first encoder, <span class="math inline">\(q_{\text{encoder1}}(Z_i \mid X_i)\)</span>, learns a multivariate Gaussian with a diagonal covariance matrix, and the second encoder, <span class="math inline">\(q_{\text{encoder2}}(\ell_i \mid X_i)\)</span>, is a log-normal distribution.</p></li>
<li><p><strong>Decoder</strong>: The inputs are the latent variables <span class="math inline">\(Z_i\)</span> and library size <span class="math inline">\(\ell_i\)</span>. The decoder predicts the mean <span class="math inline">\(\mu_{ij}\)</span> for each gene <span class="math inline">\(j\)</span> based on <span class="math inline">\(Z_i\)</span>, and <span class="math inline">\(\mu_{ij}\)</span> alongside <span class="math inline">\(\ell_i\)</span> are used to compute the objective function. (The dispersion parameter <span class="math inline">\(r_j\)</span> is learned via variational Bayesian inference — see the paper.)</p></li>
</ul>
<p>The so-called “plate diagram” in <a href="#fig-scvi-architecture2" class="quarto-xref">Figure&nbsp;<span>5.22</span></a> gives a good schematic summary of this architecture (but I’ve personally found that these diagrams are more of a “useful reminder” — they’re a bit hard to learn from directly).</p>
<div class="block">
<div id="fig-scvi-architecture2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scvi-architecture2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/scvi-architecture2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scvi-architecture2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.22: A concise caption for the plate diagram.
</figcaption>
</figure>
</div>
</div>
</section>
</section>
<section id="multi-modal-integration-applicable-beyond-cite-seq" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="multi-modal-integration-applicable-beyond-cite-seq"><span class="header-section-number">5.6</span> Multi-modal integration (applicable beyond CITE-seq)</h2>
<p>Coming back to CITE-seq data <a href="#sec-cite-seq_tech" class="quarto-xref"><span>Section 5.4</span></a>, suppose we have the following goal — we want to create a unified low-dimensional embedding (i.e., a “map” or “atlas” of sorts) that integrates the information from both the transcriptome and proteome. That is, there are some cell-type variations present only in the RNA or only in the ADT (Antibody-Derived Tags)<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, and we want to create one unified embedding that combines all these information together. See <a href="#fig-wnn_separate-1" class="quarto-xref">Figure&nbsp;<span>5.23</span></a> and <a href="#fig-wnn_separate-2" class="quarto-xref">Figure&nbsp;<span>5.24</span></a> as an example.</p>
<div class="block">
<div id="fig-wnn_separate-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wnn_separate-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/wnn_seperate.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wnn_separate-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.23: (Top) The UMAP of either just the genes (left) or just the proteins (right). From <a href="https://satijalab.org/seurat/articles/weighted_nearest_neighbor_analysis" class="uri">https://satijalab.org/seurat/articles/weighted_nearest_neighbor_analysis</a> for a method called WNN (which isn’t TotalVI, but this figure is good at illustrating the point).
</figcaption>
</figure>
</div>
</div>
<div class="block">
<div id="fig-wnn_separate-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wnn_separate-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/wnn_together.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wnn_separate-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.24: (Bottom) The UMAP after a method (WNN in this case) integrates the RNA and ADT together. From <a href="https://satijalab.org/seurat/articles/weighted_nearest_neighbor_analysis" class="uri">https://satijalab.org/seurat/articles/weighted_nearest_neighbor_analysis</a>.
</figcaption>
</figure>
</div>
</div>
<p>Your first thought might be to simply perform PCA on the entire single-cell sequencing (i.e., one big PCA on all the 2000+ highly variable genes, and 100-ish proteins). However, this might not work as you might hope, since this wouldn’t account for the different scales and sparsities of RNA and ADT signals. That is, the RNA-dominated gene expression data could overwhelm the smaller protein feature set. This addresses the core challenge in simply combining modalities — differences in dimensionality, normalization, and noise — so that a single embedding captures both transcriptomic and proteomic variation while preserving each modality’s unique biological signal.</p>
<p>We will mention this concept of integration between two omics more in <span class="quarto-unresolved-ref">?sec-atac_integration</span> when we talk about 10x multiome (RNA &amp; ATAC). For now though, we will focus on TotalVI, which is one of the most common ways to achieve this goal for CITE-seq data.</p>
<section id="sec-union_totalvi" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="sec-union_totalvi"><span class="header-section-number">5.6.1</span> TotalVI (and other methods for the “union” of information)</h3>
<div class="block3">
<p><strong>Input/Output.</strong> The input to TotalVI is a multi-modal sequencing matrix, where 1) the count matrix (for gene expression) <span class="math inline">\(X \in \mathbb{Z}_+^{n\times p}\)</span> for <span class="math inline">\(n\)</span> cells and <span class="math inline">\(p\)</span> genes, and 2) the count matrix (for protein abundance) <span class="math inline">\(Y \in \mathbb{Z}_+^{n\times q}\)</span> for <span class="math inline">\(n\)</span> cells and <span class="math inline">\(q\)</span> proteins. The output is: 1) <span class="math inline">\(Z \in \mathbb{R}^{n \times k}\)</span>, where <span class="math inline">\(k \ll \min\{p,q\}\)</span> (where there are <span class="math inline">\(k\)</span> latent dimensions), and also 2) the normalized gene expression matrix <span class="math inline">\(X' \in \mathbb{R}^{n\times p}\)</span>, and 3) the normalized protein abundance matrix <span class="math inline">\(Y' \in \mathbb{R}^{n\times q}\)</span>.</p>
</div>
<p>TotalVI <span class="citation" data-cites="gayoso2021joint">(<a href="references.html#ref-gayoso2021joint" role="doc-biblioref"><strong>gayoso2021joint?</strong></a>)</span> is a deep generative model specifically designed for the joint analysis of single-cell RNA and protein data obtained from techniques like CITE-seq. Unlike traditional single-cell RNA-seq methods that analyze transcriptomic data in isolation, TotalVI simultaneously incorporates transcriptomic and proteomic information to create a unified representation of cell states.</p>
<p>TotalVI builds upon the VAE framework of scVI, adapting it to the multimodal nature of CITE-seq data. The RNA counts are modeled with a negative binomial distribution, as in scVI, to capture overdispersion in single-cell data. For proteins, TotalVI introduces a mixture model that separates background and foreground components. This enables accurate modeling of protein measurements, even in the presence of high background noise. TotalVI’s decoder uses neural networks to predict the parameters of both the RNA and protein likelihoods, while the encoder maps cells into a shared low-dimensional latent space that reflects their joint transcriptomic and proteomic profiles. See <a href="#fig-totalvi" class="quarto-xref">Figure&nbsp;<span>5.25</span></a> for an overview.</p>
<div class="block">
<div id="fig-totalvi" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-totalvi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/totalvi.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-totalvi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.25: From <span class="citation" data-cites="gayoso2021joint">(<a href="references.html#ref-gayoso2021joint" role="doc-biblioref"><strong>gayoso2021joint?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<section id="the-totalvi-model-and-key-equations" class="level4" data-number="5.6.1.1">
<h4 data-number="5.6.1.1" class="anchored" data-anchor-id="the-totalvi-model-and-key-equations"><span class="header-section-number">5.6.1.1</span> The TotalVI model and key equations</h4>
<p>TotalVI models RNA and protein counts <span class="math inline">\(x_{ng}\)</span> and <span class="math inline">\(y_{nt}\)</span> for gene <span class="math inline">\(g\)</span> and protein <span class="math inline">\(t\)</span> in cell <span class="math inline">\(n\)</span> as follows:</p>
<p><span class="math display">\[
x_{ij} \sim \text{NB}(\ell_i \rho_{ij}, \theta_g), \quad \text{and}\quad y_{it} \sim \text{NB-Mixture}(\text{Foreground} + \text{Background}).
\]</span></p>
<p>First, the RNA side: - <span class="math inline">\(\ell_n\)</span>: RNA library size for cell <span class="math inline">\(n\)</span>. - <span class="math inline">\(\rho_{ng}\)</span>: Normalized RNA expression frequency, predicted by the decoder network from the latent variables. - <span class="math inline">\(\theta_g\)</span>: Gene-specific overdispersion parameter for gene <span class="math inline">\(g\)</span> (estimated for each gene during training, much like in scVI).</p>
<p>The values in <span class="math inline">\(\rho_{ng}\)</span> are dictated by the learned latent embedding <span class="math inline">\(z_n\)</span> using an encoder. A separate encoder will learn <span class="math inline">\(\ell_n\)</span>.</p>
<p>Next, the Protein side. The protein counts <span class="math inline">\(y_{it}\)</span> are modeled with a negative binomial mixture:</p>
<p><span class="math display">\[
y_{nt} \sim v_{nt} \,\text{NB}(\beta_{nt}, \phi_t) + (1 - v_{nt}) \,\text{NB}(\alpha_{nt} \beta_{nt}, \phi_t),
\]</span></p>
<p>where: - <span class="math inline">\(v_{nt}\)</span>: Bernoulli variable indicating whether the count is from the background or foreground. - <span class="math inline">\(\beta_{nt}\)</span>: Protein background mean, learned during training. - <span class="math inline">\(\alpha_{nt}\)</span>: Scaling factor for foreground counts. - <span class="math inline">\(\phi_t\)</span>: Protein-specific overdispersion parameter.</p>
<p>The values in <span class="math inline">\(v_{nt}\)</span> and <span class="math inline">\(\alpha_{nt}\)</span> (but not <span class="math inline">\(\beta_{nt}\)</span>) are dictated by the latent embedding <span class="math inline">\(z_n\)</span><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<p>The latent embedding<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> <span class="math inline">\(z_n \in \mathbb{R}^k\)</span> dictates: 1. <span class="math inline">\(\rho_{ng}\)</span> (normalized gene expression), 2. <span class="math inline">\(\alpha_{nt}\)</span> (protein foreground counts), and 3. <span class="math inline">\(\phi_t\)</span> (probability of the protein displaying a foreground count).</p>
<div class="block">
<div id="fig-totalvi-architecture" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-totalvi-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/totalvi-architecture.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-totalvi-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.26: From <span class="citation" data-cites="gayoso2021joint">(<a href="references.html#ref-gayoso2021joint" role="doc-biblioref"><strong>gayoso2021joint?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>
<p>Now we understand the generative model (decoder). The encoder is composed of three separate encoders: 1. Encoding observed gene expression <span class="math inline">\(x_n\)</span> and protein expression <span class="math inline">\(y_n\)</span> to yield <span class="math inline">\(z_n\)</span>, 2. Encoding just <span class="math inline">\(x_n\)</span> to yield <span class="math inline">\(\ell_n\)</span>, and 3. Encoding just <span class="math inline">\(y_n\)</span> to yield <span class="math inline">\(\beta_n\)</span>.</p>
<p>The objective function is the evidence lower bound (ELBO) to be minimized:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\text{encoders}, \text{decoders}; x_{\text{all cells}}, y_{\text{all cells}})
&amp;=
\sum_{n} \bigg[-\mathbb{E}_{q_{\text{encoders}}(z_n, \ell_n, \beta_n | x_n, y_n)} \log p_{\text{decoders}}(x_n, y_n | z_n, \ell_n, \beta_n) \\
&amp;\quad + \text{KL}\big(q_{\text{encoder1}}(z_n | x_n, y_n) \, \| \, p_{\text{prior1}}(z_n)\big) \\
&amp;\quad + \text{KL}\big(q_{\text{encoder2}}(\ell_n | x_n) \, \| \, p_{\text{prior2}}(\ell_n)\big) \\
&amp;\quad + \text{KL}\big(q_{\text{encoder3}}(\beta_n | y_n) \, \| \, p_{\text{prior3}}(\beta_n)\big) \bigg].
\end{aligned}
\]</span></p>
<div id="rem-union" class="block2 proof remark">
<p><span class="proof-title"><em>Remark 5.2</em>. </span><strong>Why did I call these “union” methods?</strong><br>
From the generative model</p>
<p><span class="math display">\[
p_{\text{decoders}}(x_n, y_n | z_n, \ell_n, \beta_n)
\]</span></p>
<p>the latent embedding <span class="math inline">\(z_n\)</span> is used to generate both the RNA <span class="math inline">\(x_n\)</span> and protein <span class="math inline">\(y_n\)</span>. Intuitively, <span class="math inline">\(z_n\)</span> likely contains more information than either <span class="math inline">\(x_n\)</span> or <span class="math inline">\(y_n\)</span> alone. The goal of TotalVI is to create one big atlas that provides a bird’s-eye view of all the sources of variation in the data — some originating from RNA, others from protein.</p>
</div>
<p><strong>A brief note on other approaches.</strong><br>
A popular non-deep-learning approach is Weighted Nearest Neighbors (WNN) <span class="citation" data-cites="hao2020integrated">(<a href="references.html#ref-hao2020integrated" role="doc-biblioref"><strong>hao2020integrated?</strong></a>)</span> (shown in <a href="#fig-wnn_separate-1" class="quarto-xref">Figure&nbsp;<span>5.23</span></a> and <a href="#fig-wnn_separate-2" class="quarto-xref">Figure&nbsp;<span>5.24</span></a>). More generally, methods for joint embeddings of multimodal data are discussed in <span class="citation" data-cites="wang2024progress">(<a href="references.html#ref-wang2024progress" role="doc-biblioref"><strong>wang2024progress?</strong></a>)</span>, <span class="citation" data-cites="brombacher2022performance">(<a href="references.html#ref-brombacher2022performance" role="doc-biblioref"><strong>brombacher2022performance?</strong></a>)</span>, and <span class="citation" data-cites="makrodimitris2024depth">(<a href="references.html#ref-makrodimitris2024depth" role="doc-biblioref"><strong>makrodimitris2024depth?</strong></a>)</span>. Deep learning dominates this field, with differences in how architectures and losses manage information flow. See <a href="#fig-poe" class="quarto-xref">Figure&nbsp;<span>5.27</span></a>.</p>
<div class="block">
<div id="fig-poe" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chap4/poe.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.27: From <span class="citation" data-cites="makrodimitris2024depth">(<a href="references.html#ref-makrodimitris2024depth" role="doc-biblioref"><strong>makrodimitris2024depth?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</div>


</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Proteins typically degrade much slower than mRNA fragments. See https://book.bionumbers.org/how-fast-do-rnas-and-proteins-degrade. For this reason, you might hypothesize that “cellular memory” is stored via proteins, not mRNA.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See https://www.youtube.com/watch?v=P_fHJIYENdI for a fun YouTube video for more about this.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Each feature in the protein modality in CITE-seq data is often called ADT. It might sometimes be called a protein marker as well.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Notice there is no “library size” for protein.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Each feature in the protein modality in CITE-seq data is often called ADT. It might sometimes be called a protein marker as well.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Notice there is no “library size” for protein.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Technically, <span class="math inline">\(z_n\)</span> is a Logistic Normal, which means it’s a distribution over the simplex. See more details in <a href="https://docs.scvi-tools.org/en/latest/user_guide/models/totalvi.html" class="uri">https://docs.scvi-tools.org/en/latest/user_guide/models/totalvi.html</a>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>It’s not too important to understand what this means, but you can see <span class="citation" data-cites="kingma2019introduction">(<a href="references.html#ref-kingma2019introduction" role="doc-biblioref"><strong>kingma2019introduction?</strong></a>)</span> for a formal definition.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Some details about this “complicated math”: The MAP is generally intractable to compute due to a very annoying normalization constant. So instead of computing MAP directly, we maximize the ELBO, which is a lower-bound of the likelihood. This is really the same trick used in deriving the EM algorithm. See <a href="https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29" class="uri">https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29</a> or <span class="citation" data-cites="lotfollahi2019scgen">(<a href="references.html#ref-lotfollahi2019scgen" role="doc-biblioref"><strong>lotfollahi2019scgen?</strong></a>)</span> for a derivation of the ELBO, and <span class="citation" data-cites="kingma2013auto">(<a href="references.html#ref-kingma2013auto" role="doc-biblioref"><strong>kingma2013auto?</strong></a>)</span> for the formal objective function.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Each feature in the protein modality in CITE-seq data is often called ADT. It might sometimes be called a protein marker as well.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Notice there is no “library size” for protein.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Technically, <span class="math inline">\(z_n\)</span> is a Logistic Normal, which means it’s a distribution over the simplex. See more details in <a href="https://docs.scvi-tools.org/en/latest/user_guide/models/totalvi.html" class="uri">https://docs.scvi-tools.org/en/latest/user_guide/models/totalvi.html</a>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter3_rna.html" class="pagination-link" aria-label="Single-cell RNA-sequencing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Single-cell RNA-sequencing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter5_epigenetics.html" class="pagination-link" aria-label="Single-cell epigenetics">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Single-cell epigenetics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>